"""
Supervised Fine-tuning Trainer
Function: Implement supervised fine-tuning of Qwen-7B-math on GSM8K dataset
"""

import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import yaml
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional, Union
import wandb
from torch.utils.data import DataLoader
from utils.cache_utils import suppress_past_key_values_warning, update_model_for_modern_cache


class SFTTrainer:
    """Supervised Fine-tuning Trainer"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–SFTè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # æŠ‘åˆ¶past_key_valuesè­¦å‘Š
        suppress_past_key_values_warning()
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get("logging", {}).get("use_wandb", False):
            wandb.init(
                project=config["logging"]["wandb_project"],
                config=config
            )
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model"]["student_model_name"],
                trust_remote_code=True,
                padding_side="left"
            )
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config["model"]["student_model_name"],
                torch_dtype=getattr(torch, self.config["device"]["torch_dtype"]),
                device_map=self.config["device"]["device_map"],
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å¹¶ä¿®å¤embeddingå¤§å°ä»¥åŒ¹é…tokenizerï¼ˆåœ¨åº”ç”¨LoRAä¹‹å‰ï¼‰
            # è¿™å¯ä»¥é˜²æ­¢SFTé˜¶æ®µè®­ç»ƒæ—¶å‡ºç°vocab_sizeä¸åŒ¹é…ï¼Œé¿å…RLé˜¶æ®µåŠ è½½checkpointæ—¶çš„é—®é¢˜
            tokenizer_vocab_size = len(self.tokenizer)
            try:
                input_emb_size = self.model.get_input_embeddings().weight.size(0)
                output_emb_size = None
                if hasattr(self.model, 'get_output_embeddings') and self.model.get_output_embeddings() is not None:
                    output_emb_size = self.model.get_output_embeddings().weight.size(0)
                
                self.logger.info(f"ğŸ“Š Embeddingå¤§å°æ£€æŸ¥ï¼ˆåº”ç”¨LoRAå‰ï¼‰:")
                self.logger.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                self.logger.info(f"   model input_embeddings.size(0): {input_emb_size}")
                if output_emb_size is not None:
                    self.logger.info(f"   model output_embeddings.size(0): {output_emb_size}")
                self.logger.info(f"   model.config.vocab_size: {getattr(self.model.config, 'vocab_size', 'N/A')}")
                
                # å¦‚æœembeddingå¤§å°ä¸tokenizerä¸åŒ¹é…ï¼Œæ‰§è¡Œresize
                if input_emb_size != tokenizer_vocab_size:
                    self.logger.warning(f"âš ï¸ æ¨¡å‹embeddingå¤§å° ({input_emb_size}) != tokenizerå¤§å° ({tokenizer_vocab_size})")
                    self.logger.info(f"   æ­£åœ¨resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                    self.model.resize_token_embeddings(tokenizer_vocab_size)
                    self.logger.info(f"âœ… resize_token_embeddingså®Œæˆ")
                    
                    # éªŒè¯resizeæ˜¯å¦æˆåŠŸ
                    new_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                    if new_input_emb_size != tokenizer_vocab_size:
                        self.logger.error(f"âŒ resize_token_embeddingså¤±è´¥ï¼æ–°å¤§å°: {new_input_emb_size} != {tokenizer_vocab_size}")
                        raise ValueError(f"Resizeå¤±è´¥: {new_input_emb_size} != {tokenizer_vocab_size}")
                    else:
                        self.logger.info(f"âœ… resizeæˆåŠŸéªŒè¯: input_embeddings.size(0) = {new_input_emb_size}")
                else:
                    self.logger.info(f"âœ… embeddingå¤§å°ä¸tokenizeråŒ¹é…ï¼Œæ— éœ€resize")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ resize_token_embeddingsæ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸æ”¯æŒæˆ–å·²é‡åŒ–ï¼‰: {e}")
                # å¦‚æœæ˜¯ValueErrorï¼ˆresizeå¤±è´¥ï¼‰ï¼Œåº”è¯¥æŠ›å‡ºå¼‚å¸¸
                if isinstance(e, ValueError):
                    raise
                # å…¶ä»–é”™è¯¯ï¼ˆå¦‚æ¨¡å‹ä¸æ”¯æŒresizeï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­
            
            # åº”ç”¨LoRAï¼ˆç°åœ¨embeddingå¤§å°å·²ç»åŒ¹é…ï¼‰
            if self.config["model"].get("use_lora", True):
                lora_config = LoraConfig(**self.config["lora"])
                self.model = get_peft_model(self.model, lora_config)
                self.logger.info("LoRA configuration applied")
                
                # ğŸ”¥ å…³é”®ï¼šåº”ç”¨LoRAåå†æ¬¡éªŒè¯embeddingå¤§å°ï¼ˆLoRAä¸åº”è¯¥æ”¹å˜embeddingå¤§å°ï¼Œä½†æ£€æŸ¥ä¸€ä¸‹ï¼‰
                try:
                    final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                    if final_input_emb_size != tokenizer_vocab_size:
                        self.logger.warning(f"âš ï¸ åº”ç”¨LoRAåï¼Œinput_embeddingså¤§å° ({final_input_emb_size}) != tokenizer ({tokenizer_vocab_size})")
                        self.logger.info(f"   æ­£åœ¨å†æ¬¡resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                        try:
                            self.model.resize_token_embeddings(tokenizer_vocab_size)
                            new_final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                            if new_final_input_emb_size == tokenizer_vocab_size:
                                self.logger.info(f"âœ… LoRAåresizeæˆåŠŸ: input_embeddings = {new_final_input_emb_size}")
                            else:
                                self.logger.error(f"âŒ LoRAåresizeå¤±è´¥: {new_final_input_emb_size} != {tokenizer_vocab_size}")
                        except Exception as e2:
                            self.logger.warning(f"âš ï¸ LoRAåresizeå¤±è´¥: {e2}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ— æ³•æ£€æŸ¥LoRAåçš„embeddingå¤§å°: {e}")
            
            # æ›´æ–°æ¨¡å‹ä»¥ä½¿ç”¨ç°ä»£ç¼“å­˜
            self.model = update_model_for_modern_cache(self.model)
            
            self.logger.info("Model setup completed")
            
        except Exception as e:
            self.logger.error(f"Model setup failed: {e}")
            raise
    
    def prepare_dataset(self, dataset: Dataset) -> Dataset:
        """å‡†å¤‡æ•°æ®é›†"""
        def preprocess_function(examples):
            texts = []
            for question, answer in zip(examples["question"], examples["answer"]):
                prompt = f"Question: {question}\nAnswer: "
                full_text = prompt + answer + self.tokenizer.eos_token
                texts.append(full_text)
            return {"text": texts}
        
        # é¢„å¤„ç†æ•°æ®
        processed_dataset = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # åˆ†è¯
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=self.config["sft"]["max_length"],
                return_tensors="pt"
            )
        
        tokenized_dataset = processed_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        
        self.logger.info(f"Dataset preparation completed: {len(tokenized_dataset)} samples")
        return tokenized_dataset
    
    def create_data_collator(self):
        """åˆ›å»ºæ•°æ®æ•´ç†å™¨"""
        return DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )
    
    def setup_training_arguments(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        return TrainingArguments(
            output_dir=self.config["sft"]["output_dir"],
            per_device_train_batch_size=self.config["sft"]["per_device_train_batch_size"],
            per_device_eval_batch_size=self.config["sft"]["per_device_eval_batch_size"],
            num_train_epochs=self.config["sft"]["num_train_epochs"],
            learning_rate=float(self.config["sft"]["learning_rate"]),
            save_strategy=self.config["sft"]["save_strategy"],
            eval_strategy=self.config["sft"].get("eval_strategy", self.config["sft"].get("evaluation_strategy", "epoch")),
            logging_steps=self.config["sft"]["logging_steps"],
            save_total_limit=self.config["sft"]["save_total_limit"],
            load_best_model_at_end=self.config["sft"]["load_best_model_at_end"],
            metric_for_best_model=self.config["sft"]["metric_for_best_model"],
            greater_is_better=self.config["sft"]["greater_is_better"],
            warmup_steps=self.config["sft"]["warmup_steps"],
            fp16=self.config["training"]["fp16"],
            bf16=self.config["training"].get("bf16", False),  # æ·»åŠ BF16æ”¯æŒ
            dataloader_num_workers=self.config["training"]["dataloader_num_workers"],
            remove_unused_columns=self.config["training"]["remove_unused_columns"],
            report_to="wandb" if self.config.get("logging", {}).get("use_wandb", False) else None,
        )
    
    def train(self, train_dataset: Dataset, eval_dataset: Optional[Dataset] = None):
        """å¼€å§‹è®­ç»ƒ"""
        try:
            # å‡†å¤‡æ•°æ®
            train_dataset = self.prepare_dataset(train_dataset)
            if eval_dataset is not None:
                eval_dataset = self.prepare_dataset(eval_dataset)
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            data_collator = self.create_data_collator()
            
            # è®¾ç½®è®­ç»ƒå‚æ•°
            training_args = self.setup_training_arguments()
            
            # åˆ›å»ºè®­ç»ƒå™¨
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
            )
            
            # å¼€å§‹è®­ç»ƒ
            self.logger.info("Starting SFT training...")
            self.trainer.train()
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_model(self.config["sft"]["output_dir"])
            
            self.logger.info("SFT training completed")
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}")
            raise
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            Path(save_path).mkdir(parents=True, exist_ok=True)
            
            if hasattr(self.model, 'save_pretrained'):
                # ä¿å­˜LoRAé€‚é…å™¨
                self.model.save_pretrained(save_path)
                self.tokenizer.save_pretrained(save_path)
            else:
                # ä¿å­˜å®Œæ•´æ¨¡å‹
                torch.save(self.model.state_dict(), os.path.join(save_path, "pytorch_model.bin"))
                self.tokenizer.save_pretrained(save_path)
            
            self.logger.info(f"Model saved to: {save_path}")
            
        except Exception as e:
            self.logger.error(f"Model save failed: {e}")
            raise
    
    def evaluate(self, eval_dataset: Dataset) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if self.trainer is None:
            raise ValueError("Trainer not initialized")
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        eval_dataset = self.prepare_dataset(eval_dataset)
        
        # è¯„ä¼°
        eval_results = self.trainer.evaluate(eval_dataset)
        
        self.logger.info(f"Evaluation results: {eval_results}")
        return eval_results
    
    def generate_sample(self, prompt: str, max_length: int = 256) -> str:
        """ç”Ÿæˆæ ·æœ¬"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("Model not initialized")
        
        self.model.eval()
        
        with torch.no_grad():
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config("config/training_config.yaml")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(config)
    
    # è®¾ç½®æ¨¡å‹
    trainer.setup_model()
    
    # åŠ è½½GSM8Kæ•°æ®é›†
    from datasets import load_dataset
    from data.gsm8k_processor import GSM8KProcessor
    
    print("Loading GSM8K dataset...")
    try:
        # åŠ è½½GSM8Kæ•°æ®é›†
        gsm8k_dataset = load_dataset("gsm8k", "main")
        
        # åˆ›å»ºGSM8Kå¤„ç†å™¨
        processor = GSM8KProcessor(trainer.tokenizer, max_length=config["sft"]["max_length"])
        
        # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºè®­ç»ƒæ•°æ®
        train_dataset = gsm8k_dataset["train"]
        
        # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†ï¼ˆå…¨é‡æ•°æ®ï¼‰
        eval_dataset = gsm8k_dataset["test"]
        
        print(f"Training set size: {len(train_dataset)}")
        print(f"Validation set size: {len(eval_dataset)}")
        
        # éªŒè¯æ•°æ®é›†è´¨é‡
        processor.validate_data(train_dataset, num_samples=3)
        processor.validate_data(eval_dataset, num_samples=3)
        
    except Exception as e:
        print(f"Failed to load GSM8K dataset: {e}")
        print("Cannot proceed with training, please check network connection and dependencies")
        return
    
    # è®­ç»ƒ
    trainer.train(train_dataset, eval_dataset)
    
    # è¯„ä¼°
    eval_results = trainer.evaluate(eval_dataset)
    print(f"Final evaluation results: {eval_results}")
    
    # ç”Ÿæˆæ ·æœ¬
    sample_questions = [
        "Question: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many meters does he run a week?\nAnswer: ",
        "Question: A robe takes 2 bolts of blue fabric and half that much white fabric. How many bolts of fabric does it take?\nAnswer: ",
        "Question: Josh decides to try flipping a house. He buys it for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How much profit did he make?\nAnswer: "
    ]
    
    print("\n=== Sample Generation Test ===")
    for i, sample_prompt in enumerate(sample_questions, 1):
        print(f"\nSample {i}:")
        print(f"Question: {sample_prompt}")
        sample_response = trainer.generate_sample(sample_prompt, max_length=200)
        print(f"Generated Answer: {sample_response}")
        print("-" * 50)


if __name__ == "__main__":
    main()





