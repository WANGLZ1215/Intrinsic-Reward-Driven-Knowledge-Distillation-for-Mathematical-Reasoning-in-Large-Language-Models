"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
åŠŸèƒ½ï¼šå®ç°åŸºäºå†…åœ¨å¥–åŠ±çš„PPOè®­ç»ƒ
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
import yaml
import logging
import os
import gc
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
import numpy as np
import wandb
from collections import deque
from tqdm import tqdm
import time

from models.teacher_model import TeacherModel
from models.student_model import StudentModel
from models.cache_manager import CacheManager
from rewards.intrinsic_reward import IntrinsicRewardComputer
from rewards.reward_normalizer import RewardNormalizer
from rewards.reward_combiner import RewardCombiner
from data.gsm8k_processor import GSM8KProcessor
from utils.math_utils import extract_final_answer, is_answer_correct
from training.ppo_utils import (
    ParallelRewardProcessor, ParallelModelInference, 
    AsyncCacheManager, ParallelDataLoader,
    create_parallel_processor, create_parallel_inference, create_async_cache_manager,
    compute_grad_norm  # ç”¨äºæ£€æŸ¥æ¢¯åº¦
)
import functools
from utils.cache_utils import suppress_past_key_values_warning, update_model_for_modern_cache


def handle_errors(func):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            # è·å–loggerå®ä¾‹
            if hasattr(args[0], 'logger'):
                logger = args[0].logger
            else:
                logger = logging.getLogger(__name__)
            
            logger.error(f"âŒ å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡æ–°æŠ›å‡º
            if isinstance(e, (ValueError, TypeError, KeyError)):
                raise
            else:
                logger.error(f"âŒ æœªçŸ¥é”™è¯¯ç±»å‹ï¼Œç»§ç»­æ‰§è¡Œ")
                return None
    
    return wrapper


def validate_data_batch(batch: Dict) -> bool:
    """éªŒè¯æ•°æ®æ‰¹æ¬¡"""
    required_keys = ["input_ids", "attention_mask"]
    
    for key in required_keys:
        if key not in batch:
            print(f"âŒ ç¼ºå°‘å¿…éœ€çš„æ‰¹æ¬¡é”®: {key}")
            return False
        
        if not isinstance(batch[key], torch.Tensor):
            print(f"âŒ æ‰¹æ¬¡é”® {key} ä¸æ˜¯å¼ é‡")
            return False
        
        if batch[key].numel() == 0:
            print(f"âŒ æ‰¹æ¬¡é”® {key} ä¸ºç©º")
            return False
    
    # æ£€æŸ¥æ‰¹æ¬¡å¤§å°ä¸€è‡´æ€§
    batch_size = batch["input_ids"].shape[0]
    for key in required_keys:
        if batch[key].shape[0] != batch_size:
            print(f"âŒ æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´: {key}")
            return False
    
    return True


def validate_config(config: Dict) -> Dict:
    """éªŒè¯å’Œè¡¥å……é…ç½®"""
    # é»˜è®¤é…ç½®
    default_config = {
        "model": {
            "teacher_model_name": "Qwen/Qwen2.5-32B-Instruct",
            "student_model_name": "Qwen/Qwen2.5-7B-Instruct",
            "cache_size": 10000,
            "cache_policy": "LRU",
            "use_lora": True
        },
        "device": {
            "device_map": "auto",
            "torch_dtype": "bfloat16"
        },
        "reward": {
            "temperature": 1.0,
            "normalization": "mean_std",
            "lambda_intrinsic": 0.7,
            "lambda_correctness": 0.3,
            "update_rate": 0.01,
            "clip_min": -5.0,
            "clip_max": 5.0,
            "use_adaptive_weights": True,
            "adaptation_rate": 0.01,
            "reasoning_weight": 0.0,
            "format_weight": 0.0
        },
        "ppo": {
            "learning_rate": 1e-5,
            "batch_size": 8,
            "mini_batch_size": 4,
            "ppo_epochs": 4,
            "clip_ratio": 0.2,
            "value_loss_coef": 0.1,
            "entropy_coef": 0.01,
            "kl_coef": 0.05,
            "gamma": 0.99,
            "lambda_gae": 0.95,
            "max_grad_norm": 1.0,
            "max_length": 512,
            "temperature": 0.7,
            "do_sample": True,
            "output_dir": "./checkpoints/rl_model"
        },
        "training": {
            "max_steps": 1000,
            "save_steps": 50,
            "eval_steps": 100,
            "logging_steps": 10
        },
        "parallel": {
            "enabled": True,
            "num_workers": 4,
            "use_threads": True,
            "inference_batch_size": 16,
            "cache_queue_size": 1000,
            "use_parallel_data_loader": True,
            "data_loader_workers": 4
        },
        "logging": {
            "use_wandb": False,
            "wandb_project": "intrinsic-reward-distillation",
            "use_tensorboard": True,
            "tensorboard_log_dir": "./logs"
        }
    }
    
    # é€’å½’åˆå¹¶é…ç½®
    def merge_config(base, override):
        for key, value in override.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                merge_config(base[key], value)
            else:
                base[key] = value
        return base
    
    return merge_config(default_config, config)


class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–RLè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = validate_config(config)  # éªŒè¯å’Œè¡¥å……é…ç½®
        
        # æŠ‘åˆ¶past_key_valuesè­¦å‘Š
        suppress_past_key_values_warning()
        
        self.teacher_model = None
        self.student_model = None
        self.ppo_model = None  # PPOæ¨¡å‹ï¼ˆå¸¦ValueHeadï¼‰
        self.ppo_trainer = None
        self.cache_manager = None
        
        # å¥–åŠ±è®¡ç®—ç»„ä»¶
        self.intrinsic_computer = None
        self.reward_normalizer = None
        self.reward_combiner = None
        
        # æ•°æ®å¤„ç†å™¨
        self.data_processor = None
        
        # å¹¶è¡Œå¤„ç†ç»„ä»¶
        self.parallel_processor = None
        self.parallel_inference_student = None
        self.parallel_inference_teacher = None
        self.async_cache_manager = None
        self.parallel_data_loader = None
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            "step": 0,
            "total_rewards": [],
            "intrinsic_rewards": [],
            "correctness_rewards": [],
            "combined_rewards": [],
            "policy_losses": [],
            "value_losses": [],
            "kl_divergences": []
        }
        
        # å†…å­˜ç®¡ç† - æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´æ¸…ç†é¢‘ç‡
        # å¯¹äºH200 140GBç­‰å¤§æ˜¾å­˜GPUï¼Œå¯ä»¥å‡å°‘æ¸…ç†é¢‘ç‡ä»¥æå‡é€Ÿåº¦
        # æ³¨æ„ï¼šåˆå§‹åŒ–æ—¶CUDAå¯èƒ½æœªå°±ç»ªï¼Œå…ˆè®¾ç½®é»˜è®¤å€¼
        self._memory_cleanup_interval = 3
        self._force_cleanup_every_n_steps = 2
        self._last_cleanup_step = 0
        self._vram_detected = False  # æ ‡è®°æ˜¯å¦å·²æ£€æµ‹VRAM
        
        # æ€§èƒ½ä¼˜åŒ–
        self._use_mixed_precision = self.config.get("device", {}).get("use_mixed_precision", True)
        self._gradient_accumulation_steps = self.config.get("training", {}).get("gradient_accumulation_steps", 1)
        self._gradient_accumulation_count = 0
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # ç¼“å­˜å¹¶è¡Œé…ç½®æ£€æŸ¥ç»“æœ
        # æ³¨æ„ï¼šé»˜è®¤ç¦ç”¨å¹¶è¡Œå¤„ç†ï¼Œé¿å…tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜
        self._use_parallel = self.config.get("parallel", {}).get("enabled", False)
    
    def _create_progress_bar(self, iterable, desc: str, total: int = None, unit: str = "sample"):
        """åˆ›å»ºæ ‡å‡†åŒ–çš„è¿›åº¦æ¡"""
        return tqdm(
            iterable,
            total=total or len(iterable) if hasattr(iterable, '__len__') else None,
            desc=desc,
            unit=unit,
            ncols=80,
            leave=False
        )
    
    def _cleanup_memory(self, step: int, force: bool = False):
        """æ¸…ç†å†…å­˜"""
        should_cleanup = force or (step - self._last_cleanup_step >= self._memory_cleanup_interval)
        
        if should_cleanup:
            # æ¸…ç†PyTorchç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¸…ç†Pythonåƒåœ¾å›æ”¶
            gc.collect()
            
            # æ¸…ç†è®­ç»ƒç»Ÿè®¡ï¼ˆä¿ç•™æœ€è¿‘çš„æ•°æ®ï¼‰
            for key in ["total_rewards", "intrinsic_rewards", "correctness_rewards", 
                       "combined_rewards", "policy_losses", "value_losses", "kl_divergences"]:
                if len(self.training_stats[key]) > 500:
                    self.training_stats[key] = self.training_stats[key][-500:]
            
            # è®°å½•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
            if self.cache_manager:
                cache_stats = self.cache_manager.get_stats()
                self.logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡(CacheManager): å‘½ä¸­ç‡={cache_stats['hit_rate']:.3f}, "
                               f"å¤§å°={cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
            
            # âœ… ä¿®å¤ï¼šTeacheræ¨¡å‹å†…éƒ¨ä¹Ÿæœ‰ç¼“å­˜ï¼Œè®°å½•å…¶ç»Ÿè®¡ä¿¡æ¯
            if self.teacher_model and hasattr(self.teacher_model, 'get_cache_stats'):
                teacher_cache_stats = self.teacher_model.get_cache_stats()
                self.logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡(Teacherå†…éƒ¨): å‘½ä¸­ç‡={teacher_cache_stats['hit_rate']:.3f}, "
                               f"å¤§å°={teacher_cache_stats['cache_size']}/{teacher_cache_stats['max_cache_size']}")
            
            self._last_cleanup_step = step
            if not force:  # å¼ºåˆ¶æ¸…ç†æ—¶ä¸æ‰“å°æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                self.logger.info(f"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ (step {step})")
    
    def cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        try:
            # æ¸…ç†æ¨¡å‹
            if self.teacher_model:
                del self.teacher_model
                self.teacher_model = None
            
            if self.student_model:
                del self.student_model
                self.student_model = None
            
            if self.ppo_trainer:
                del self.ppo_trainer
                self.ppo_trainer = None
            
            # æ¸…ç†ç¼“å­˜
            if self.cache_manager:
                self.cache_manager.clear()  # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
                del self.cache_manager
                self.cache_manager = None
            
            # æ¸…ç†å¹¶è¡Œç»„ä»¶
            if self.parallel_processor:
                del self.parallel_processor
                self.parallel_processor = None
            
            if self.parallel_inference_student:
                del self.parallel_inference_student
                self.parallel_inference_student = None
            
            if self.parallel_inference_teacher:
                del self.parallel_inference_teacher
                self.parallel_inference_teacher = None
            
            if self.async_cache_manager:
                del self.async_cache_manager
                self.async_cache_manager = None
            
            if self.parallel_data_loader:
                del self.parallel_data_loader
                self.parallel_data_loader = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("ğŸ§¹ æ‰€æœ‰èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")
            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        # åˆå§‹åŒ–wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get("logging", {}).get("use_wandb", False):
            wandb.init(
                project=self.config["logging"]["wandb_project"],
                config=self.config
            )
    
    def setup_models(self):
        """è®¾ç½®æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ @handle_errorsï¼Œå› ä¸ºéœ€è¦ç¡®ä¿å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯è¿”å› None
        try:
            self.logger.info("ğŸš€ å¼€å§‹è®¾ç½®æ¨¡å‹...")
            
            # æ£€æŸ¥GPUæ•°é‡å¹¶å†³å®šæ¨¡å‹åˆ†é…ç­–ç•¥
            num_gpus = torch.cuda.device_count()
            self.logger.info(f"ğŸ“Š æ£€æµ‹åˆ° {num_gpus} ä¸ªGPUè®¾å¤‡")
            
            # ğŸ¯ æ£€æµ‹GPUæ˜¾å­˜å¤§å°å¹¶è°ƒæ•´æ¸…ç†é¢‘ç‡ï¼ˆH200ä¼˜åŒ–ï¼‰
            if num_gpus >= 1 and not self._vram_detected:
                try:
                    total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    self.logger.info(f"ğŸ“Š GPU 0æ˜¾å­˜: {total_vram_gb:.1f}GB")
                    if total_vram_gb >= 120:  # H200 140GBæˆ–ç±»ä¼¼å¤§æ˜¾å­˜GPU
                        self._memory_cleanup_interval = 5  # ğŸ”¥ ç´§æ€¥ä¿®å¤ï¼šä»50é™åˆ°5ï¼Œæ›´é¢‘ç¹æ¸…ç†
                        self._force_cleanup_every_n_steps = 3  # ğŸ”¥ ç´§æ€¥ä¿®å¤ï¼šä»50é™åˆ°3
                        self.logger.info("âš ï¸ æ£€æµ‹åˆ°H200ç­‰å¤§æ˜¾å­˜GPUï¼Œä½†ä½¿ç”¨ä¿å®ˆæ¸…ç†ç­–ç•¥ï¼ˆæ¯5æ­¥æ¸…ç†ï¼‰é¿å…OOM")
                    else:  # A100 80GBæˆ–æ›´å°æ˜¾å­˜
                        self._memory_cleanup_interval = 3
                        self._force_cleanup_every_n_steps = 2
                        self.logger.info(f"ğŸ“Š ä½¿ç”¨ä¿å®ˆæ¸…ç†ç­–ç•¥ï¼ˆæ¯3æ­¥æ¸…ç†ï¼‰")
                    self._vram_detected = True
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ£€æµ‹GPUæ˜¾å­˜å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {e}")
            
            # æ ¹æ®GPUæ•°é‡é€‰æ‹©æœ€ä¼˜åˆ†é…ç­–ç•¥
            if num_gpus >= 4:
                # 4å¡æˆ–æ›´å¤šï¼šTeacherè·¨GPU 0,1è‡ªåŠ¨åˆ†å¸ƒï¼ŒStudentåœ¨GPU 2ï¼ŒGPU 3å¤‡ç”¨/ç¼“å­˜
                # ä½¿ç”¨max_memoryé™åˆ¶Teacheræ¨¡å‹åªåœ¨GPU 0å’Œ1ä¸Šåˆ†é…
                # è¿™æ ·å¯ä»¥ç¡®ä¿Studentæ¨¡å‹å¯ä»¥å®‰å…¨ä½¿ç”¨GPU 2
                import os
                # è®¾ç½®max_memoryé™åˆ¶ï¼Œåªå…è®¸åœ¨GPU 0å’Œ1ä¸Šåˆ†é…æ¨¡å‹
                max_memory = {
                    0: "75GB",  # GPU 0: é¢„ç•™5GBç³»ç»Ÿæ˜¾å­˜
                    1: "75GB",  # GPU 1: é¢„ç•™5GBç³»ç»Ÿæ˜¾å­˜
                }
                teacher_device_map = "auto"  # é…åˆmax_memoryä½¿ç”¨ï¼Œè‡ªåŠ¨åˆ†é…åˆ°GPU 0å’Œ1
                # ä¸´æ—¶è®¾ç½®max_memoryç¯å¢ƒå˜é‡ï¼ˆå¦‚æœHuggingFaceæ”¯æŒï¼‰
                # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦åœ¨from_pretrainedä¸­ä¼ å…¥max_memoryå‚æ•°
                student_device_map = "cuda:2"  # Studentæ¨¡å‹ï¼ˆ7Bï¼‰æ”¾åœ¨GPU 2
                self.logger.info("âœ… 4å¡é…ç½®ï¼šTeacherâ†’GPU 0,1ï¼ˆè‡ªåŠ¨å¹³è¡¡åˆ†å¸ƒï¼‰ï¼ŒStudentâ†’GPU 2ï¼ŒGPU 3å¤‡ç”¨/ç¼“å­˜")
                self.logger.info("   æ˜¾å­˜åˆ†é…ï¼šTeacher(32B)çº¦70GBï¼ŒStudent(7B+PPO)çº¦40-50GBï¼Œå‰©ä½™~200GBå®‰å…¨ä½™é‡")
                # å­˜å‚¨max_memoryä¾›åç»­ä½¿ç”¨
                self._teacher_max_memory = max_memory
            elif num_gpus >= 2:
                # 2å¡ï¼šTeacherå•å¡GPU 1ï¼ŒStudentå•å¡GPU 0
                # ğŸ¯ æ³¨æ„ï¼šæ¨¡å‹å¹¶è¡Œï¼ˆè·¨GPUï¼‰ä¼šå¢åŠ é€šä¿¡å¼€é”€å’Œå»¶è¿Ÿ
                # å•å¡140GB H200è¶³å¤Ÿè£…ä¸‹Teacher 32Bï¼ˆ64GBï¼‰+ Student 7Bï¼ˆ14GBï¼‰
                teacher_device_map = "cuda:1"  # Teacher 32Bå•å¡GPU 1
                student_device_map = "cuda:0"  # Student 7B + PPOå•å¡GPU 0
                self.logger.info("âœ… 2å¡é…ç½®ï¼šStudent+PPOâ†’GPU 0, Teacherâ†’GPU 1")
                self.logger.info("   æ˜¾å­˜ï¼šGPU 0çº¦50GBï¼ŒGPU 1çº¦70GBï¼Œéƒ½å……è¶³")
                self.logger.info("   ğŸ’¡ å¦‚é‡OOMï¼Œå¯èƒ½æ˜¯batch_sizeå¤ªå¤§ï¼Œè€Œéå•å¡å®¹é‡ä¸è¶³")
            else:
                # å¦‚æœåªæœ‰ä¸€ä¸ªGPUï¼Œä½¿ç”¨auto
                teacher_device_map = self.config["device"]["device_map"]
                student_device_map = self.config["device"]["device_map"]
                self.logger.warning("âš ï¸ åªæœ‰ä¸€ä¸ªGPUï¼Œä½¿ç”¨autoåˆ†é…")
            
            # åŠ è½½æ•™å¸ˆæ¨¡å‹
            self.logger.info("ğŸ“š åŠ è½½æ•™å¸ˆæ¨¡å‹...")
            with tqdm(total=3, desc="æ•™å¸ˆæ¨¡å‹åŠ è½½", ncols=80) as pbar:
                from models.teacher_model import TeacherModel
                # å‡†å¤‡Teacheræ¨¡å‹åˆå§‹åŒ–å‚æ•°
                teacher_kwargs = {
                    "model_name": self.config["model"]["teacher_model_name"],
                    "cache_size": self.config["model"]["cache_size"],
                    "cache_policy": self.config["model"]["cache_policy"],
                    "device": teacher_device_map,  # ä½¿ç”¨æ˜¾å¼åˆ†é…çš„GPU
                    "torch_dtype": getattr(torch, self.config["device"]["torch_dtype"])
                }
                # å¦‚æœæ˜¯4å¡é…ç½®ï¼Œä¼ é€’max_memoryé™åˆ¶
                if num_gpus >= 4 and hasattr(self, '_teacher_max_memory'):
                    teacher_kwargs["max_memory"] = self._teacher_max_memory
                
                self.teacher_model = TeacherModel(**teacher_kwargs)
                pbar.update(1)
                pbar.set_postfix({"status": "æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ"})
                
                # æ£€æŸ¥Teacheræ¨¡å‹å®é™…åˆ†å¸ƒ
                if hasattr(self.teacher_model.model, 'hf_device_map'):
                    device_map = self.teacher_model.model.hf_device_map
                    self.logger.info(f"ğŸ“Š Teacheræ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {device_map}")
                    
                    # âš ï¸ æ£€æŸ¥è®¾å¤‡åˆ†é…æ˜¯å¦å¹³è¡¡ï¼ˆä»…å¯¹4å¡é…ç½®ï¼‰
                    if num_gpus >= 4 and isinstance(device_map, dict):
                        gpu_0_layers = sum(1 for v in device_map.values() if v == 0 or (isinstance(v, (list, tuple)) and 0 in v))
                        gpu_1_layers = sum(1 for v in device_map.values() if v == 1 or (isinstance(v, (list, tuple)) and 1 in v))
                        total_layers = gpu_0_layers + gpu_1_layers
                        if total_layers > 0:
                            balance_ratio = min(gpu_0_layers, gpu_1_layers) / max(gpu_0_layers, gpu_1_layers)
                            if balance_ratio < 0.7:  # å¦‚æœåˆ†é…ä¸å¹³è¡¡åº¦è¶…è¿‡30%
                                self.logger.warning(f"âš ï¸ Teacheræ¨¡å‹è®¾å¤‡åˆ†é…ä¸å¹³è¡¡ï¼šGPU 0æœ‰{gpu_0_layers}å±‚ï¼ŒGPU 1æœ‰{gpu_1_layers}å±‚ï¼ˆå¹³è¡¡åº¦{balance_ratio:.2%}ï¼‰")
                                self.logger.warning("   å»ºè®®æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´device_map")
                
                # åŠ è½½å­¦ç”Ÿæ¨¡å‹
                self.logger.info("ğŸ“ åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
                from models.student_model import StudentModel
                self.student_model = StudentModel(
                    model_name=self.config["model"]["student_model_name"],
                    lora_config=self.config["lora"],
                    device=student_device_map,  # ä½¿ç”¨æ˜¾å¼åˆ†é…çš„GPU
                    torch_dtype=getattr(torch, self.config["device"]["torch_dtype"]),
                    use_lora=True
                )
                pbar.update(1)
                pbar.set_postfix({"status": "å­¦ç”Ÿæ¨¡å‹åŠ è½½å®Œæˆ"})
                
                # ğŸ”¥ å…³é”®éªŒè¯ï¼šç¡®ä¿teacherå’Œstudentä½¿ç”¨ç›¸åŒtokenizeræˆ–ç›¸åŒå¤§å°
                teacher_tok_size = len(self.teacher_model.tokenizer)
                student_tok_size = len(self.student_model.tokenizer)
                if teacher_tok_size != student_tok_size:
                    self.logger.warning(f"âš ï¸ Teacher tokenizerå¤§å° ({teacher_tok_size}) != Student tokenizerå¤§å° ({student_tok_size})")
                    self.logger.warning(f"   è¿™å¯èƒ½å¯¼è‡´vocab_sizeä¸åŒ¹é…é—®é¢˜ï¼Œå·²å¯ç”¨çš„'é™åŸŸ+clamp'ç­–ç•¥ä¼šä¿æŠ¤")
                else:
                    self.logger.info(f"âœ… Teacherå’ŒStudent tokenizerå¤§å°ä¸€è‡´: {teacher_tok_size}")
                
                # ğŸ”¥ å…³é”®éªŒè¯ï¼šæ£€æŸ¥çœŸå®embeddingå¤§å°
                try:
                    teacher_input_emb = self.teacher_model.model.get_input_embeddings().weight.size(0)
                    student_input_emb = self.student_model.model.get_input_embeddings().weight.size(0)
                    self.logger.info(f"ğŸ“Š çœŸå®embeddingå¤§å°:")
                    self.logger.info(f"   Teacher input_embeddings: {teacher_input_emb}")
                    self.logger.info(f"   Student input_embeddings: {student_input_emb}")
                    self.logger.info(f"   Teacher tokenizer: {teacher_tok_size}")
                    self.logger.info(f"   Student tokenizer: {student_tok_size}")
                    
                    if teacher_input_emb != teacher_tok_size:
                        self.logger.warning(f"âš ï¸ Teacher embedding ({teacher_input_emb}) != tokenizer ({teacher_tok_size})")
                    if student_input_emb != student_tok_size:
                        self.logger.warning(f"âš ï¸ Student embedding ({student_input_emb}) != tokenizer ({student_tok_size})")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ— æ³•æ£€æŸ¥embeddingå¤§å°: {e}")
                
                # æ£€æŸ¥Studentæ¨¡å‹å®é™…åˆ†å¸ƒ
                if hasattr(self.student_model.model, 'hf_device_map'):
                    self.logger.info(f"ğŸ“Š Studentæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {self.student_model.model.hf_device_map}")
                
                # è®¾ç½®PPOæ¨¡å‹
                self.logger.info("âš™ï¸ è®¾ç½®PPOæ¨¡å‹...")
                self.ppo_model = self.student_model.setup_for_ppo()
                pbar.update(1)
                pbar.set_postfix({"status": "PPOæ¨¡å‹è®¾ç½®å®Œæˆ"})
            
            # æ›´æ–°æ¨¡å‹ä»¥ä½¿ç”¨ç°ä»£ç¼“å­˜
            self.teacher_model.model = update_model_for_modern_cache(self.teacher_model.model)
            self.student_model.model = update_model_for_modern_cache(self.student_model.model)
            
            self.logger.info("âœ… æ¨¡å‹è®¾ç½®å®Œæˆ")
            self.logger.info("æ¨¡å‹è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def setup_components(self):
        """è®¾ç½®å¥–åŠ±è®¡ç®—ç»„ä»¶"""
        try:
            print("ğŸ”§ å¼€å§‹è®¾ç½®ç»„ä»¶...")
            
            with tqdm(total=6, desc="ç»„ä»¶è®¾ç½®", ncols=80) as pbar:
                # ç¼“å­˜ç®¡ç†å™¨
                print("ğŸ’¾ è®¾ç½®ç¼“å­˜ç®¡ç†å™¨...")
                from models.cache_manager import CacheManager
                self.cache_manager = CacheManager(
                    max_cache_size=self.config["model"]["cache_size"],
                    eviction_policy=self.config["model"]["cache_policy"]
                )
                pbar.update(1)
                pbar.set_postfix({"status": "ç¼“å­˜ç®¡ç†å™¨è®¾ç½®å®Œæˆ"})
                
                # å†…åœ¨å¥–åŠ±è®¡ç®—å™¨
                print("ğŸ§  è®¾ç½®å†…åœ¨å¥–åŠ±è®¡ç®—å™¨...")
                self.intrinsic_computer = IntrinsicRewardComputer(
                    temperature=self.config["reward"]["temperature"],
                    normalization_method=self.config["reward"]["normalization"],
                    update_rate=self.config["reward"].get("update_rate", 0.01)
                )
                pbar.update(1)
                pbar.set_postfix({"status": "å†…åœ¨å¥–åŠ±è®¡ç®—å™¨è®¾ç½®å®Œæˆ"})
                
                # å¥–åŠ±å½’ä¸€åŒ–å™¨
                print("ğŸ“Š è®¾ç½®å¥–åŠ±å½’ä¸€åŒ–å™¨...")
                self.reward_normalizer = RewardNormalizer(
                    method=self.config["reward"]["normalization"],
                    clip_min=self.config["reward"].get("clip_min", -5.0),
                    clip_max=self.config["reward"].get("clip_max", 5.0)
                )
                pbar.update(1)
                pbar.set_postfix({"status": "å¥–åŠ±å½’ä¸€åŒ–å™¨è®¾ç½®å®Œæˆ"})
                
                # å¥–åŠ±ç»„åˆå™¨
                print("ğŸ”— è®¾ç½®å¥–åŠ±ç»„åˆå™¨...")
                self.reward_combiner = RewardCombiner(
                    lambda_intrinsic=self.config["reward"]["lambda_intrinsic"],
                    lambda_correctness=self.config["reward"]["lambda_correctness"],
                    lambda_reasoning=self.config["reward"].get("reasoning_weight", 0.0),
                    lambda_format=self.config["reward"].get("format_weight", 0.0),
                    use_adaptive_weights=self.config["reward"].get("use_adaptive_weights", True),
                    adaptation_rate=self.config["reward"].get("adaptation_rate", 0.01)
                )
                pbar.update(1)
                pbar.set_postfix({"status": "å¥–åŠ±ç»„åˆå™¨è®¾ç½®å®Œæˆ"})
                
                # è®°å½•è‡ªé€‚åº”æƒé‡çŠ¶æ€
                if self.config["reward"].get("use_adaptive_weights", True):
                    self.logger.info("è‡ªé€‚åº”æƒé‡åŠŸèƒ½å·²å¯ç”¨")
                    self.logger.info(f"åˆå§‹æƒé‡ - å†…åœ¨: {self.config['reward']['lambda_intrinsic']}, "
                                   f"æ­£ç¡®æ€§: {self.config['reward']['lambda_correctness']}, "
                                   f"æ¨ç†: {self.config['reward'].get('reasoning_weight', 0.0)}, "
                                   f"æ ¼å¼: {self.config['reward'].get('format_weight', 0.0)}")
                    self.logger.info(f"æƒé‡é€‚åº”ç‡: {self.config['reward'].get('adaptation_rate', 0.01)}")
                else:
                    self.logger.info("è‡ªé€‚åº”æƒé‡åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨å›ºå®šæƒé‡")
                
                # æ•°æ®å¤„ç†å™¨
                print("ğŸ“ è®¾ç½®æ•°æ®å¤„ç†å™¨...")
                if self.student_model is None or self.student_model.tokenizer is None:
                    raise ValueError("student_model æˆ– tokenizer æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
                
                self.data_processor = GSM8KProcessor(
                    tokenizer=self.student_model.tokenizer,
                    max_length=self.config["ppo"]["max_length"]
                )
                pbar.update(1)
                pbar.set_postfix({"status": "æ•°æ®å¤„ç†å™¨è®¾ç½®å®Œæˆ"})
                
                # åˆå§‹åŒ–å¹¶è¡Œå¤„ç†ç»„ä»¶
                print("âš¡ è®¾ç½®å¹¶è¡Œå¤„ç†ç»„ä»¶...")
                self._setup_parallel_components()
                pbar.update(1)
                pbar.set_postfix({"status": "å¹¶è¡Œå¤„ç†ç»„ä»¶è®¾ç½®å®Œæˆ"})
            
            print("âœ… ç»„ä»¶è®¾ç½®å®Œæˆ")
            self.logger.info("ç»„ä»¶è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_parallel_components(self):
        """è®¾ç½®å¹¶è¡Œå¤„ç†ç»„ä»¶"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            use_parallel = self._use_parallel
            if not use_parallel:
                self.logger.info("å¹¶è¡Œå¤„ç†å·²ç¦ç”¨")
                return
            
            # å¹¶è¡Œå¥–åŠ±å¤„ç†å™¨
            self.parallel_processor = create_parallel_processor(self.config)
            
            # å¹¶è¡Œæ¨¡å‹æ¨ç†å™¨
            self.parallel_inference_student = create_parallel_inference(
                self.student_model, self.config
            )
            self.parallel_inference_teacher = create_parallel_inference(
                self.teacher_model, self.config
            )
            
            # å¼‚æ­¥ç¼“å­˜ç®¡ç†å™¨
            self.async_cache_manager = create_async_cache_manager(
                self.cache_manager, self.config
            )
            
            # å¯åŠ¨å¼‚æ­¥ç¼“å­˜å·¥ä½œçº¿ç¨‹
            self.async_cache_manager.start_async_worker()
            
            self.logger.info("å¹¶è¡Œå¤„ç†ç»„ä»¶è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œå¤„ç†ç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            # å¦‚æœå¹¶è¡Œå¤„ç†è®¾ç½®å¤±è´¥ï¼Œå›é€€åˆ°ä¸²è¡Œå¤„ç†
            self.logger.warning("å›é€€åˆ°ä¸²è¡Œå¤„ç†æ¨¡å¼")
    
    def setup_ppo_trainer(self):
        """è®¾ç½®PPOè®­ç»ƒå™¨"""
        try:
            # æ£€æŸ¥ppo_modelæ˜¯å¦å·²è®¾ç½®
            if self.ppo_model is None:
                raise ValueError("ppo_model æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
            
            # æ£€æŸ¥student_modelå’Œtokenizeræ˜¯å¦å·²è®¾ç½®
            if self.student_model is None or self.student_model.tokenizer is None:
                raise ValueError("student_model æˆ– tokenizer æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
            
            from inspect import signature

            raw = dict(
                model_name=self.config["model"]["student_model_name"],
                learning_rate=float(self.config["ppo"]["learning_rate"]),
                batch_size=self.config["ppo"]["batch_size"],
                mini_batch_size=self.config["ppo"]["mini_batch_size"],
                ppo_epochs=self.config["ppo"]["ppo_epochs"],
                # å…¼å®¹ clip_range / clip_ratio
                clip_range=self.config["ppo"].get("clip_range", self.config["ppo"].get("clip_ratio", 0.2)),
                value_loss_coef=self.config["ppo"].get("value_loss_coef", 0.5),
                entropy_beta=self.config["ppo"].get("entropy_coef", 0.0),  # å…¼å®¹ entropy_coef
                kl_coef=self.config["ppo"].get("kl_coef", 0.01),
                gamma=self.config["ppo"].get("gamma", 0.99),
                lambda_gae=self.config["ppo"].get("lambda_gae", 0.95),
                max_grad_norm=self.config["ppo"].get("max_grad_norm", 1.0),
                # âœ… æ·»åŠ  forward_batch_size é…ç½®ï¼Œé™ä½å‰å‘æ‰¹å¤„ç†å³°å€¼æ˜¾å­˜
                forward_batch_size=self.config["ppo"].get("forward_batch_size", 2),
                # âœ… æ·»åŠ ç”Ÿæˆå‚æ•°ï¼Œç¡®ä¿KLæ•£åº¦è®¡ç®—æ­£ç¡®
                temperature=self.config["ppo"].get("temperature", 0.7),
                top_k=self.config["ppo"].get("top_k", 50),
                top_p=self.config["ppo"].get("top_p", 1.0),
                log_with="wandb" if self.config.get("logging", {}).get("use_wandb", False) else None,
                tracker_project_name=self.config.get("logging", {}).get("wandb_project", "intrinsic-reward-distillation"),
            )

            # åªä¿ç•™ PPOConfig.__init__ çœŸæ­£æ”¯æŒçš„é”®
            allowed = set(signature(PPOConfig.__init__).parameters.keys())
            filtered_raw = {k: v for k, v in raw.items() if k in allowed}
            
            # è°ƒè¯•ï¼šæ‰“å°è¢«è¿‡æ»¤æ‰çš„é”®
            filtered_out = {k: v for k, v in raw.items() if k not in allowed}
            if filtered_out:
                self.logger.warning(f"âš ï¸ PPOConfigä¸æ”¯æŒçš„å‚æ•°ï¼ˆå°†è¢«å¿½ç•¥ï¼‰: {filtered_out.keys()}")
            
            ppo_config = PPOConfig(**filtered_raw)
            
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸åœ¨è¿™é‡Œä¼ å…¥ datasetï¼Œå› ä¸ºæ•°æ®æ˜¯åœ¨è®­ç»ƒå¾ªç¯ä¸­åŠ¨æ€åˆ›å»ºçš„
            # æ˜¾å¼è®¾ç½® dataset=None ä»¥é¿å…è­¦å‘Š
            self.ppo_trainer = PPOTrainer(
                config=ppo_config,
                model=self.ppo_model,
                tokenizer=self.student_model.tokenizer,
                dataset=None  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®åŠ è½½é€»è¾‘ï¼Œä¸ä¼ å…¥æ•°æ®é›†
            )
            
            # ğŸ” è¯Šæ–­ï¼šæ£€æŸ¥ref_modelè®¾ç½®
            if hasattr(self.ppo_trainer, 'ref_model'):
                self.logger.info("âœ… PPO traineræœ‰ref_model")
                # æ£€æŸ¥ref_modelå‚æ•°æ˜¯å¦å†»ç»“
                if hasattr(self.ppo_trainer.ref_model, 'parameters'):
                    frozen_params = sum(1 for p in self.ppo_trainer.ref_model.parameters() if not p.requires_grad)
                    total_params = sum(1 for _ in self.ppo_trainer.ref_model.parameters())
                    self.logger.info(f"   Ref modelå†»ç»“å‚æ•°: {frozen_params}/{total_params}")
            else:
                self.logger.error("âŒ PPO traineræ²¡æœ‰ref_modelï¼è¿™æ˜¯å¯¼è‡´KL=0çš„æ ¹æºï¼")
            
            # âœ… åº”ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼šåœ¨PPO traineråˆå§‹åŒ–åå¯ç”¨
            # æ³¨æ„ï¼šgradient_checkpointingå·²ç»åœ¨setup_for_ppoä¸­å¯ç”¨ï¼Œä½†ç¡®ä¿PPO trainerä¹Ÿåº”ç”¨
            if self.config["ppo"].get("gradient_checkpointing", False):
                try:
                    # ç¡®ä¿PPO trainerçš„æ¨¡å‹ä¹Ÿå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹
                    if hasattr(self.ppo_trainer, 'model') and hasattr(self.ppo_trainer.model, 'gradient_checkpointing_enable'):
                        self.ppo_trainer.model.gradient_checkpointing_enable()
                        self.logger.info("âœ… PPO Traineræ¨¡å‹å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
                    elif hasattr(self.ppo_trainer, 'ref_model') and hasattr(self.ppo_trainer.ref_model, 'gradient_checkpointing_enable'):
                        # ç¡®ä¿ref_modelä¹Ÿå¯ç”¨ï¼ˆç”¨äºKLæ•£åº¦è®¡ç®—ï¼‰
                        self.ppo_trainer.ref_model.gradient_checkpointing_enable()
                        self.logger.info("âœ… PPO Trainer ref_modelå·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ åœ¨PPO trainerä¸­å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            
            # ğŸ“Š GPUåˆ†é…è¯´æ˜ï¼š
            # - Teacheræ¨¡å‹ï¼šè‡ªåŠ¨åˆ†å¸ƒåœ¨GPU 0å’Œ1ï¼ˆå·²è®¾ç½®max_memoryï¼‰
            # - StudentåŸºç¡€æ¨¡å‹ï¼šåœ¨GPU 2ï¼ˆå·²è®¾ç½®device="cuda:2"ï¼‰
            # - PPOæ¨¡å‹ï¼ˆpolicy + refï¼‰ï¼šç”±Acceleratorè‡ªåŠ¨ç®¡ç†ï¼Œé€šå¸¸åœ¨GPU 0ï¼ˆä¸»è®¾å¤‡ï¼‰
            #   åŸå› ï¼šPPOè®­ç»ƒéœ€è¦policyå’Œrefåœ¨åŒä¸€è®¾å¤‡ï¼Œä»¥ä¾¿è®¡ç®—KLæ•£åº¦å’Œæ¢¯åº¦åŒæ­¥
            # 
            # âš ï¸ é‡è¦ï¼šPPOæ¨¡å‹ä¸ä¼šè‡ªåŠ¨åˆ†é…åˆ°ä¸åŒGPU
            # - è¿™æ˜¯PPO trainerçš„è®¾è®¡é™åˆ¶ï¼Œä¸æ˜¯bug
            # - é€šè¿‡gradient_checkpointingã€forward_batch_size=1ç­‰ä¼˜åŒ–é¿å…OOM
            # - GPU 0æ€»ä½¿ç”¨çº¦75GB < 80GBï¼Œå®‰å…¨ä½™é‡å……è¶³
            
            num_gpus = torch.cuda.device_count()
            # æ‰“å°GPUåˆ†é…æƒ…å†µï¼ˆé€‚ç”¨äºæ‰€æœ‰GPUé…ç½®ï¼‰
            try:
                self.logger.info("ğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†é…æƒ…å†µï¼š")
                
                def get_model_device(model_obj):
                    """å®‰å…¨åœ°è·å–æ¨¡å‹è®¾å¤‡"""
                    if model_obj is None:
                        return None
                    try:
                        # å°è¯•è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                        for param in model_obj.parameters():
                            return param.device
                    except:
                        pass
                    # å°è¯•ä»pretrained_modelè·å–
                    if hasattr(model_obj, 'pretrained_model'):
                        try:
                            for param in model_obj.pretrained_model.parameters():
                                return param.device
                        except:
                            pass
                    return None
                
                # æ£€æŸ¥Teacheræ¨¡å‹åˆ†å¸ƒ
                if self.teacher_model and hasattr(self.teacher_model, 'model'):
                    try:
                        teacher_params = list(self.teacher_model.model.parameters())[:5]  # æ£€æŸ¥å‰5ä¸ªå‚æ•°
                        devices = [p.device for p in teacher_params]
                        unique_devices = set(str(d) for d in devices)
                        self.logger.info(f"   Teacheræ¨¡å‹è®¾å¤‡: {', '.join(unique_devices)}")
                    except:
                        self.logger.info(f"   Teacheræ¨¡å‹è®¾å¤‡: æœªçŸ¥")
                
                # æ£€æŸ¥StudentåŸºç¡€æ¨¡å‹
                if self.student_model and hasattr(self.student_model, 'model'):
                    student_device = get_model_device(self.student_model.model)
                    if student_device:
                        self.logger.info(f"   StudentåŸºç¡€æ¨¡å‹è®¾å¤‡: {student_device}")
                
                # æ£€æŸ¥PPOæ¨¡å‹
                if hasattr(self.ppo_trainer, 'model') and self.ppo_trainer.model is not None:
                    policy_device = get_model_device(self.ppo_trainer.model)
                    if policy_device:
                        self.logger.info(f"   PPO Policyæ¨¡å‹è®¾å¤‡: {policy_device} (ç”±Acceleratorç®¡ç†)")
                    else:
                        self.logger.warning("   âš ï¸ æ— æ³•è·å–Policyæ¨¡å‹è®¾å¤‡")
                
                if hasattr(self.ppo_trainer, 'ref_model') and self.ppo_trainer.ref_model is not None:
                    ref_device = get_model_device(self.ppo_trainer.ref_model)
                    if ref_device:
                        self.logger.info(f"   PPO Refæ¨¡å‹è®¾å¤‡: {ref_device} (ç”±Acceleratorç®¡ç†)")
                    else:
                        self.logger.warning("   âš ï¸ æ— æ³•è·å–Refæ¨¡å‹è®¾å¤‡")
                
                # æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                self.logger.info("ğŸ“Š å„GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼š")
                for gpu_id in range(num_gpus):
                    allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
                    reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3
                    max_memory = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
                    usage_pct = (allocated / max_memory * 100) if max_memory > 0 else 0
                    self.logger.info(f"   GPU {gpu_id}: {allocated:.2f}GB / {max_memory:.2f}GB ({usage_pct:.1f}%)")
                    
                # æ ¹æ®GPUæ•°é‡æä¾›é…ç½®è¯´æ˜
                if num_gpus >= 4:
                    self.logger.info("ğŸ’¡ 4å¡é…ç½®è¯´æ˜ï¼š")
                    self.logger.info("   - Teacheræ¨¡å‹åˆ†å¸ƒåœ¨GPU 0,1ï¼ˆé€šè¿‡device_map='auto'ï¼‰")
                    self.logger.info("   - PPOæ¨¡å‹ï¼ˆpolicy+refï¼‰åœ¨GPU 0ï¼ˆAcceleratorè‡ªåŠ¨ç®¡ç†ï¼Œè¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼‰")
                elif num_gpus >= 2:
                    self.logger.info("ğŸ’¡ 2å¡é…ç½®è¯´æ˜ï¼š")
                    self.logger.info("   - Teacheræ¨¡å‹åœ¨GPU 1")
                    self.logger.info("   - Student+PPOæ¨¡å‹åœ¨GPU 0")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ£€æŸ¥æ¨¡å‹è®¾å¤‡åˆ†é…æ—¶å‡ºé”™: {e}")
            
            self.logger.info("PPOè®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"PPOè®­ç»ƒå™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def compute_intrinsic_rewards(self, questions: List[str], 
                                 student_responses: List[str]) -> torch.Tensor:
        """è®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        use_parallel = self._use_parallel
        
        if use_parallel and self.parallel_processor:
            return self._compute_intrinsic_rewards_parallel(questions, student_responses)
        else:
            return self._compute_intrinsic_rewards_sequential(questions, student_responses)
    
    def _compute_intrinsic_rewards_sequential(self, questions: List[str], 
                                            student_responses: List[str]) -> torch.Tensor:
        """ä¸²è¡Œè®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆåŸå§‹å®ç°ï¼‰"""
        # æ£€æŸ¥å¿…è¦çš„ç»„ä»¶
        if self.teacher_model is None:
            raise ValueError("teacher_model æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
        if self.student_model is None:
            raise ValueError("student_model æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
        if self.cache_manager is None:
            raise ValueError("cache_manager æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_components() æ–¹æ³•")
        if self.intrinsic_computer is None:
            raise ValueError("intrinsic_computer æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_components() æ–¹æ³•")
        if self.reward_normalizer is None:
            raise ValueError("reward_normalizer æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_components() æ–¹æ³•")
        
        intrinsic_rewards = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = self._create_progress_bar(
            zip(questions, student_responses), 
            desc="è®¡ç®—å†…åœ¨å¥–åŠ±"
        )
        
        for question, response in progress_bar:
            # æ„å»ºå®Œæ•´åºåˆ—
            full_sequence = question + response
            
            # ğŸ”¥ ç¦ç”¨ç¼“å­˜ï¼šç›´æ¥è®¡ç®—teacher logitsï¼ˆå› ä¸ºå‘½ä¸­ç‡ä¸€ç›´æ˜¯0%ï¼‰
            teacher_logits = self.teacher_model.get_logits(full_sequence, use_cache=False)
            
            # è·å–å­¦ç”Ÿtokensï¼ˆåœ¨å¾ªç¯ä¸­è°ƒç”¨ï¼Œä½†tokenizeræœ¬èº«å¾ˆå¿«ï¼‰
            student_tokens = self.student_model.tokenizer.encode(response, add_special_tokens=False)
            student_tokens = torch.tensor(student_tokens).unsqueeze(0)
            
            # è®¡ç®—é—®é¢˜éƒ¨åˆ†çš„é•¿åº¦
            question_tokens = self.student_model.tokenizer.encode(question, add_special_tokens=False)
            question_length = len(question_tokens)
            
            # è®¡ç®—å†…åœ¨å¥–åŠ±
            intrinsic_reward = self.intrinsic_computer.compute_intrinsic_reward(
                teacher_logits, student_tokens, question_length
            )
            
            # å½’ä¸€åŒ–
            normalized_intrinsic = self.reward_normalizer.normalize_intrinsic_rewards(
                intrinsic_reward
            )
            
            # è®¡ç®—trajectoryçº§åˆ«å¥–åŠ±
            trajectory_reward = self.intrinsic_computer.compute_trajectory_reward(
                normalized_intrinsic
            )
            
            intrinsic_rewards.append(trajectory_reward)
        
        return torch.tensor(intrinsic_rewards)
    
    def _compute_intrinsic_rewards_parallel(self, questions: List[str], 
                                          student_responses: List[str]) -> torch.Tensor:
        """å¹¶è¡Œè®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆä½¿ç”¨æ•™å¸ˆæ¨¡å‹å¹¶è¡Œæ¨ç†ï¼‰"""
        # æ„å»ºå®Œæ•´åºåˆ—åˆ—è¡¨
        full_sequences = [question + response for question, response in zip(questions, student_responses)]
        
        # ä½¿ç”¨æ•™å¸ˆæ¨¡å‹å¹¶è¡Œæ¨ç†è·å–logits
        teacher_logits_list = []
        if self.parallel_inference_teacher:
            # ä½¿ç”¨å¹¶è¡Œæ¨ç†è·å–æ•™å¸ˆlogits
            self.logger.info("ä½¿ç”¨æ•™å¸ˆæ¨¡å‹å¹¶è¡Œæ¨ç†è®¡ç®—logits")
            teacher_logits_list = self.parallel_inference_teacher.get_logits_batch_parallel(full_sequences)
        else:
            # å›é€€åˆ°ä¸²è¡Œæ¨ç†
            self.logger.info("ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ä¸²è¡Œæ¨ç†è®¡ç®—logits")
            for full_sequence in full_sequences:
                # ğŸ”¥ ç¦ç”¨ç¼“å­˜ï¼šç›´æ¥è®¡ç®—teacher logitsï¼ˆå› ä¸ºå‘½ä¸­ç‡ä¸€ç›´æ˜¯0%ï¼‰
                teacher_logits = self.teacher_model.get_logits(full_sequence, use_cache=False)
                teacher_logits_list.append(teacher_logits)
        
        # å¹¶è¡Œè®¡ç®—å†…åœ¨å¥–åŠ±
        with self.parallel_processor as processor:
            def compute_single_intrinsic_reward(question: str, response: str, teacher_logits: torch.Tensor) -> float:
                try:
                    # è·å–å­¦ç”Ÿtokens
                    student_tokens = self.student_model.tokenizer.encode(response, add_special_tokens=False)
                    student_tokens = torch.tensor(student_tokens).unsqueeze(0)
                    
                    # è®¡ç®—é—®é¢˜éƒ¨åˆ†çš„é•¿åº¦
                    question_tokens = self.student_model.tokenizer.encode(question, add_special_tokens=False)
                    question_length = len(question_tokens)
                    
                    # è®¡ç®—å†…åœ¨å¥–åŠ±
                    intrinsic_reward = self.intrinsic_computer.compute_intrinsic_reward(
                        teacher_logits, student_tokens, question_length
                    )
                    
                    # å½’ä¸€åŒ–
                    normalized_intrinsic = self.reward_normalizer.normalize_intrinsic_rewards(
                        intrinsic_reward
                    )
                    
                    # è®¡ç®—trajectoryçº§åˆ«å¥–åŠ±
                    trajectory_reward = self.intrinsic_computer.compute_trajectory_reward(
                        normalized_intrinsic
                    )
                    
                    return trajectory_reward
                    
                except Exception as e:
                    self.logger.error(f"å¹¶è¡Œå†…åœ¨å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
                    return 0.0
            
            # å¹¶è¡Œè®¡ç®—å¥–åŠ±
            intrinsic_rewards = []
            for i, (question, response) in enumerate(zip(questions, student_responses)):
                if i < len(teacher_logits_list) and teacher_logits_list[i] is not None:
                    reward = compute_single_intrinsic_reward(question, response, teacher_logits_list[i])
                    intrinsic_rewards.append(reward)
                else:
                    self.logger.warning(f"æ•™å¸ˆlogitsç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å¥–åŠ±: {i}")
                    intrinsic_rewards.append(0.0)
        
        return torch.tensor(intrinsic_rewards)
    
    def compute_correctness_rewards(self, questions: List[str], 
                                   student_responses: List[str]) -> torch.Tensor:
        """è®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        use_parallel = self._use_parallel
        
        if use_parallel and self.parallel_processor:
            return self._compute_correctness_rewards_parallel(questions, student_responses)
        else:
            return self._compute_correctness_rewards_sequential(questions, student_responses)
    
    def _compute_correctness_rewards_sequential(self, questions: List[str], 
                                              student_responses: List[str]) -> torch.Tensor:
        """ä¸²è¡Œè®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼ˆåŸå§‹å®ç°ï¼‰"""
        # æ£€æŸ¥å¿…è¦çš„ç»„ä»¶
        if self.teacher_model is None:
            raise ValueError("teacher_model æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
        
        correctness_rewards = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = self._create_progress_bar(
            zip(questions, student_responses), 
            desc="è®¡ç®—æ­£ç¡®æ€§å¥–åŠ±"
        )
        
        for question, response in progress_bar:
            # æå–å­¦ç”Ÿç­”æ¡ˆ
            student_answer = extract_final_answer(response)
            
            # æå–æ­£ç¡®ç­”æ¡ˆï¼ˆä»é—®é¢˜ä¸­æˆ–ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»æ•°æ®é›†ä¸­è·å–æ­£ç¡®ç­”æ¡ˆ
            teacher_response = self.teacher_model.generate_response(question, max_length=256)
            correct_answer = extract_final_answer(teacher_response)
            
            # åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            is_correct = is_answer_correct(student_answer, correct_answer)
            
            correctness_rewards.append(1.0 if is_correct else 0.0)
        
        return torch.tensor(correctness_rewards)
    
    def _compute_correctness_rewards_parallel(self, questions: List[str], 
                                            student_responses: List[str]) -> torch.Tensor:
        """å¹¶è¡Œè®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±"""
        with self.parallel_processor as processor:
            # å®šä¹‰å¥–åŠ±è®¡ç®—å‡½æ•°
            def compute_single_correctness_reward(question: str, response: str) -> float:
                try:
                    # æå–å­¦ç”Ÿç­”æ¡ˆ
                    student_answer = extract_final_answer(response)
                    
                    # æå–æ­£ç¡®ç­”æ¡ˆï¼ˆä»é—®é¢˜ä¸­æˆ–ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆï¼‰
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»æ•°æ®é›†ä¸­è·å–æ­£ç¡®ç­”æ¡ˆ
                    teacher_response = self.teacher_model.generate_response(question, max_length=256)
                    correct_answer = extract_final_answer(teacher_response)
                    
                    # åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
                    is_correct = is_answer_correct(student_answer, correct_answer)
                    
                    return 1.0 if is_correct else 0.0
                    
                except Exception as e:
                    self.logger.error(f"å¹¶è¡Œæ­£ç¡®æ€§å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
                    return 0.0
            
            # å¹¶è¡Œè®¡ç®—å¥–åŠ±
            correctness_rewards = processor.compute_rewards_parallel(
                questions, student_responses, compute_single_correctness_reward
            )
            
            return torch.tensor(correctness_rewards)
    
    def compute_combined_rewards(self, questions: List[str], 
                               student_responses: List[str]) -> torch.Tensor:
        """è®¡ç®—ç»„åˆå¥–åŠ±ï¼ˆå•çº¿ç¨‹ä¸²è¡Œå¤„ç†ï¼Œé¿å…tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰"""
        # æ£€æŸ¥å¥–åŠ±ç»„åˆå™¨æ˜¯å¦å·²è®¾ç½®
        if self.reward_combiner is None:
            raise ValueError("reward_combiner æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_components() æ–¹æ³•")
        
        # å¼ºåˆ¶ä½¿ç”¨ä¸²è¡Œè®¡ç®—ï¼Œé¿å…å¤šçº¿ç¨‹å¯¼è‡´çš„tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜
        # ä¸²è¡Œè®¡ç®—
        intrinsic_rewards = self.compute_intrinsic_rewards(questions, student_responses)
        correctness_rewards = self.compute_correctness_rewards(questions, student_responses)
        
        # ç»„åˆå¥–åŠ±
        combined_rewards = self.reward_combiner.combine_rewards(
            intrinsic_rewards, correctness_rewards
        )
        
        return combined_rewards
    
    def train_step(self, batch: Dict[str, List[str]]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ @handle_errorsï¼Œå› ä¸ºéœ€è¦ç¡®ä¿å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯è¿”å› None
        # è¿™æ ·è°ƒç”¨è€…å¯ä»¥å†³å®šå¦‚ä½•å¤„ç†é”™è¯¯
        try:
            # æ£€æŸ¥å¿…è¦çš„ç»„ä»¶
            if self.student_model is None:
                raise ValueError("student_model æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_models() æ–¹æ³•")
            if self.ppo_trainer is None:
                raise ValueError("ppo_trainer æœªè®¾ç½®ã€‚è¯·å…ˆè°ƒç”¨ setup_ppo_trainer() æ–¹æ³•")
            
            # éªŒè¯æ•°æ®æ‰¹æ¬¡
            if "questions" not in batch or not batch["questions"]:
                raise ValueError("âŒ ç¼ºå°‘é—®é¢˜æ•°æ®")
            
            if not isinstance(batch["questions"], list):
                raise ValueError("âŒ é—®é¢˜æ•°æ®ä¸æ˜¯åˆ—è¡¨")
            
            questions = batch["questions"]
            
            # æ£€æŸ¥é—®é¢˜æ•°é‡
            if len(questions) == 0:
                raise ValueError("âŒ é—®é¢˜åˆ—è¡¨ä¸ºç©º")
            
            if len(questions) > 100:  # é˜²æ­¢æ‰¹æ¬¡è¿‡å¤§
                self.logger.warning(f"âš ï¸ æ‰¹æ¬¡å¤§å°è¿‡å¤§: {len(questions)}, æˆªæ–­åˆ°100")
                questions = questions[:100]
            
            # å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
            use_parallel = self._use_parallel
            
            # ä½¿ç”¨no_gradè¿›è¡Œæ¨ç†ï¼ŒèŠ‚çœå†…å­˜
            with torch.no_grad():
                if use_parallel and self.parallel_inference_student:
                    # âœ… ç»Ÿä¸€ä½¿ç”¨é…ç½®ä¸­çš„max_length
                    student_responses = self.parallel_inference_student.generate_batch_parallel(
                        questions,
                        max_length=self.config["ppo"]["max_length"],  # âœ… ç»Ÿä¸€ä½¿ç”¨é…ç½®çš„max_length
                        temperature=self.config["ppo"]["temperature"],
                        do_sample=self.config["ppo"]["do_sample"]
                    )
                else:
                    # âœ… ç»Ÿä¸€ä½¿ç”¨é…ç½®ä¸­çš„max_lengthï¼Œç¡®ä¿ä¸€è‡´æ€§
                    # æ³¨æ„ï¼šç”Ÿæˆæ—¶çš„max_lengthæ˜¯ç”Ÿæˆæ–°tokenæ•°ï¼Œå®é™…æ€»é•¿åº¦ = query_length + max_length
                    # ä½†è¿™é‡Œä½¿ç”¨é…ç½®çš„max_lengthä½œä¸ºå‚è€ƒï¼Œå®é™…ç”Ÿæˆä¼šæ›´çŸ­ï¼ˆç”Ÿæˆæ—¶é™åˆ¶ï¼‰
                    student_responses = self.student_model.generate(
                        questions,
                        max_length=self.config["ppo"]["max_length"],  # âœ… ç»Ÿä¸€ä½¿ç”¨é…ç½®çš„max_length
                        temperature=self.config["ppo"]["temperature"],
                        do_sample=self.config["ppo"]["do_sample"]
                    )
                    # ç¡®ä¿è¿”å›å€¼æ˜¯åˆ—è¡¨ç±»å‹ï¼ˆstudent_model.generate åœ¨å•ä¸ªpromptæ—¶å¯èƒ½è¿”å›å•ä¸ªå­—ç¬¦ä¸²ï¼‰
                    if isinstance(student_responses, str):
                        student_responses = [student_responses]
                    if not isinstance(student_responses, list):
                        raise TypeError(f"student_model.generate è¿”å›äº†æ„å¤–çš„ç±»å‹: {type(student_responses)}")
            
            # è®¡ç®—å¥–åŠ±
            combined_rewards = self.compute_combined_rewards(questions, student_responses)
            
            # ğŸ” è¯Šæ–­ï¼šæ‰“å°å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‰å‡ æ­¥ï¼‰
            if self.training_stats["step"] < 3:
                self.logger.info(f"ğŸ Step {self.training_stats['step'] + 1} - å¥–åŠ±è¯Šæ–­:")
                self.logger.info(f"   Combined rewards: shape={combined_rewards.shape}, "
                               f"mean={combined_rewards.mean():.4f}, std={combined_rewards.std():.4f}, "
                               f"min={combined_rewards.min():.4f}, max={combined_rewards.max():.4f}")
                self.logger.info(f"   å¥–åŠ±å€¼åˆ†å¸ƒ: {combined_rewards.tolist()}")
            
            # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šTeacheræ¨ç†åç«‹å³æ¸…ç†æ˜¾å­˜ï¼ˆTeacheræ¨¡å‹å ç”¨å¤§é‡æ˜¾å­˜ï¼‰
            # å¥–åŠ±è®¡ç®—å®Œæˆåï¼ŒTeacherçš„ä¸­é—´æ¿€æ´»ä¸å†éœ€è¦ï¼Œç«‹å³é‡Šæ”¾
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # éªŒè¯é•¿åº¦åŒ¹é…
            if len(combined_rewards) != len(questions):
                raise ValueError(f"å¥–åŠ±æ•°é‡ {len(combined_rewards)} ä¸é—®é¢˜æ•°é‡ {len(questions)} ä¸åŒ¹é…")
            if len(student_responses) != len(questions):
                raise ValueError(f"å“åº”æ•°é‡ {len(student_responses)} ä¸é—®é¢˜æ•°é‡ {len(questions)} ä¸åŒ¹é…")
            
            # å°†é—®é¢˜è½¬æ¢ä¸ºtokenizedå¼ é‡åˆ—è¡¨
            # ä½¿ç”¨æ‰¹é‡tokenizeä»¥æé«˜æ•ˆç‡å¹¶é¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜
            # æ³¨æ„ï¼štokenizeræ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†ä¸ºäº†æ›´å®‰å…¨ï¼Œä½¿ç”¨æ‰¹é‡å¤„ç†
            try:
                tokenized_queries_batch = self.student_model.tokenizer(
                    questions,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.config["ppo"]["max_length"]
                )
                tokenized_queries = [tokenized_queries_batch["input_ids"][i] for i in range(len(questions))]
            except RuntimeError as e:
                if "Already borrowed" in str(e):
                    # å¦‚æœé‡åˆ°çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†
                    self.logger.warning("æ£€æµ‹åˆ°tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
                    tokenized_queries = []
                    for question in questions:
                        tokenized = self.student_model.tokenizer(
                            question,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=self.config["ppo"]["max_length"]
                        )
                        tokenized_queries.append(tokenized["input_ids"])
                else:
                    raise
            
            # å°†å“åº”è½¬æ¢ä¸ºtokenizedå¼ é‡åˆ—è¡¨
            try:
                tokenized_responses_batch = self.student_model.tokenizer(
                    student_responses,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.config["ppo"]["max_length"]
                )
                tokenized_responses = [tokenized_responses_batch["input_ids"][i] for i in range(len(student_responses))]
            except RuntimeError as e:
                if "Already borrowed" in str(e):
                    # å¦‚æœé‡åˆ°çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†
                    self.logger.warning("æ£€æµ‹åˆ°tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
                    tokenized_responses = []
                    for response in student_responses:
                        tokenized = self.student_model.tokenizer(
                            response,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=self.config["ppo"]["max_length"]
                        )
                        tokenized_responses.append(tokenized["input_ids"])
                else:
                    raise
            
            # PPOæ›´æ–°
            # ç¡®ä¿åˆ†æ•°éƒ½æ˜¯å¼ é‡è€Œä¸æ˜¯ float
            device = getattr(self.ppo_trainer.accelerator, "device", "cuda")
            
            # ç¡®ä¿combined_rewardsæ˜¯1D tensorï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡åˆ—è¡¨
            if combined_rewards.dim() > 1:
                combined_rewards = combined_rewards.squeeze()
            
            # éªŒè¯tokenizedåˆ—è¡¨é•¿åº¦åŒ¹é…
            if len(tokenized_queries) != len(tokenized_responses):
                raise ValueError(f"é—®é¢˜tokenæ•°é‡ {len(tokenized_queries)} ä¸å“åº”tokenæ•°é‡ {len(tokenized_responses)} ä¸åŒ¹é…")
            if len(tokenized_queries) != len(combined_rewards):
                raise ValueError(f"é—®é¢˜tokenæ•°é‡ {len(tokenized_queries)} ä¸å¥–åŠ±æ•°é‡ {len(combined_rewards)} ä¸åŒ¹é…")
            
            scores = []
            for i in range(len(combined_rewards)):
                reward = combined_rewards[i]
                if torch.is_tensor(reward):
                    scores.append(reward.to(device=device, dtype=torch.float32))
                else:
                    scores.append(torch.tensor(reward, dtype=torch.float32, device=device))
            
            # æœ€ç»ˆéªŒè¯scoresé•¿åº¦
            if len(scores) != len(tokenized_queries):
                raise ValueError(f"åˆ†æ•°æ•°é‡ {len(scores)} ä¸æŸ¥è¯¢æ•°é‡ {len(tokenized_queries)} ä¸åŒ¹é…")
            
            # âœ… ä¿®å¤ï¼šåœ¨åˆ é™¤combined_rewardsä¹‹å‰ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåç»­æ—¥å¿—å’Œç»Ÿè®¡æ›´æ–°ï¼‰
            # ä¿å­˜å‡å€¼å¥–åŠ±ï¼ˆç§»åˆ°CPUå¹¶è½¬ä¸ºPythonæ ‡é‡ï¼Œé‡Šæ”¾GPUæ˜¾å­˜ï¼‰
            mean_reward_for_stats = torch.mean(combined_rewards).cpu().item()
            # ä¿å­˜combined_rewardsçš„CPUå‰¯æœ¬ç”¨äºç»Ÿè®¡æ›´æ–°ï¼ˆä¿æŒä¸ºtensoræ ¼å¼ï¼Œå› ä¸º_update_training_statséœ€è¦ï¼‰
            combined_rewards_cpu = combined_rewards.cpu().clone()
            
            # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåœ¨PPO stepä¹‹å‰ç«‹å³æ¸…ç†å†…å­˜å’Œé‡Šæ”¾ä¸éœ€è¦çš„å˜é‡
            # é‡Šæ”¾åŸå§‹å­—ç¬¦ä¸²æ•°æ®ï¼ˆå·²tokenizeï¼Œä¸å†éœ€è¦ï¼‰
            if 'questions' in locals():
                del questions
            if 'student_responses' in locals():
                del student_responses
            # é‡Šæ”¾æ‰¹æ¬¡tokenizedå¼ é‡ï¼ˆå·²ç»æå–ä¸ºåˆ—è¡¨ï¼‰
            if 'tokenized_queries_batch' in locals():
                del tokenized_queries_batch
            if 'tokenized_responses_batch' in locals():
                del tokenized_responses_batch
            # é‡Šæ”¾combined_rewardsï¼ˆå·²è½¬æ¢ä¸ºscoresï¼ŒCPUå‰¯æœ¬å·²ä¿å­˜ï¼‰
            del combined_rewards
            
            # ğŸ”¥ æç«¯æ˜¾å­˜æ¸…ç†ï¼šåœ¨PPO stepä¹‹å‰ï¼ˆlog_softmaxæ˜¯æ˜¾å­˜å³°å€¼ï¼‰
            # åœ¨4Ã—GPUé…ç½®ä¸‹ï¼Œlog_softmaxéœ€è¦åŒæ—¶è®¡ç®—policyå’Œrefæ¨¡å‹ï¼Œæ˜¾å­˜å‹åŠ›æå¤§
            if torch.cuda.is_available():
                # æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜
                for gpu_id in range(torch.cuda.device_count()):
                    with torch.cuda.device(gpu_id):
                        torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()  # Pythonåƒåœ¾å›æ”¶
            
            # ğŸ”¥ æ ¸å¿ƒé—®é¢˜ï¼šPPO traineråœ¨batched_forward_passä¸­è®¡ç®—log_softmaxæ—¶ä¼šå±•å¼€å…¨è¯è¡¨çŸ©é˜µ (BÃ—TÃ—V)
            # å³ä½¿batch_size=1, seq_len=192ï¼Œvocab_size=100kæ—¶ï¼Œè¿™ä¸ªçŸ©é˜µåœ¨bfloat16ä¸‹ä¹Ÿéœ€è¦çº¦38MB
            # ä½†policyå’Œrefæ¨¡å‹åŒæ—¶è®¡ç®—ï¼ŒåŠ ä¸Šæ¢¯åº¦ï¼Œæ˜¾å­˜å³°å€¼ä¼šé£™å‡åˆ°å‡ ç™¾MBç”šè‡³GB
            # å…³é”®ï¼šå³ä½¿4å¼ å¡æ•°æ®å¹¶è¡Œï¼Œæ¯å¼ å¡ä»è¦åœ¨è‡ªå·±çš„micro-batchä¸Šè®¡ç®—å®Œæ•´ (seq_len Ã— vocab_size)
            
            # âœ… ä¼˜åŒ–ç­–ç•¥ï¼šåœ¨è°ƒç”¨PPO trainerå‰ï¼Œæ£€æŸ¥å¹¶æˆªæ–­è¶…é•¿åºåˆ—
            max_allowed_length = self.config["ppo"]["max_length"]
            
            # æˆªæ–­è¶…é•¿åºåˆ—ï¼ˆé¿å…log_softmax OOMï¼‰
            truncated_queries = []
            truncated_responses = []
            for q, r in zip(tokenized_queries, tokenized_responses):
                # è®¡ç®—æ€»é•¿åº¦ï¼ˆquery + responseï¼‰
                total_len = len(q) + len(r)
                
                if total_len > max_allowed_length:
                    # å¦‚æœè¶…é•¿ï¼Œä¼˜å…ˆä¿ç•™queryï¼Œæˆªæ–­response
                    query_len = len(q)
                    max_response_len = max(0, max_allowed_length - query_len)
                    
                    if max_response_len > 0:
                        # æˆªæ–­responseåˆ°å…è®¸çš„æœ€å¤§é•¿åº¦
                        truncated_r = r[:max_response_len]
                        truncated_queries.append(q)
                        truncated_responses.append(truncated_r)
                        if self.training_stats["step"] < 3:  # åªåœ¨å‰å‡ æ­¥è­¦å‘Š
                            self.logger.warning(f"âš ï¸ åºåˆ—è¶…é•¿ï¼ˆ{total_len} > {max_allowed_length}ï¼‰ï¼Œå·²æˆªæ–­responseåˆ°{max_response_len}ã€‚å»ºè®®é™ä½max_lengthé¿å…log_softmax OOMã€‚")
                    else:
                        # queryæœ¬èº«å¤ªé•¿ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                        self.logger.warning(f"âš ï¸ Queryå¤ªé•¿ï¼ˆ{query_len} > {max_allowed_length}ï¼‰ï¼Œè·³è¿‡æ­¤æ ·æœ¬ã€‚")
                        continue
                else:
                    truncated_queries.append(q)
                    truncated_responses.append(r)
            
            # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½è¢«æˆªæ–­ï¼Œè·³è¿‡è¿™ä¸€æ­¥
            if len(truncated_queries) == 0:
                self.logger.error("âŒ æ‰€æœ‰åºåˆ—éƒ½è¢«æˆªæ–­ï¼Œè·³è¿‡æ­¤è®­ç»ƒæ­¥éª¤")
                return None
            
            # ğŸ” è®¾ç½®æ¢¯åº¦hookæ¥æ•è·æ¢¯åº¦èŒƒæ•°ï¼ˆåœ¨PPO stepä¹‹å‰ï¼‰
            grad_norm_from_hook = None
            grad_hook_handles = []
            grad_norms = []  # åœ¨å¤–éƒ¨å®šä¹‰ï¼Œç¡®ä¿åœ¨æ•´ä¸ªtry-finallyå—ä¸­å¯è§
            
            # ä¸ºå¯è®­ç»ƒå‚æ•°æ³¨å†Œhookï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
            try:
                if hasattr(self.ppo_trainer, 'model') and self.ppo_trainer.model is not None:
                    # å­˜å‚¨æ¢¯åº¦ç”¨äºè®¡ç®—èŒƒæ•°
                    for name, param in self.ppo_trainer.model.named_parameters():
                        if param.requires_grad:
                            # æ³¨å†Œhookï¼Œåœ¨æ¢¯åº¦è®¡ç®—æ—¶è®°å½•
                            def make_hook(n=name):
                                def hook(grad):
                                    if grad is not None:
                                        grad_norms.append((n, grad.norm().item()))
                                    return grad  # ä¿æŒæ¢¯åº¦ä¸å˜ï¼Œåªç”¨äºç›‘æ§
                                return hook
                            handle = param.register_hook(make_hook(name))
                            grad_hook_handles.append(handle)
            except Exception as e:
                # Hookæ³¨å†Œå¤±è´¥ä¸å½±å“è®­ç»ƒ
                pass
            
            # æ‰§è¡ŒPPO stepï¼ˆlog_softmaxè®¡ç®—å‘ç”Ÿåœ¨è¿™é‡Œï¼‰
            try:
                stats = self.ppo_trainer.step(
                    queries=truncated_queries,
                    responses=truncated_responses,
                    scores=scores[:len(truncated_queries)]  # è°ƒæ•´scoresé•¿åº¦åŒ¹é…æˆªæ–­åçš„åºåˆ—
                )
                
                # ä»hookä¸­è·å–æ¢¯åº¦èŒƒæ•°ï¼ˆå¦‚æœæœ‰ï¼‰
                if grad_norms:
                    total_grad_norm_sq = sum(norm**2 for _, norm in grad_norms)
                    grad_norm_from_hook = (total_grad_norm_sq ** 0.5)
            except torch.cuda.OutOfMemoryError as e:
                # ğŸ”¥ å¦‚æœlog_softmaxé˜¶æ®µOOMï¼Œè¿›è¡Œæç«¯æ¸…ç†å¹¶æä¾›è¯¦ç»†è¯Šæ–­
                self.logger.error("âŒ PPO stepä¸­log_softmaxé˜¶æ®µOOMï¼Œæ‰§è¡Œæç«¯æ˜¾å­˜æ¸…ç†...")
                for gpu_id in range(torch.cuda.device_count()):
                    with torch.cuda.device(gpu_id):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                gc.collect()
                
                # è¯¦ç»†è¯Šæ–­ä¿¡æ¯
                max_seq_len = max((len(q) + len(r) for q, r in zip(truncated_queries, truncated_responses)), default=0)
                vocab_size = len(self.student_model.tokenizer) if hasattr(self.student_model, 'tokenizer') else "unknown"
                estimated_memory_mb = (max_seq_len * vocab_size * 2) / (1024 * 1024) if isinstance(vocab_size, int) else "unknown"
                
                error_msg = (
                    f"PPO step log_softmax OOMï¼ˆå³ä½¿batch_size=1, max_length={max_allowed_length}ï¼‰ã€‚\n"
                    f"è¯Šæ–­ä¿¡æ¯ï¼š\n"
                    f"  - æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_len}\n"
                    f"  - è¯è¡¨å¤§å°: {vocab_size}\n"
                    f"  - ä¼°ç®—log_softmaxæ˜¾å­˜: ~{estimated_memory_mb}MBï¼ˆå•æ¨¡å‹ï¼Œä¸å«æ¢¯åº¦ï¼‰\n"
                    f"  - å®é™…æ˜¾å­˜éœ€æ±‚: ä¼°ç®—å€¼ Ã— 2ï¼ˆpolicy+refï¼‰ Ã— 2ï¼ˆæ¢¯åº¦ï¼‰â‰ˆ {estimated_memory_mb * 4 if isinstance(estimated_memory_mb, (int, float)) else 'unknown'}MB\n"
                    f"è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"  1) é™ä½max_lengthåˆ°128æˆ–æ›´å°ï¼ˆæœ€æœ‰æ•ˆï¼‰\n"
                    f"  2) æ£€æŸ¥GPUåˆ†é…æ˜¯å¦å‡åŒ€ï¼ˆTeacherä¸åº”é›†ä¸­åœ¨ä¸€å¼ å¡ï¼‰\n"
                    f"  3) è€ƒè™‘ä½¿ç”¨2Ã—GPUè€Œé4Ã—GPUï¼ˆå‡å°‘æ•°æ®å¹¶è¡Œå¼€é”€ï¼‰\n"
                    f"  4) ä½¿ç”¨æ›´å¤§çš„GPUï¼ˆH100 120GBï¼‰\n"
                    f"åŸå§‹é”™è¯¯: {e}"
                )
                raise RuntimeError(error_msg)
            finally:
                # æ¸…ç†hook
                for handle in grad_hook_handles:
                    handle.remove()
                grad_hook_handles.clear()
                grad_norms.clear()  # æ¸…ç©ºæ¢¯åº¦èŒƒæ•°åˆ—è¡¨
            
            # ğŸ”¥ æç«¯æ˜¾å­˜æ¸…ç†ï¼šåœ¨PPO stepä¹‹åç«‹å³æ¸…ç†
            # log_softmaxè®¡ç®—å®Œæˆåçš„æ˜¾å­˜ç¢ç‰‡éœ€è¦ç«‹å³æ¸…ç†
            if torch.cuda.is_available():
                for gpu_id in range(torch.cuda.device_count()):
                    with torch.cuda.device(gpu_id):
                        torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()
            
            # ğŸ” è¯Šæ–­ï¼šæ‰“å°statsè¯¦ç»†ä¿¡æ¯ï¼ˆæ¯æ­¥éƒ½æ‰“å°å‰3æ­¥ï¼Œç„¶åæ¯10æ­¥ï¼‰
            should_log_verbose = self.training_stats["step"] < 3 or self.training_stats["step"] % 10 == 0
            if should_log_verbose and stats is not None and isinstance(stats, dict):
                available_keys = list(stats.keys())
                self.logger.info(f"ğŸ“Š Step {self.training_stats['step'] + 1} - PPO statså¯ç”¨é”®: {available_keys}")
                
                # æ‰“å°æ‰€æœ‰statsçš„å€¼ï¼Œæ–¹ä¾¿è°ƒè¯•
                for key in available_keys:
                    try:
                        value = stats[key]
                        # å°è¯•è½¬æ¢ä¸ºæ ‡é‡
                        if isinstance(value, (torch.Tensor, np.ndarray)):
                            value = value.item() if hasattr(value, 'item') else float(value)
                        self.logger.info(f"  {key} = {value}")
                    except Exception as e:
                        self.logger.warning(f"  æ— æ³•æ‰“å° {key}: {e}")
                
                # ğŸ” é¢å¤–è¯Šæ–­ï¼šæ£€æŸ¥å…³é”®æŒ‡æ ‡
                self.logger.info(f"ğŸ” å…³é”®è¯Šæ–­:")
                self.logger.info(f"  å¥–åŠ±: mean={mean_reward_for_stats:.4f}")
                self.logger.info(f"  statsç±»å‹: {type(stats)}")
                self.logger.info(f"  statsé”®æ•°é‡: {len(stats)}")
            
            # ğŸ” é¢å¤–è¯Šæ–­ï¼šå¦‚æœstatsä¸ºç©ºæˆ–åªåŒ…å«æå°‘æ•°é”®ï¼Œå‘å‡ºè­¦å‘Š
            if stats is not None and isinstance(stats, dict) and len(stats) < 3:
                self.logger.error(f"âŒ PPO statså­—å…¸å¼‚å¸¸ï¼šåªæœ‰{len(stats)}ä¸ªé”®ï¼Œå¯èƒ½PPOTraineræœªæ­£ç¡®è®¡ç®—ï¼")
                self.logger.error(f"   å¯ç”¨é”®: {list(stats.keys())}")
                self.logger.error(f"   statså†…å®¹: {stats}")
            
            # ğŸ” è¯Šæ–­ï¼šæ£€æŸ¥ç­–ç•¥æ˜¯å¦çœŸçš„åœ¨æ›´æ–°ï¼ˆæ ¹æ®æ—¥å¿—åˆ†æï¼Œå‘ç°KLä¸º0çš„ä¸¥é‡é—®é¢˜ï¼‰
            if stats is not None and isinstance(stats, dict):
                # å®šä¹‰to_scalarè¾…åŠ©å‡½æ•°ï¼ˆç”¨äºè¯Šæ–­ï¼‰
                def _to_scalar(value, default=0):
                    if value is None:
                        return default
                    if isinstance(value, (np.ndarray, np.generic)):
                        return float(value.item() if hasattr(value, 'item') else float(value))
                    if isinstance(value, torch.Tensor):
                        return float(value.item() if hasattr(value, 'item') else float(value))
                    return float(value)
                
                # æ£€æŸ¥KLæ•£åº¦
                approx_kl = _to_scalar(stats.get("ppo/policy/approxkl") or stats.get("ppo/policy/policykl") or 0)
                advantages_mean = _to_scalar(stats.get("ppo/policy/advantages_mean") or 0)
                clipfrac = _to_scalar(stats.get("ppo/policy/clipfrac") or 0)
                policy_loss_val = _to_scalar(stats.get("ppo/loss/policy") or stats.get("ppo/policy/loss") or 0)
                
                # å¦‚æœKLä¸º0ä¸”ä¼˜åŠ¿æ¥è¿‘0ï¼Œè¯´æ˜ç­–ç•¥å‡ ä¹æ²¡æœ‰æ›´æ–°
                # æ³¨æ„ï¼šè®­ç»ƒåˆæœŸï¼ˆå‰å‡ æ­¥ï¼‰å‡ºç°è¿™ç§æƒ…å†µå¯èƒ½æ­£å¸¸ï¼Œåªæœ‰åœ¨æŒç»­å¤šæ­¥åæ‰è­¦å‘Š
                if abs(approx_kl) < 1e-6 and abs(advantages_mean) < 1e-6:
                    # åªåœ¨ç¬¬5æ­¥ä¹‹åæ‰è­¦å‘Šï¼Œé¿å…è®­ç»ƒåˆæœŸçš„æ­£å¸¸æ³¢åŠ¨
                    if self.training_stats["step"] >= 5 and (self.training_stats["step"] % 50 == 0 or self.training_stats["step"] < 20):
                        # æ£€æŸ¥æœ€è¿‘å‡ æ­¥æ˜¯å¦éƒ½æ˜¯è¿™æ ·ï¼ˆæ’é™¤å•æ¬¡æ³¢åŠ¨ï¼‰
                        if len(self.training_stats["kl_divergences"]) >= 5:
                            recent_kls = self.training_stats["kl_divergences"][-5:]
                            all_kl_zero = all(abs(k) < 1e-6 for k in recent_kls if k is not None)
                            if all_kl_zero:
                                self.logger.warning(f"âš ï¸ è­¦å‘Šï¼šç­–ç•¥å¯èƒ½æ²¡æœ‰æ›´æ–°ï¼ï¼ˆå·²æŒç»­è‡³å°‘5æ­¥ï¼‰")
                                self.logger.warning(f"   KLæ•£åº¦: {approx_kl:.10f} (æ¥è¿‘0)")
                                self.logger.warning(f"   ä¼˜åŠ¿å‡å€¼: {advantages_mean:.10f} (æ¥è¿‘0)")
                                self.logger.warning(f"   ç­–ç•¥æŸå¤±: {policy_loss_val:.10f}")
                                self.logger.warning(f"   è£å‰ªæ¯”ä¾‹: {clipfrac:.4f}")
                                self.logger.warning(f"   å¯èƒ½åŸå› :")
                                self.logger.warning(f"     1. å¥–åŠ±å°ºåº¦é—®é¢˜ï¼ˆå¥–åŠ±å¤ªå°æˆ–å¤ªå¤§ï¼‰")
                                self.logger.warning(f"     2. ä¼˜åŠ¿å½’ä¸€åŒ–é—®é¢˜")
                                self.logger.warning(f"     3. å­¦ä¹ ç‡å¤ªå°")
                                self.logger.warning(f"     4. policyå’Œref_modelå®Œå…¨ç›¸åŒï¼ˆåº”è¯¥ä¸åŒï¼‰")
                        elif self.training_stats["step"] < 10:
                            # å‰å‡ æ­¥åªåšä¿¡æ¯è®°å½•ï¼Œä¸è­¦å‘Š
                            self.logger.info(f"â„¹ï¸ Step {self.training_stats['step'] + 1}: KL={approx_kl:.10f}, ä¼˜åŠ¿={advantages_mean:.10f} (è®­ç»ƒåˆæœŸï¼Œç»§ç»­è§‚å¯Ÿ)")
                
                # æ£€æŸ¥ä»·å€¼å‡½æ•°è®­ç»ƒ
                val_var_explained = _to_scalar(stats.get("ppo/val/var_explained") or 0)
                if val_var_explained < 0:
                    # åªåœ¨ç¬¬5æ­¥ä¹‹åä¸”æŒç»­å¤šæ­¥æ‰è­¦å‘Š
                    if self.training_stats["step"] >= 5 and (self.training_stats["step"] % 50 == 0 or self.training_stats["step"] < 20):
                        self.logger.warning(f"âš ï¸ ä»·å€¼å‡½æ•°è®­ç»ƒå¼‚å¸¸ï¼švar_explained = {val_var_explained:.4f} (è´Ÿå€¼è¡¨ç¤ºä»·å€¼å‡½æ•°æ¯”ç®€å•é¢„æµ‹å‡å€¼è¿˜å·®)")
            
            # ğŸ” æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åœ¨æ›´æ–°ï¼ˆç”¨äºè¯Šæ–­ç­–ç•¥æ˜¯å¦çœŸçš„åœ¨è®­ç»ƒï¼‰
            # æ³¨æ„ï¼šPPOTraineråœ¨step()å†…éƒ¨å®Œæˆæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–ï¼Œstep()è¿”å›åæ¢¯åº¦å·²è¢«æ¸…é™¤
            # æ‰€ä»¥æˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ¨¡å‹å‚æ•°çš„å˜åŒ–æ¥éªŒè¯æ›´æ–°æ˜¯å¦å‘ç”Ÿ
            
            # åˆå§‹åŒ–ï¼šä¿å­˜åˆå§‹å‚æ•°çŠ¶æ€ï¼ˆä»…åœ¨ç¬¬ä¸€æ­¥ï¼‰
            if not hasattr(self, '_prev_model_params'):
                if hasattr(self.ppo_trainer, 'model') and self.ppo_trainer.model is not None:
                    self._prev_model_params = {}
                    for name, param in self.ppo_trainer.model.named_parameters():
                        if param.requires_grad:
                            self._prev_model_params[name] = param.data.clone()
                    if self.training_stats["step"] == 0:
                        self.logger.info(f"ğŸ“Š å·²ä¿å­˜åˆå§‹æ¨¡å‹å‚æ•°çŠ¶æ€ï¼ˆ{len(self._prev_model_params)}ä¸ªå¯è®­ç»ƒå‚æ•°ï¼‰")
            
            # æ£€æŸ¥å‚æ•°å˜åŒ–ï¼ˆæ¯æ¬¡éƒ½è®¡ç®—ï¼Œç”¨äºwandbè®°å½•ï¼‰
            param_change_info = {}
            if hasattr(self, '_prev_model_params'):
                try:
                    if hasattr(self.ppo_trainer, 'model') and self.ppo_trainer.model is not None:
                        max_change = 0.0
                        total_change = 0.0
                        changed_params = 0
                        total_params = 0
                        
                        for name, param in self.ppo_trainer.model.named_parameters():
                            if param.requires_grad and name in self._prev_model_params:
                                total_params += 1
                                prev_param = self._prev_model_params[name]
                                # è®¡ç®—å‚æ•°å˜åŒ–
                                param_diff = (param.data - prev_param).abs()
                                max_param_change = param_diff.max().item()
                                mean_param_change = param_diff.mean().item()
                                
                                if max_param_change > 1e-8:  # æœ‰æ˜¾è‘—å˜åŒ–
                                    changed_params += 1
                                    max_change = max(max_change, max_param_change)
                                    total_change += mean_param_change
                                
                                # æ›´æ–°ä¿å­˜çš„å‚æ•°
                                self._prev_model_params[name] = param.data.clone()
                        
                        if total_params > 0:
                            param_change_info = {
                                "total_params": total_params,
                                "changed_params": changed_params,
                                "max_change": max_change,
                                "avg_change": total_change / changed_params if changed_params > 0 else 0.0,
                                "change_ratio": changed_params / total_params
                            }
                except Exception as e:
                    param_change_info = {"error": str(e)}
            
            # æ‰“å°å‚æ•°å˜åŒ–ä¿¡æ¯ï¼ˆæ¯5æ­¥æˆ–50æ­¥ï¼‰
            if param_change_info and "error" not in param_change_info:
                if self.training_stats["step"] < 10 or self.training_stats["step"] % 50 == 0:
                    self.logger.info(
                        f"ğŸ“Š Step {self.training_stats['step'] + 1} - å‚æ•°æ›´æ–°: "
                        f"{param_change_info['changed_params']}/{param_change_info['total_params']} ä¸ªå‚æ•°æœ‰å˜åŒ–, "
                        f"æœ€å¤§å˜åŒ–={param_change_info['max_change']:.8f}, "
                        f"å¹³å‡å˜åŒ–={param_change_info['avg_change']:.8f}"
                    )
                    
                    # å¦‚æœå‚æ•°å®Œå…¨æ²¡æœ‰å˜åŒ–ï¼Œå‘å‡ºè­¦å‘Š
                    if param_change_info['changed_params'] == 0 and self.training_stats["step"] >= 5:
                        self.logger.warning(
                            f"âš ï¸ Step {self.training_stats['step'] + 1} - æ¨¡å‹å‚æ•°å®Œå…¨æ²¡æœ‰æ›´æ–°ï¼"
                            f"è¿™å¯èƒ½è¡¨æ˜ç­–ç•¥è®­ç»ƒæœ‰é—®é¢˜ï¼ˆæ¢¯åº¦ä¸º0æˆ–ä¼˜åŒ–å™¨æœªæ‰§è¡Œï¼‰ã€‚"
                        )
            
            # ğŸ” ä½¿ç”¨hookè·å–çš„æ¢¯åº¦èŒƒæ•°ï¼ˆä¼˜å…ˆï¼‰
            grad_norm = grad_norm_from_hook
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨CPUå‰¯æœ¬ï¼‰
            self._update_training_stats(stats, combined_rewards_cpu)
            
            # è®°å½•åˆ°wandb
            if self.config.get("logging", {}).get("use_wandb", False):
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½è½¬æ¢ä¸ºPythonæ ‡é‡
                def to_scalar(value, default=0):
                    if value is None:
                        return default
                    if isinstance(value, (np.ndarray, np.generic)):
                        return float(value.item() if hasattr(value, 'item') else float(value))
                    if isinstance(value, torch.Tensor):
                        return float(value.item() if hasattr(value, 'item') else float(value))
                    return float(value)
                
                # ğŸ” æ­£ç¡®è·å–losså€¼ï¼šä¸è¦ç”¨clipfracå†’å……loss
                # å…ˆå°è¯•è·å–çœŸæ­£çš„losså€¼
                policy_loss_key = None
                value_loss_key = None
                for key in stats.keys():
                    if 'policy' in key and 'loss' in key and 'clip' not in key:
                        policy_loss_key = key
                    if 'value' in key and 'loss' in key and 'clip' not in key:
                        value_loss_key = key
                
                log_data = {
                    "step": self.training_stats["step"],
                    "mean_reward": mean_reward_for_stats,  # âœ… ä½¿ç”¨å·²ä¿å­˜çš„å‡å€¼
                    # ğŸ” ä½¿ç”¨æ­£ç¡®çš„é”®åè·å–lossï¼ˆä»æ—¥å¿—ä¸­å‘ç°å®é™…é”®åæ˜¯ppo/loss/policyå’Œppo/loss/valueï¼‰
                    "policy_loss": to_scalar(stats.get(policy_loss_key) if policy_loss_key else stats.get("ppo/loss/policy") or stats.get("ppo/policy/loss", 0)),
                    "value_loss": to_scalar(stats.get(value_loss_key) if value_loss_key else stats.get("ppo/loss/value") or stats.get("ppo/val/loss", 0)),
                    "kl_divergence": to_scalar(stats.get("ppo/policy/kl") or stats.get("objective/kl") or 0),
                    "policy_clipfrac": to_scalar(stats.get("ppo/policy/clipfrac") or 0),  # âœ… Clipç‡
                    "value_clipfrac": to_scalar(stats.get("ppo/val/clipfrac") or 0),  # âœ… ä»·å€¼å‡½æ•°Clipç‡
                    "objective/clipfrac": to_scalar(stats.get("objective/clipfrac") or 0),
                    "objective/entropy": to_scalar(stats.get("objective/entropy") or stats.get("ppo/policy/entropy") or 0),
                }
                
                # æ·»åŠ æ¢¯åº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if grad_norm is not None:
                    log_data["grad_norm"] = grad_norm
                else:
                    # å°è¯•ä»statsä¸­è·å–ï¼ˆæŸäº›PPOå®ç°å¯èƒ½ä¼šè®°å½•æ¢¯åº¦èŒƒæ•°ï¼‰
                    if isinstance(stats, dict):
                        grad_norm_from_stats = stats.get("ppo/grad_norm") or stats.get("train/grad_norm") or stats.get("grad_norm")
                        if grad_norm_from_stats is not None:
                            log_data["grad_norm"] = to_scalar(grad_norm_from_stats)
                
                # æ·»åŠ å‚æ•°å˜åŒ–ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if param_change_info and "error" not in param_change_info:
                    log_data.update({
                        "param_change/ratio": param_change_info.get("change_ratio", 0.0),
                        "param_change/changed_count": param_change_info.get("changed_params", 0),
                        "param_change/total_count": param_change_info.get("total_params", 0),
                        "param_change/max": param_change_info.get("max_change", 0.0),
                        "param_change/avg": param_change_info.get("avg_change", 0.0),
                    })
                
                # ğŸ” è¯Šæ–­ï¼šå¦‚æœlossä¸º0ï¼Œæ‰“å°æ‰€æœ‰statsé”®å¸®åŠ©è°ƒè¯•
                if log_data["policy_loss"] == 0 and log_data["value_loss"] == 0:
                    self.logger.warning(f"âš ï¸ PPO losses are zero! Available stats keys: {list(stats.keys())}")
                
                # æ·»åŠ è‡ªé€‚åº”æƒé‡ä¿¡æ¯
                if self.config["reward"].get("use_adaptive_weights", True):
                    weight_stats = self.reward_combiner.get_statistics()
                    if "adaptive_weights" in weight_stats:
                        log_data.update({
                            "adaptive_weight_intrinsic": float(weight_stats["adaptive_weights"].get("intrinsic", 0.0)),
                            "adaptive_weight_correctness": float(weight_stats["adaptive_weights"].get("correctness", 0.0))
                        })
                    if "weight_performance" in weight_stats:
                        log_data.update({
                            "weight_performance_intrinsic": float(weight_stats["weight_performance"].get("intrinsic", 0.0)),
                            "weight_performance_correctness": float(weight_stats["weight_performance"].get("correctness", 0.0))
                        })
                    
                    # æ·»åŠ æƒé‡å˜åŒ–è¶‹åŠ¿
                    if "weight_trend" in weight_stats and weight_stats["weight_trend"]:
                        log_data.update({
                            "weight_trend_intrinsic": float(weight_stats["weight_trend"].get("intrinsic", 0.0)),
                            "weight_trend_correctness": float(weight_stats["weight_trend"].get("correctness", 0.0))
                        })
                
                wandb.log(log_data)
            
            self.training_stats["step"] += 1
            
            # ğŸ”¥ æ¯æ­¥ç»“æŸå‰æ¸…ç†æ˜¾å­˜ï¼Œé¿å…ç´¯ç§¯å¯¼è‡´OOM
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            return stats
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
            raise
    
    def _update_training_stats(self, stats: Dict, rewards: torch.Tensor):
        """æ›´æ–°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        # æ£€æŸ¥statsæ˜¯å¦ä¸ºNone
        if stats is None:
            self.logger.warning("statsä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼æ›´æ–°è®­ç»ƒç»Ÿè®¡")
            stats = {}
        
        # ğŸ” ç¡®ä¿rewardsæ˜¯tensorï¼Œç„¶åè½¬æ¢ä¸ºPythonæ ‡é‡
        if isinstance(rewards, torch.Tensor):
            mean_reward = torch.mean(rewards).item()
        elif isinstance(rewards, (list, tuple)):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºtensorå†è®¡ç®—
            mean_reward = float(np.mean([float(r.item() if hasattr(r, 'item') else float(r)) for r in rewards]))
        else:
            mean_reward = float(rewards) if isinstance(rewards, (int, float)) else 0.0
        
        self.training_stats["total_rewards"].append(mean_reward)
        
        # ç¡®ä¿ä»statsä¸­è·å–çš„å€¼è½¬æ¢ä¸ºPythonæ ‡é‡
        def to_scalar(value, default=0):
            if value is None:
                return default
            if isinstance(value, (np.ndarray, np.generic)):
                return float(value.item() if hasattr(value, 'item') else float(value))
            if isinstance(value, torch.Tensor):
                return float(value.item() if hasattr(value, 'item') else float(value))
            if isinstance(value, (np.int64, np.int32)):
                return int(value)
            if isinstance(value, (np.float64, np.float32)):
                return float(value)
            return float(value)
        
        # ğŸ” ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°ï¼Œä¸åº”ç”¨clipfracå†’å……loss
        # TRLå¯èƒ½ä½¿ç”¨ä¸åŒçš„é”®åï¼šppo/policy/loss, objective/policy_lossç­‰
        # å…ˆæ‰¾åˆ°çœŸæ­£çš„lossé”®
        policy_loss_key = None
        value_loss_key = None
        if isinstance(stats, dict):
            # ğŸ” è¯Šæ–­ï¼šæ‰“å°æ‰€æœ‰å¯ç”¨çš„é”®ï¼ˆåªåœ¨ç¬¬ä¸€æ­¥ï¼‰
            if len(self.training_stats["policy_losses"]) == 0:
                self.logger.info(f"ğŸ” ç¬¬1æ­¥statså­—å…¸çš„æ‰€æœ‰é”®: {list(stats.keys())}")
            
            # å°è¯•å¤šç§å¯èƒ½çš„é”®åï¼ˆæ ¹æ®å®é™…æ—¥å¿—ï¼ŒTRLä½¿ç”¨ppo/loss/policyæ ¼å¼ï¼‰
            possible_policy_keys = [
                'ppo/loss/policy',  # âœ… å®é™…é”®åï¼ˆä»æ—¥å¿—ä¸­å‘ç°ï¼‰
                'ppo/policy/loss', 'objective/policy_loss', 'policy_loss',
                'ppo/policy/clipped_objective', 'objective/clipped_surrogate',
                'train/policy/loss', 'loss/policy'
            ]
            possible_value_keys = [
                'ppo/loss/value',  # âœ… å®é™…é”®åï¼ˆä»æ—¥å¿—ä¸­å‘ç°ï¼‰
                'ppo/value/loss', 'ppo/val/loss', 'value_loss',
                'objective/value_loss', 'train/value/loss', 'loss/value'
            ]
            
            # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            for key in stats.keys():
                key_lower = key.lower()
                if 'policy' in key_lower and 'loss' in key_lower and 'clip' not in key_lower:
                    policy_loss_key = key
                if 'value' in key_lower and 'loss' in key_lower and 'clip' not in key_lower:
                    value_loss_key = key
            
            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å¯èƒ½çš„é”®å
            if policy_loss_key is None:
                for possible_key in possible_policy_keys:
                    if possible_key in stats:
                        policy_loss_key = possible_key
                        break
            
            if value_loss_key is None:
                for possible_key in possible_value_keys:
                    if possible_key in stats:
                        value_loss_key = possible_key
                        break
        
        # ğŸ” è¯Šæ–­ï¼šå¦‚æœæ‰¾ä¸åˆ°policy lossé”®ï¼Œè®°å½•è­¦å‘Šå¹¶æ‰“å°æ‰€æœ‰é”®
        if policy_loss_key is None and isinstance(stats, dict):
            if len(self.training_stats["policy_losses"]) < 10:  # å‰10æ­¥è­¦å‘Š
                self.logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°policy_lossé”®ï¼")
                self.logger.warning(f"   å¯ç”¨é”®: {list(stats.keys())}")
                self.logger.warning(f"   å°è¯•çš„é”®å: {possible_policy_keys}")
                # å°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«'loss'çš„é”®
                loss_keys = [k for k in stats.keys() if 'loss' in k.lower()]
                if loss_keys:
                    self.logger.warning(f"   åŒ…å«'loss'çš„é”®: {loss_keys}")
        
        if value_loss_key is None and isinstance(stats, dict):
            if len(self.training_stats["value_losses"]) < 10:  # å‰10æ­¥è­¦å‘Š
                self.logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°value_lossé”®ï¼")
                self.logger.warning(f"   å¯ç”¨é”®: {list(stats.keys())}")
        
        # å°è¯•è·å–æŸå¤±å€¼ï¼Œå¦‚æœæ‰¾ä¸åˆ°å°±ä½¿ç”¨é»˜è®¤å€¼0ï¼Œä½†è®°å½•è­¦å‘Š
        if isinstance(stats, dict):
            if policy_loss_key:
                policy_loss_value = to_scalar(stats.get(policy_loss_key), 0)
            else:
                # å°è¯•æœ€åä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å€¼
                policy_loss_value = to_scalar(
                    stats.get("ppo/loss/policy") or  # âœ… å®é™…é”®åï¼ˆä»æ—¥å¿—ä¸­å‘ç°ï¼‰
                    stats.get("ppo/policy/loss") or 
                    stats.get("objective/policy_loss") or 
                    stats.get("policy_loss") or 0
                )
                if policy_loss_value == 0 and len(self.training_stats["policy_losses"]) < 5:
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŸå¤±ç›¸å…³çš„é”®éƒ½æ˜¯0æˆ–è€…ä¸å­˜åœ¨
                    all_loss_zero = True
                    for key in stats.keys():
                        if 'loss' in key.lower() and to_scalar(stats.get(key), -1) != 0:
                            all_loss_zero = False
                            break
                    if all_loss_zero:
                        self.logger.error(f"âŒ æ‰€æœ‰æŸå¤±ç›¸å…³çš„é”®éƒ½ä¸º0æˆ–ä¸å­˜åœ¨ï¼")
                        self.logger.error(f"   è¿™å¯èƒ½æ˜¯PPOè®­ç»ƒæœªæ­£ç¡®æ‰§è¡Œï¼")
            
            if value_loss_key:
                value_loss_value = to_scalar(stats.get(value_loss_key), 0)
            else:
                value_loss_value = to_scalar(
                    stats.get("ppo/loss/value") or  # âœ… å®é™…é”®åï¼ˆä»æ—¥å¿—ä¸­å‘ç°ï¼‰
                    stats.get("ppo/value/loss") or 
                    stats.get("ppo/val/loss") or 
                    stats.get("value_loss") or 0
                )
        else:
            policy_loss_value = 0
            value_loss_value = 0
        
        # ğŸ” ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡ï¼ˆé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯ï¼‰
        self.training_stats["policy_losses"].append(float(policy_loss_value))
        self.training_stats["value_losses"].append(float(value_loss_value))
        kl_value = to_scalar(stats.get("ppo/policy/kl") or stats.get("objective/kl") or 0 if isinstance(stats, dict) else 0)
        self.training_stats["kl_divergences"].append(float(kl_value))
        
        # ä¿æŒæœ€è¿‘1000æ­¥çš„ç»Ÿè®¡
        max_history = 1000
        for key in self.training_stats:
            if isinstance(self.training_stats[key], list) and len(self.training_stats[key]) > max_history:
                self.training_stats[key] = self.training_stats[key][-max_history:]
        
        # å®šæœŸæ¸…ç†å†…å­˜
        current_step = len(self.training_stats["total_rewards"])
        self._cleanup_memory(current_step)
    
    def train(self, train_dataset, max_steps: Optional[int] = None):
        """å¼€å§‹è®­ç»ƒï¼ˆæ”¯æŒå¹¶è¡Œæ•°æ®åŠ è½½ï¼‰"""
        try:
            max_steps = max_steps or self.config["training"]["max_steps"]
            
            # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
            current_step = self.training_stats.get("step", 0)
            start_step = current_step
            remaining_steps = max_steps - current_step
            
            if current_step > 0:
                self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œå½“å‰æ­¥æ•°: {current_step}")
                self.logger.info(f"å‰©ä½™è®­ç»ƒæ­¥æ•°: {remaining_steps}")
            else:
                self.logger.info(f"å¼€å§‹æ–°çš„RLè®­ç»ƒï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
            
            # è®¾ç½®å¹¶è¡Œæ•°æ®åŠ è½½å™¨
            use_parallel = self._use_parallel
            if use_parallel and self.config.get("parallel", {}).get("use_parallel_data_loader", True):
                batch_size = self.config["ppo"]["batch_size"]
                num_workers = self.config.get("parallel", {}).get("data_loader_workers", 4)
                self.parallel_data_loader = ParallelDataLoader(
                    train_dataset, batch_size, num_workers, shuffle=True
                )
                self.logger.info("å¹¶è¡Œæ•°æ®åŠ è½½å™¨å·²å¯ç”¨")
            
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = tqdm(
                range(start_step, max_steps), 
                initial=current_step,
                desc="RLè®­ç»ƒè¿›åº¦", 
                unit="step",
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
            )
            
            # è®­ç»ƒç»Ÿè®¡
            start_time = time.time()
            step_times = []
            
            for step in progress_bar:
                step_start_time = time.time()
                
                # â­â­â­â­â˜† æ¯2æ­¥å¼ºåˆ¶æ¸…ç†å†…å­˜ï¼ˆæ›´é¢‘ç¹ï¼‰
                if step > 0 and step % self._force_cleanup_every_n_steps == 0:
                    self._cleanup_memory(step, force=True)
                
                # åˆ›å»ºæ‰¹æ¬¡æ•°æ®
                batch = None  # âœ… åˆå§‹åŒ–batchå˜é‡ï¼Œé¿å…UnboundLocalError
                try:
                    batch = self._create_batch(train_dataset)
                    
                    # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                    stats = self.train_step(batch)
                except torch.cuda.OutOfMemoryError as e:
                    # ğŸ”¥ OOMé”™è¯¯å¤„ç†ï¼šæ¸…ç†å†…å­˜
                    self.logger.error(f"âŒ Step {step + 1} OOMé”™è¯¯ï¼Œæ¸…ç†å†…å­˜...")
                    self._cleanup_memory(step, force=True)
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    raise RuntimeError(f"OOMé”™è¯¯ï¼Œå»ºè®®è¿›ä¸€æ­¥å‡å°batch_sizeæˆ–max_length: {e}")
                except RuntimeError as e:
                    error_str = str(e)
                    # ğŸ” CUDA device-side asserté”™è¯¯å¤„ç†
                    if "device-side assert" in error_str or "CUDA error" in error_str:
                        self.logger.error(f"âŒ Step {step + 1} CUDA device-side asserté”™è¯¯")
                        self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_str[:200]}")
                        # æ¸…ç†CUDAç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                        # ğŸ” å…³é”®ï¼šCUDAé”™è¯¯å¯èƒ½æŸåæ¨¡å‹çŠ¶æ€ï¼Œéœ€è¦è·³è¿‡è¿™ä¸€æ­¥
                        self.logger.warning(f"   è·³è¿‡æ­¤è®­ç»ƒæ­¥éª¤ï¼Œç»§ç»­è®­ç»ƒ...")
                        stats = None  # æ ‡è®°ä¸ºå¤±è´¥
                    else:
                        # å…¶ä»–RuntimeErrorç›´æ¥æŠ›å‡º
                        raise
                finally:
                    # âœ… ä¿®å¤ï¼šå®‰å…¨æ¸…ç†batchï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    if batch is not None:
                        try:
                            del batch
                        except:
                            pass
                    
                    # å®šæœŸæ¸…ç†å†…å­˜
                    if step % self._memory_cleanup_interval == 0:
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                
                # æ£€æŸ¥train_stepæ˜¯å¦æˆåŠŸï¼ˆå¯èƒ½è¿”å›Noneï¼‰
                if stats is None:
                    self.logger.warning(f"è®­ç»ƒæ­¥éª¤ {step + 1} å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡æ›´æ–°")
                    # è®¡ç®—æ­¥éª¤æ—¶é—´
                    step_time = time.time() - step_start_time
                    step_times.append(step_time)
                    # ä½¿ç”¨ç©ºçš„statså­—å…¸ç»§ç»­
                    stats = {}
                    continue
                
                # è®¡ç®—æ­¥éª¤æ—¶é—´
                step_time = time.time() - step_start_time
                step_times.append(step_time)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                avg_reward = float(np.mean(self.training_stats["total_rewards"][-10:])) if self.training_stats["total_rewards"] else 0.0
                avg_step_time = float(np.mean(step_times[-10:])) if step_times and len(step_times) > 0 else 0.0
                progress_bar.set_postfix({
                    'avg_reward': f'{avg_reward:.4f}',
                    'step_time': f'{step_time:.2f}s',
                    'avg_step_time': f'{avg_step_time:.2f}s'
                })
                
                # å®šæœŸä¿å­˜å’Œè¯„ä¼°
                if (step + 1) % self.config["training"]["save_steps"] == 0:
                    try:
                        self.save_checkpoint(step + 1)
                        progress_bar.write(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: step {step + 1}")
                    except Exception as e:
                        # ğŸ” å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥ä¸åº”è¯¥ä¸­æ–­è®­ç»ƒ
                        self.logger.error(f"âŒ Step {step + 1} æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
                        self.logger.error(f"   è®­ç»ƒå°†ç»§ç»­ï¼Œä½†æ­¤æ£€æŸ¥ç‚¹å¯èƒ½ä¸å®Œæ•´æˆ–ä¸¢å¤±")
                        import traceback
                        self.logger.error(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è®­ç»ƒ
                    
                    # ä¿å­˜è‡ªé€‚åº”æƒé‡
                    if self.config["reward"].get("use_adaptive_weights", True):
                        weight_file = f"{self.config['ppo']['output_dir']}/adaptive_weights_step_{step + 1}.json"
                        self.reward_combiner.save_weights(weight_file)
                
                if (step + 1) % self.config["training"]["eval_steps"] == 0:
                    self.evaluate_model()
                    progress_bar.write(f"ğŸ“Š æ¨¡å‹è¯„ä¼°å®Œæˆ: step {step + 1}")
                
                # æ—¥å¿—è¾“å‡º
                if (step + 1) % self.config["training"]["logging_steps"] == 0:
                    # ç¡®ä¿statsä¸æ˜¯None
                    if stats is not None:
                        self._log_training_progress(step + 1, stats)
                    else:
                        self.logger.warning(f"æ­¥éª¤ {step + 1} çš„statsä¸ºNoneï¼Œè·³è¿‡æ—¥å¿—è®°å½•")
                    progress_bar.write(f"ğŸ“ è®­ç»ƒæ—¥å¿—: step {step + 1}")
                    
                    # è¾“å‡ºè‡ªé€‚åº”æƒé‡çŠ¶æ€
                    if self.config["reward"].get("use_adaptive_weights", True):
                        weight_stats = self.reward_combiner.get_statistics()
                        if "adaptive_weights" in weight_stats:
                            intrinsic_weight = float(weight_stats['adaptive_weights'].get('intrinsic', 0.0))
                            correctness_weight = float(weight_stats['adaptive_weights'].get('correctness', 0.0))
                            progress_bar.write(f"ğŸ¯ è‡ªé€‚åº”æƒé‡: å†…åœ¨={intrinsic_weight:.4f}, "
                                             f"æ­£ç¡®æ€§={correctness_weight:.4f}")
            
            # å…³é—­è¿›åº¦æ¡
            progress_bar.close()
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_dir = self.config['ppo']['output_dir']
            self.save_final_model(final_model_dir)
            
            # è®­ç»ƒå®Œæˆç»Ÿè®¡
            total_time = time.time() - start_time
            avg_step_time = float(np.mean(step_times)) if step_times and len(step_times) > 0 else 0.0
            
            self.logger.info("ğŸ‰ RLè®­ç»ƒå®Œæˆ!")
            self.logger.info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}ç§’")
            self.logger.info(f"âš¡ å¹³å‡æ¯æ­¥æ—¶é—´: {float(avg_step_time):.2f}ç§’")
            final_avg_reward = float(np.mean(self.training_stats['total_rewards'][-10:])) if self.training_stats['total_rewards'] else 0.0
            self.logger.info(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.4f}")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _create_batch(self, dataset) -> Dict[str, List[str]]:
        """åˆ›å»ºæ‰¹æ¬¡æ•°æ®ï¼ˆæ”¯æŒå¹¶è¡Œæ•°æ®åŠ è½½ï¼‰"""
        batch_size = self.config["ppo"]["batch_size"]
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        if len(dataset) == 0:
            raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºæ‰¹æ¬¡")
        
        # å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæ•°æ®é›†å¤§å°ï¼Œä½¿ç”¨æ•°æ®é›†å¤§å°å¹¶å…è®¸é‡å¤é‡‡æ ·
        actual_batch_size = min(batch_size, len(dataset))
        replace = actual_batch_size > len(dataset)
        
        # ä½¿ç”¨å¹¶è¡Œæ•°æ®åŠ è½½å™¨æˆ–ç›´æ¥éšæœºé€‰æ‹©
        if self.parallel_data_loader:
            # ä½¿ç”¨å¹¶è¡Œæ•°æ®åŠ è½½å™¨
            try:
                batch_data = next(iter(self.parallel_data_loader))
                questions = [item["question"] for item in batch_data]
            except StopIteration:
                # å¦‚æœæ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼Œå›é€€åˆ°éšæœºé€‰æ‹©
                self.logger.warning("å¹¶è¡Œæ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼Œå›é€€åˆ°éšæœºé€‰æ‹©")
                indices = np.random.choice(len(dataset), size=actual_batch_size, replace=replace)
                questions = [dataset[int(i)]["question"] for i in indices]
        else:
            # éšæœºé€‰æ‹©æ ·æœ¬
            indices = np.random.choice(len(dataset), size=actual_batch_size, replace=replace)
            # å°† numpy.int64 è½¬æ¢ä¸º Python intï¼Œé¿å… TypeError
            questions = [dataset[int(i)]["question"] for i in indices]
        
        return {"questions": questions}
    
    def save_checkpoint(self, step: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = f"{self.config['ppo']['output_dir']}/checkpoint-{step}"
        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        save_success = True
        failed_parts = []
        
        # ä¿å­˜å­¦ç”Ÿæ¨¡å‹
        try:
            self.student_model.save_model(checkpoint_dir)
            self.logger.debug("âœ“ å­¦ç”Ÿæ¨¡å‹å·²ä¿å­˜")
        except Exception as e:
            self.logger.error(f"âŒ å­¦ç”Ÿæ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            save_success = False
            failed_parts.append("student_model")
        
        # ä¿å­˜ç¼“å­˜
        try:
            cache_file = os.path.join(checkpoint_dir, "teacher_cache.pkl")
            self.cache_manager.save_cache(cache_file)
            self.logger.debug("âœ“ æ•™å¸ˆç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            self.logger.error(f"âŒ æ•™å¸ˆç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            # ç¼“å­˜å¤±è´¥ä¸è§†ä¸ºä¸¥é‡é”™è¯¯ï¼Œåªè®°å½•è­¦å‘Š
            failed_parts.append("teacher_cache")
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        try:
            stats_file = os.path.join(checkpoint_dir, "training_stats.json")
            import json
            
            # ğŸ” ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            def make_json_serializable(obj):
                """é€’å½’åœ°å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
                if isinstance(obj, dict):
                    return {k: make_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [make_json_serializable(item) for item in obj]
                elif isinstance(obj, (np.ndarray, np.generic)):
                    return float(obj.item() if hasattr(obj, 'item') else float(obj))
                elif isinstance(obj, torch.Tensor):
                    return float(obj.item() if hasattr(obj, 'item') else float(obj))
                elif isinstance(obj, (np.int64, np.int32)):
                    return int(obj)
                elif isinstance(obj, (np.float64, np.float32)):
                    return float(obj)
                elif isinstance(obj, (int, float, str, bool)) or obj is None:
                    return obj
                else:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆæœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼‰
                    try:
                        return str(obj)
                    except:
                        return None
            
            # åˆ›å»ºå¯åºåˆ—åŒ–çš„è®­ç»ƒç»Ÿè®¡å‰¯æœ¬
            serializable_stats = make_json_serializable(self.training_stats)
            
            # ğŸ” éªŒè¯ä¿å­˜å‰çš„æ•°æ®å®Œæ•´æ€§
            if not isinstance(serializable_stats.get("step"), (int, float)):
                self.logger.warning(f"âš ï¸ 'step'ç±»å‹å¼‚å¸¸: {type(serializable_stats.get('step'))}ï¼Œä¿®å¤ä¸ºint")
                serializable_stats["step"] = int(self.training_stats.get("step", 0))
            
            # éªŒè¯åˆ—è¡¨é•¿åº¦ä¸€è‡´æ€§ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            list_keys = ["total_rewards", "policy_losses", "value_losses", "kl_divergences"]
            list_lengths = {key: len(serializable_stats.get(key, [])) for key in list_keys}
            if len(set(list_lengths.values())) > 1:
                self.logger.warning(f"âš ï¸ è®­ç»ƒç»Ÿè®¡åˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´: {list_lengths}")
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_stats, f, indent=2, ensure_ascii=False)
            
            # éªŒè¯æ–‡ä»¶å·²ä¿å­˜
            if os.path.exists(stats_file):
                file_size = os.path.getsize(stats_file) / 1024  # KB
                self.logger.debug(f"âœ“ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜ ({file_size:.2f} KB)")
            else:
                raise FileNotFoundError(f"è®­ç»ƒç»Ÿè®¡æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {stats_file}")
        except TypeError as e:
            self.logger.error(f"âŒ è®­ç»ƒç»Ÿè®¡JSONåºåˆ—åŒ–å¤±è´¥ï¼ˆç±»å‹é”™è¯¯ï¼‰: {e}")
            self.logger.error(f"   è¿™å¯èƒ½æ˜¯ç”±äºè®­ç»ƒç»Ÿè®¡ä¸­åŒ…å«ä¸å¯åºåˆ—åŒ–çš„ç±»å‹ï¼ˆå¦‚torch.Tensorï¼‰")
            # å°è¯•ä¿å­˜ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
            try:
                simple_stats = {
                    "step": int(self.training_stats.get("step", 0)),
                    "total_rewards_count": len(self.training_stats.get("total_rewards", [])),
                    "policy_losses_count": len(self.training_stats.get("policy_losses", [])),
                    "value_losses_count": len(self.training_stats.get("value_losses", [])),
                    "kl_divergences_count": len(self.training_stats.get("kl_divergences", [])),
                    "last_10_rewards": [float(r) if isinstance(r, (int, float)) else 0.0 
                                      for r in self.training_stats.get("total_rewards", [])[-10:]],
                    "last_10_policy_losses": [float(l) if isinstance(l, (int, float)) else 0.0 
                                            for l in self.training_stats.get("policy_losses", [])[-10:]],
                    "note": "å®Œæ•´ç»Ÿè®¡åºåˆ—åŒ–å¤±è´¥ï¼Œä»…ä¿å­˜æ‘˜è¦ä¿¡æ¯"
                }
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(simple_stats, f, indent=2, ensure_ascii=False)
                self.logger.warning(f"âš ï¸ å·²ä¿å­˜ç®€åŒ–ç‰ˆè®­ç»ƒç»Ÿè®¡ï¼ˆæ‘˜è¦ä¿¡æ¯ï¼‰")
            except Exception as e2:
                self.logger.error(f"âŒ ä¿å­˜ç®€åŒ–ç‰ˆè®­ç»ƒç»Ÿè®¡ä¹Ÿå¤±è´¥: {e2}")
                save_success = False
                failed_parts.append("training_stats")
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒç»Ÿè®¡ä¿å­˜å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            save_success = False
            failed_parts.append("training_stats")
        
        # æ€»ç»“ä¿å­˜ç»“æœ
        if save_success:
            if failed_parts:
                self.logger.warning(f"âš ï¸ æ£€æŸ¥ç‚¹å·²éƒ¨åˆ†ä¿å­˜: {checkpoint_dir} (å¤±è´¥éƒ¨åˆ†: {failed_parts})")
            else:
                self.logger.info(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
        else:
            self.logger.error(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {checkpoint_dir} (å¤±è´¥éƒ¨åˆ†: {failed_parts})")
            raise RuntimeError(f"æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥ï¼Œå…³é”®ç»„ä»¶æœªä¿å­˜: {failed_parts}")
    
    def load_checkpoint(self, checkpoint_dir: str):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"""
        try:
            import json
            import pickle
            
            if not Path(checkpoint_dir).exists():
                raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
            
            self.logger.info(f"æ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_dir}")
            
            # åŠ è½½æ¨¡å‹ï¼ˆéœ€è¦å…ˆåˆå§‹åŒ–æ¨¡å‹ï¼‰
            if self.student_model is None:
                raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ£€æŸ¥ç‚¹ã€‚è¯·å…ˆè°ƒç”¨ setup_models()")
            
            # åŠ è½½å­¦ç”Ÿæ¨¡å‹æƒé‡
            self.logger.info("åŠ è½½å­¦ç”Ÿæ¨¡å‹æƒé‡...")
            self.student_model.load_model(checkpoint_dir, load_adapter=True)
            
            # é‡æ–°è®¾ç½®PPOæ¨¡å‹ï¼ˆå› ä¸ºæ¨¡å‹æƒé‡å·²æ›´æ–°ï¼‰
            self.logger.info("é‡æ–°è®¾ç½®PPOæ¨¡å‹...")
            self.ppo_model = self.student_model.setup_for_ppo()
            
            # é‡æ–°è®¾ç½®PPOè®­ç»ƒå™¨
            self.logger.info("é‡æ–°è®¾ç½®PPOè®­ç»ƒå™¨...")
            self.setup_ppo_trainer()
            
            # åŠ è½½ç¼“å­˜
            cache_file = os.path.join(checkpoint_dir, "teacher_cache.pkl")
            if os.path.exists(cache_file):
                self.logger.info("åŠ è½½ç¼“å­˜...")
                self.cache_manager.load_cache(cache_file)
            
            # åŠ è½½è®­ç»ƒç»Ÿè®¡
            stats_file = os.path.join(checkpoint_dir, "training_stats.json")
            if os.path.exists(stats_file):
                self.logger.info("åŠ è½½è®­ç»ƒç»Ÿè®¡...")
                try:
                    with open(stats_file, 'r', encoding='utf-8') as f:
                        loaded_stats = json.load(f)
                    
                    # ğŸ” éªŒè¯åŠ è½½çš„ç»Ÿè®¡æ•°æ®çš„å®Œæ•´æ€§
                    required_keys = ["step", "total_rewards", "policy_losses", "value_losses", "kl_divergences"]
                    missing_keys = [key for key in required_keys if key not in loaded_stats]
                    if missing_keys:
                        self.logger.warning(f"âš ï¸ åŠ è½½çš„è®­ç»ƒç»Ÿè®¡ç¼ºå°‘é”®: {missing_keys}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                        # è¡¥å……ç¼ºå¤±çš„é”®
                        for key in missing_keys:
                            if key == "step":
                                loaded_stats[key] = 0
                            else:
                                loaded_stats[key] = []
                    
                    # éªŒè¯æ•°æ®ç±»å‹
                    if not isinstance(loaded_stats.get("step"), int):
                        self.logger.warning(f"âš ï¸ 'step'ç±»å‹ä¸æ­£ç¡®: {type(loaded_stats.get('step'))}ï¼Œè½¬æ¢ä¸ºint")
                        loaded_stats["step"] = int(loaded_stats.get("step", 0))
                    
                    for key in ["total_rewards", "policy_losses", "value_losses", "kl_divergences"]:
                        if key in loaded_stats and not isinstance(loaded_stats[key], list):
                            self.logger.warning(f"âš ï¸ '{key}'ç±»å‹ä¸æ­£ç¡®: {type(loaded_stats[key])}ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨")
                            loaded_stats[key] = []
                    
                    self.training_stats = loaded_stats
                    self.logger.info(f"âœ“ è®­ç»ƒç»Ÿè®¡åŠ è½½æˆåŠŸ")
                except json.JSONDecodeError as e:
                    self.logger.error(f"âŒ è®­ç»ƒç»Ÿè®¡JSONè§£æå¤±è´¥: {e}")
                    self.logger.warning("å°†ä½¿ç”¨ç©ºçš„è®­ç»ƒç»Ÿè®¡ï¼Œä»æ­¥æ•°0å¼€å§‹")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è®­ç»ƒ
                except Exception as e:
                    self.logger.error(f"âŒ åŠ è½½è®­ç»ƒç»Ÿè®¡å¤±è´¥: {e}")
                    import traceback
                    self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    self.logger.warning("å°†ä½¿ç”¨ç©ºçš„è®­ç»ƒç»Ÿè®¡ï¼Œä»æ­¥æ•°0å¼€å§‹")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è®­ç»ƒ
            
            self.logger.info(f"âœ… æˆåŠŸä»æ£€æŸ¥ç‚¹æ¢å¤")
            # ğŸ” ç¡®ä¿stepæ˜¯intç±»å‹
            current_step = int(self.training_stats.get('step', 0))
            self.training_stats['step'] = current_step
            self.logger.info(f"   å½“å‰æ­¥æ•°: {current_step}")
            
            # å®‰å…¨åœ°è®¡ç®—å¹³å‡å¥–åŠ±ï¼ˆé¿å…åˆ—è¡¨ä¸ºç©ºæˆ–ç±»å‹é”™è¯¯ï¼‰
            rewards_list = self.training_stats.get('total_rewards', [])
            if rewards_list and len(rewards_list) > 0:
                try:
                    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•°å€¼ç±»å‹
                    numeric_rewards = [float(r) for r in rewards_list[-100:] if isinstance(r, (int, float))]
                    if numeric_rewards:
                        avg_reward = np.mean(numeric_rewards)
                        self.logger.info(f"   å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
                    else:
                        self.logger.warning("   å¹³å‡å¥–åŠ±: æ— æœ‰æ•ˆå¥–åŠ±æ•°æ®")
                except Exception as e:
                    self.logger.warning(f"   è®¡ç®—å¹³å‡å¥–åŠ±å¤±è´¥: {e}")
            else:
                self.logger.warning("   å¹³å‡å¥–åŠ±: å¥–åŠ±åˆ—è¡¨ä¸ºç©º")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            raise
    
    def save_final_model(self, save_path: str):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹"""
        try:
            Path(save_path).mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜å­¦ç”Ÿæ¨¡å‹
            self.student_model.save_model(save_path)
            
            # ä¿å­˜è®­ç»ƒé…ç½®
            config_file = os.path.join(save_path, "training_config.yaml")
            import yaml
            with open(config_file, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            
            # ä¿å­˜æœ€ç»ˆè®­ç»ƒç»Ÿè®¡
            final_stats_file = os.path.join(save_path, "final_training_stats.json")
            import json
            
            # ğŸ” ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„ï¼ˆå¤ç”¨ç›¸åŒçš„å‡½æ•°ï¼‰
            def make_json_serializable(obj):
                """é€’å½’åœ°å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
                if isinstance(obj, dict):
                    return {k: make_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [make_json_serializable(item) for item in obj]
                elif isinstance(obj, (np.ndarray, np.generic)):
                    return float(obj.item() if hasattr(obj, 'item') else float(obj))
                elif isinstance(obj, torch.Tensor):
                    return float(obj.item() if hasattr(obj, 'item') else float(obj))
                elif isinstance(obj, (np.int64, np.int32)):
                    return int(obj)
                elif isinstance(obj, (np.float64, np.float32)):
                    return float(obj)
                elif isinstance(obj, (int, float, str, bool)) or obj is None:
                    return obj
                else:
                    try:
                        return str(obj)
                    except:
                        return None
            
            # åˆ›å»ºå¯åºåˆ—åŒ–çš„è®­ç»ƒç»Ÿè®¡å‰¯æœ¬
            serializable_stats = make_json_serializable(self.training_stats)
            
            # ğŸ” éªŒè¯ä¿å­˜å‰çš„æ•°æ®å®Œæ•´æ€§
            if not isinstance(serializable_stats.get("step"), (int, float)):
                self.logger.warning(f"âš ï¸ 'step'ç±»å‹å¼‚å¸¸: {type(serializable_stats.get('step'))}ï¼Œä¿®å¤ä¸ºint")
                serializable_stats["step"] = int(self.training_stats.get("step", 0))
            
            with open(final_stats_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_stats, f, indent=2, ensure_ascii=False)
            
            # éªŒè¯æ–‡ä»¶å·²ä¿å­˜
            if os.path.exists(final_stats_file):
                file_size = os.path.getsize(final_stats_file) / 1024  # KB
                self.logger.info(f"âœ“ æœ€ç»ˆè®­ç»ƒç»Ÿè®¡å·²ä¿å­˜ ({file_size:.2f} KB)")
            else:
                raise FileNotFoundError(f"æœ€ç»ˆè®­ç»ƒç»Ÿè®¡æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {final_stats_file}")
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            model_info = {
                "model_type": "RL_trained_student_model",
                "base_model": self.config["model"]["student_model_name"],
                "teacher_model": self.config["model"]["teacher_model_name"],
                "training_steps": self.training_stats["step"],
                "final_reward": self.training_stats["total_rewards"][-1] if self.training_stats["total_rewards"] else 0.0,
                "training_date": str(Path().cwd()),
                "config_summary": {
                    "ppo_learning_rate": self.config["ppo"]["learning_rate"],
                    "ppo_epochs": self.config["ppo"]["ppo_epochs"],
                    "reward_lambda_intrinsic": self.config["reward"]["lambda_intrinsic"],
                    "reward_lambda_correctness": self.config["reward"]["lambda_correctness"]
                }
            }
            
            model_info_file = os.path.join(save_path, "model_info.json")
            with open(model_info_file, 'w') as f:
                json.dump(model_info, f, indent=2)
            
            self.logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")
            
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„è¯„ä¼°é€»è¾‘
            # ä¾‹å¦‚åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•å‡†ç¡®ç‡ç­‰
            
            avg_reward = np.mean(self.training_stats["total_rewards"][-100:]) if self.training_stats["total_rewards"] else 0.0
            
            self.logger.info(f"æ¨¡å‹è¯„ä¼° - å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
            
            if self.config.get("logging", {}).get("use_wandb", False):
                wandb.log({
                    "eval/avg_reward": avg_reward,
                    "eval/step": self.training_stats["step"]
                })
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
    
    def _log_training_progress(self, step: int, stats: Dict):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        # æ£€æŸ¥statsæ˜¯å¦ä¸ºNone
        if stats is None:
            self.logger.warning(f"æ­¥éª¤ {step} çš„statsä¸ºNoneï¼Œæ— æ³•è®°å½•è®­ç»ƒè¿›åº¦")
            return
        
        # ç¡®ä¿è½¬æ¢ä¸ºPythonæ ‡é‡ï¼Œé¿å…numpyæ•°ç»„æ ¼å¼åŒ–é”™è¯¯
        avg_reward = float(np.mean(self.training_stats["total_rewards"][-100:])) if self.training_stats["total_rewards"] else 0.0
        avg_kl = float(np.mean(self.training_stats["kl_divergences"][-100:])) if self.training_stats["kl_divergences"] else 0.0
        
        # ä»statsä¸­è·å–å€¼å¹¶è½¬æ¢ä¸ºPythonæ ‡é‡
        # ğŸ” æ­£ç¡®è·å–losså€¼ï¼šä¸ä½¿ç”¨clipfracå†’å……loss
        policy_loss = 0.0
        if isinstance(stats, dict):
            # å…ˆå°è¯•æ‰¾åˆ°çœŸæ­£çš„lossé”®
            for key in stats.keys():
                if 'policy' in key and 'loss' in key and 'clip' not in key:
                    policy_loss = stats[key]
                    break
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æ ‡å‡†é”®å
                policy_loss = stats.get('ppo/policy/loss', 0)
            
            if isinstance(policy_loss, (np.ndarray, np.generic)):
                policy_loss = float(policy_loss.item() if hasattr(policy_loss, 'item') else float(policy_loss))
            else:
                policy_loss = float(policy_loss)
        
        # åŸºç¡€è®­ç»ƒä¿¡æ¯
        log_message = (
            f"Step {step}: "
            f"Avg Reward: {avg_reward:.4f}, "
            f"Avg KL: {avg_kl:.4f}, "
            f"Policy Loss: {policy_loss:.4f}"
        )
        
        # å¦‚æœå¯ç”¨äº†è‡ªé€‚åº”æƒé‡ï¼Œæ·»åŠ æƒé‡ä¿¡æ¯
        if self.config["reward"].get("use_adaptive_weights", False):
            current_weights = self.reward_combiner.get_current_weights()
            log_message += (
                f" | Weights - Intrinsic: {current_weights['intrinsic']:.3f}, "
                f"Correctness: {current_weights['correctness']:.3f}"
            )
            if current_weights.get('reasoning', 0) > 0:
                log_message += f", Reasoning: {current_weights['reasoning']:.3f}"
            if current_weights.get('format', 0) > 0:
                log_message += f", Format: {current_weights['format']:.3f}"
        
        self.logger.info(log_message)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢å¼‚æ­¥ç¼“å­˜å·¥ä½œçº¿ç¨‹
            if self.async_cache_manager:
                self.async_cache_manager.stop_async_worker()
            
            # æ¸…ç†ç¼“å­˜ç®¡ç†å™¨
            if self.cache_manager:
                self.cache_manager.cleanup()
            
            # æ¸…ç†å¹¶è¡Œå¤„ç†å™¨
            if self.parallel_processor:
                self.parallel_processor = None
            
            # æ¸…ç†å¹¶è¡Œæ¨ç†å™¨
            if self.parallel_inference_student:
                self.parallel_inference_student = None
            if self.parallel_inference_teacher:
                self.parallel_inference_teacher = None
            
            # æ¸…ç†å¹¶è¡Œæ•°æ®åŠ è½½å™¨
            if self.parallel_data_loader:
                self.parallel_data_loader = None
            
            if self.config.get("logging", {}).get("use_wandb", False):
                wandb.finish()
            
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config("config/training_config.yaml")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RLTrainer(config)
    
    # è®¾ç½®æ¨¡å‹å’Œç»„ä»¶
    trainer.setup_models()
    trainer.setup_components()
    trainer.setup_ppo_trainer()
    
    # åŠ è½½GSM8Kæ•°æ®é›†
    from datasets import load_dataset
    from data.gsm8k_processor import GSM8KProcessor
    
    print("æ­£åœ¨åŠ è½½GSM8Kæ•°æ®é›†...")
    try:
        # åŠ è½½GSM8Kæ•°æ®é›†
        gsm8k_dataset = load_dataset("gsm8k", "main")
        
        # åˆ›å»ºGSM8Kå¤„ç†å™¨
        processor = GSM8KProcessor(trainer.student_model.tokenizer, max_length=config["ppo"]["max_length"])
        
        # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºè®­ç»ƒæ•°æ®
        dataset = gsm8k_dataset["train"]
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(dataset)}")
        
        # éªŒè¯æ•°æ®é›†è´¨é‡
        processor.validate_data(dataset, num_samples=3)
        
        # è®­ç»ƒ
        trainer.train(dataset, max_steps=100)
        
    except Exception as e:
        print(f"åŠ è½½GSM8Kæ•°æ®é›†å¤±è´¥: {e}")
        print("æ— æ³•è¿›è¡Œè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–é¡¹")
        return
        
    finally:
        # æ¸…ç†èµ„æº
        trainer.cleanup()


if __name__ == "__main__":
    main()



