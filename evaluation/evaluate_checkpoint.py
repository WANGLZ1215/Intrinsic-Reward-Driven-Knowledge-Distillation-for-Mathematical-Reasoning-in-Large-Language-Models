#!/usr/bin/env python3
"""
RLæ¨¡å‹æ£€æŸ¥ç‚¹è¯„ä¼°è„šæœ¬
åŠŸèƒ½ï¼šè¯„ä¼°æœ¬åœ°ä¿å­˜çš„RLè®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š

è®¾è®¡ç”¨äºVAST AIåœ¨çº¿è¯„ä¼°ç¯å¢ƒï¼š
- æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
- ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ç­”æ¡ˆæå–å‡½æ•° extract_answer_unified
- å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- æ£€æŸ¥ç‚¹éªŒè¯å’Œè·¯å¾„è§£æ
"""

import argparse
import yaml
import logging
import os
import json
from pathlib import Path
import sys
from datetime import datetime
from tqdm import tqdm
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
# æ”¯æŒä»evaluationç›®å½•æˆ–scriptsç›®å½•è¿è¡Œ
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.append(str(project_root))

from models.student_model import StudentModel
from models.teacher_model import TeacherModel
from evaluation.reasoning_evaluator import ReasoningEvaluator
from evaluation.metrics import ComprehensiveEvaluator
from data.gsm8k_processor import GSM8KProcessor
from datasets import load_dataset
from utils.math_utils import extract_answer_unified  # ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ç­”æ¡ˆæå–å‡½æ•°


def setup_logging(log_level: str = "INFO", log_file: str = None):
    """è®¾ç½®æ—¥å¿—"""
    # åˆ›å»ºlogsç›®å½•
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åç§°
    if log_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = logs_dir / f"evaluate_checkpoint_{timestamp}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # é…ç½®æ—¥å¿—ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def evaluate_checkpoint(
    checkpoint_path: str,
    teacher_model_path: str,
    config: dict,
    eval_samples: int = None,
    output_file: str = "evaluation_results.json",
    **kwargs
):
    """
    è¯„ä¼°æ£€æŸ¥ç‚¹æ¨¡å‹
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¦‚ checkpoints/rl_model/checkpoint-1000ï¼‰
        teacher_model_path: æ•™å¸ˆæ¨¡å‹è·¯å¾„
        config: é…ç½®å­—å…¸
        eval_samples: è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        output_file: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„
    """
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 80)
    logger.info("å¼€å§‹è¯„ä¼°RLæ¨¡å‹æ£€æŸ¥ç‚¹")
    logger.info(f"æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
    logger.info(f"æ•™å¸ˆæ¨¡å‹: {teacher_model_path}")
    logger.info("=" * 80)
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    results = {
        "checkpoint_path": checkpoint_path,
        "teacher_model": teacher_model_path,
        "evaluation_time": datetime.now().isoformat(),
        "accuracy": 0.0,
        "reasoning_quality": {
            "overall_score": 0.0,
            "step_coverage": 0.0,
            "logical_consistency": 0.0,
            "kl_divergence": 0.0,
            "answer_correctness": 0.0
        },
        "distillation_effect": {
            "overall_score": 0.0,
            "kl_divergence": 0.0,
            "cosine_similarity": 0.0,
            "js_divergence": 0.0
        },
        "statistics": {
            "total_samples": 0,
            "correct_samples": 0,
            "incorrect_samples": 0,
            "average_reasoning_score": 0.0,
            "average_distillation_score": 0.0
        },
        "individual_results": []
    }
    
    try:
        # æ­¥éª¤1: åŠ è½½å­¦ç”Ÿæ¨¡å‹ï¼ˆä»æ£€æŸ¥ç‚¹ï¼‰
        logger.info("æ­¥éª¤1/5: åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
        
        # ç¡®ä¿æ£€æŸ¥ç‚¹è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ï¼ˆVAST AIå…¼å®¹æ€§ï¼‰
        checkpoint_path_abs = Path(checkpoint_path).resolve()
        if not checkpoint_path_abs.exists():
            # å°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent
            checkpoint_path_abs = (project_root / checkpoint_path).resolve()
        
        if not checkpoint_path_abs.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path} (å°è¯•äº†: {checkpoint_path_abs})")
        
        logger.info(f"ä½¿ç”¨æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path_abs}")
        
        # éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶
        required_files = [
            checkpoint_path_abs / "adapter_config.json"
        ]
        weight_files = [
            checkpoint_path_abs / "adapter_model.safetensors",
            checkpoint_path_abs / "adapter_model.bin"
        ]
        
        if not required_files[0].exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {required_files[0]}")
        
        if not any(f.exists() for f in weight_files):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç¼ºå°‘æƒé‡æ–‡ä»¶: {[str(f) for f in weight_files]}")
        
        logger.info(f"æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {[f.name for f in required_files + weight_files if f.exists()]}")
        
        student_model = StudentModel(
            model_name=str(checkpoint_path_abs),
            lora_config=config["lora"],
            device=config["device"]["device_map"],
            torch_dtype=getattr(torch, config["device"]["torch_dtype"]),
            use_lora=True
        )
        logger.info("âœ… å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ­¥éª¤2: åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        teacher_model = None
        skip_teacher = kwargs.get("skip_teacher", False)
        if not skip_teacher:
            logger.info("æ­¥éª¤2/5: åŠ è½½æ•™å¸ˆæ¨¡å‹...")
            try:
                teacher_model = TeacherModel(
                    model_name=teacher_model_path,
                    cache_size=config["model"]["cache_size"],
                    cache_policy=config["model"]["cache_policy"],
                    device=config["device"]["device_map"],
                    torch_dtype=getattr(torch, config["device"]["torch_dtype"])
                )
                logger.info("âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.warning("   å°†ç»§ç»­è¯„ä¼°ä½†è·³è¿‡æ•™å¸ˆæ¨¡å‹ç›¸å…³æŒ‡æ ‡")
                teacher_model = None
        else:
            logger.info("æ­¥éª¤2/5: è·³è¿‡æ•™å¸ˆæ¨¡å‹åŠ è½½ï¼ˆ--skip_teacherï¼‰")
        
        # æ­¥éª¤3: åŠ è½½è¯„ä¼°æ•°æ®
        logger.info("æ­¥éª¤3/5: åŠ è½½è¯„ä¼°æ•°æ®...")
        try:
            dataset = load_dataset("gsm8k", "main")
            logger.info(f"æ•°æ®é›†åŠ è½½æˆåŠŸ: train={len(dataset['train'])}, test={len(dataset['test'])}")
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise
        
        if eval_samples is not None:
            eval_samples = min(eval_samples, len(dataset["test"]))
            eval_dataset = dataset["test"].select(range(eval_samples))
            logger.info(f"ä½¿ç”¨ {eval_samples} ä¸ªæµ‹è¯•æ ·æœ¬ï¼ˆæ€»å…± {len(dataset['test'])} ä¸ªï¼‰")
        else:
            eval_dataset = dataset["test"]
            logger.info(f"ä½¿ç”¨å…¨éƒ¨ {len(eval_dataset)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # æ­¥éª¤4: åˆ›å»ºè¯„ä¼°å™¨
        logger.info("æ­¥éª¤4/5: åˆå§‹åŒ–è¯„ä¼°å™¨...")
        reasoning_evaluator = ReasoningEvaluator()
        comprehensive_evaluator = ComprehensiveEvaluator()
        logger.info("âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æ­¥éª¤5: æ‰§è¡Œè¯„ä¼°
        logger.info("æ­¥éª¤5/5: å¼€å§‹è¯„ä¼°...")
        logger.info("=" * 80)
        
        total_samples = len(eval_dataset)
        correct_count = 0
        
        # ç´¯ç§¯å„é¡¹æŒ‡æ ‡
        reasoning_scores = []
        distillation_scores = []
        step_coverage_scores = []
        logical_consistency_scores = []
        answer_correctness_scores = []
        kl_divergences = []
        cosine_similarities = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for idx, sample in enumerate(tqdm(eval_dataset, desc="è¯„ä¼°è¿›åº¦", ncols=100)):
            question = sample["question"]
            ground_truth = sample["answer"]
            
            try:
                # æ ¼å¼åŒ–é—®é¢˜æç¤º
                formatted_question = f"Question: {question}\nAnswer: "
                
                # å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆ
                try:
                    student_response = student_model.generate(
                        formatted_question,
                        max_length=512,
                        temperature=0.7,
                        do_sample=True
                    )
                    # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(student_response, str):
                        student_response = str(student_response) if student_response else ""
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {idx+1} å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
                    student_response = ""
                
                # æ•™å¸ˆæ¨¡å‹ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
                teacher_response = ""
                if teacher_model is not None:
                    try:
                        teacher_response = teacher_model.generate_response(
                            formatted_question,
                            max_length=512,
                            temperature=0.7
                        )
                        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                        if not isinstance(teacher_response, str):
                            teacher_response = str(teacher_response) if teacher_response else ""
                    except Exception as e:
                        logger.warning(f"æ ·æœ¬ {idx+1} æ•™å¸ˆæ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
                        teacher_response = ""
                else:
                    teacher_response = ""  # è·³è¿‡æ•™å¸ˆæ¨¡å‹
                
                # è·å–logitsç”¨äºè’¸é¦è¯„ä¼°ï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–è¯„ä¼°ï¼‰
                student_logits = None
                teacher_logits = None
                if student_response and teacher_response:
                    try:
                        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¾“å…¥ä¸ä¸ºç©ºï¼Œé¿å…ç´¢å¼•è¶Šç•Œ
                        full_text = formatted_question + student_response
                        if len(full_text.strip()) > 0:
                            student_logits = student_model.get_logits(full_text)
                            # ğŸ” éªŒè¯logitsç»´åº¦ï¼ˆé˜²æ­¢ç©ºtensorå¯¼è‡´ç´¢å¼•è¶Šç•Œï¼‰
                            if student_logits is not None and student_logits.numel() == 0:
                                student_logits = None
                    except (IndexError, RuntimeError) as e:
                        logger.warning(f"æ ·æœ¬ {idx+1} è·å–å­¦ç”Ÿlogitså¤±è´¥ï¼ˆç´¢å¼•è¶Šç•Œï¼‰: {e}")
                        student_logits = None
                    except Exception as e:
                        logger.debug(f"æ ·æœ¬ {idx+1} è·å–å­¦ç”Ÿlogitså¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
                        student_logits = None
                    
                    try:
                        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¾“å…¥ä¸ä¸ºç©ºï¼Œé¿å…ç´¢å¼•è¶Šç•Œ
                        full_text = formatted_question + teacher_response
                        if len(full_text.strip()) > 0:
                            teacher_logits = teacher_model.get_logits(full_text)
                            # ğŸ” éªŒè¯logitsç»´åº¦ï¼ˆé˜²æ­¢ç©ºtensorå¯¼è‡´ç´¢å¼•è¶Šç•Œï¼‰
                            if teacher_logits is not None and teacher_logits.numel() == 0:
                                teacher_logits = None
                    except (IndexError, RuntimeError) as e:
                        logger.warning(f"æ ·æœ¬ {idx+1} è·å–æ•™å¸ˆlogitså¤±è´¥ï¼ˆç´¢å¼•è¶Šç•Œï¼‰: {e}")
                        teacher_logits = None
                    except Exception as e:
                        logger.debug(f"æ ·æœ¬ {idx+1} è·å–æ•™å¸ˆlogitså¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
                        teacher_logits = None
                
                # æå–ç­”æ¡ˆï¼ˆä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ç­”æ¡ˆæå–å‡½æ•° extract_answer_unifiedï¼‰
                try:
                    ground_truth_text, ground_truth_num = extract_answer_unified(ground_truth)
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {idx+1} æå–ground_truthç­”æ¡ˆå¤±è´¥: {e}")
                    ground_truth_text, ground_truth_num = "", None
                
                try:
                    student_answer_text, student_answer_num = extract_answer_unified(student_response)
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {idx+1} æå–studentç­”æ¡ˆå¤±è´¥: {e}")
                    student_answer_text, student_answer_num = "", None
                
                # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆä½¿ç”¨æ•°å€¼æ¯”è¾ƒï¼Œæ›´å‡†ç¡®ï¼‰
                is_correct = False
                if ground_truth_num is not None and student_answer_num is not None:
                    # ä½¿ç”¨æ•°å€¼æ¯”è¾ƒï¼ˆå®¹å¿å°è¯¯å·®ï¼Œä¸utils/math_utils.pyä¿æŒä¸€è‡´ï¼‰
                    tolerance = 1e-6
                    if abs(ground_truth_num) < 1e-10:
                        # çœŸå€¼æ¥è¿‘0ï¼Œä½¿ç”¨ç»å¯¹è¯¯å·®
                        is_correct = abs(student_answer_num - ground_truth_num) < tolerance
                    else:
                        # ä½¿ç”¨ç›¸å¯¹è¯¯å·®
                        relative_error = abs(student_answer_num - ground_truth_num) / abs(ground_truth_num)
                        is_correct = relative_error < tolerance
                elif ground_truth_text and student_answer_text:
                    # å¦‚æœæ— æ³•æå–æ•°å­—ï¼Œä½¿ç”¨æ–‡æœ¬æ¯”è¾ƒ
                    is_correct = ground_truth_text.strip().lower() == student_answer_text.strip().lower()
                else:
                    # å¦‚æœéƒ½æ— æ³•æå–ï¼Œæ ‡è®°ä¸ºé”™è¯¯
                    is_correct = False
                    if idx < 5:  # åªå¯¹å‰å‡ ä¸ªæ ·æœ¬è¯¦ç»†æ—¥å¿—
                        logger.debug(f"æ ·æœ¬ {idx+1} æ— æ³•æå–ç­”æ¡ˆ: ground_truth_text={ground_truth_text}, student_answer_text={student_answer_text}")
                
                if is_correct:
                    correct_count += 1
                
                # è¯„ä¼°æ¨ç†è´¨é‡ï¼ˆå¦‚æœå“åº”ä¸ä¸ºç©ºï¼‰
                try:
                    if student_response and teacher_response:
                        reasoning_result = reasoning_evaluator.evaluate_reasoning_quality(
                            student_response=student_response,
                            teacher_response=teacher_response,
                            ground_truth_answer=ground_truth_num,
                            student_logits=student_logits,
                            teacher_logits=teacher_logits
                        )
                    else:
                        # å¦‚æœå“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
                        reasoning_result = {
                            "overall_score": 0.0,
                            "step_coverage": {"step_coverage": 0.0},
                            "logical_consistency": {"overall_consistency": 0.0}
                        }
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {idx+1} æ¨ç†è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
                    reasoning_result = {
                        "overall_score": 0.0,
                        "step_coverage": {"step_coverage": 0.0},
                        "logical_consistency": {"overall_consistency": 0.0}
                    }
                
                # è¯„ä¼°è’¸é¦æ•ˆæœï¼ˆå¦‚æœå“åº”ä¸ä¸ºç©ºä¸”logitså¯ç”¨ï¼‰
                try:
                    if student_response and ground_truth:
                        distillation_result = comprehensive_evaluator.evaluate_comprehensive(
                            predictions=[student_response],
                            ground_truths=[ground_truth],
                            student_logits=student_logits,
                            teacher_logits=teacher_logits
                        )
                    else:
                        distillation_result = {"overall_score": 0.0}
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {idx+1} è’¸é¦æ•ˆæœè¯„ä¼°å¤±è´¥: {e}")
                    distillation_result = {"overall_score": 0.0}
                
                # ç´¯ç§¯æŒ‡æ ‡
                reasoning_score = reasoning_result["overall_score"]
                reasoning_scores.append(reasoning_score)
                
                distillation_score = distillation_result.get("overall_score", 0.0)
                distillation_scores.append(distillation_score)
                
                step_coverage_scores.append(reasoning_result.get("step_coverage", {}).get("step_coverage", 0.0))
                logical_consistency_scores.append(
                    reasoning_result.get("logical_consistency", {}).get("overall_consistency", 0.0)
                )
                
                if "answer_correctness" in reasoning_result:
                    answer_correctness_scores.append(
                        reasoning_result["answer_correctness"].get("correctness_score", 0.0)
                    )
                
                if student_logits is not None and teacher_logits is not None:
                    kl_divergences.append(reasoning_result.get("kl_divergence", 0.0))
                    cosine_similarities.append(distillation_result.get("cosine_similarity", 0.0))
                
                # ä¿å­˜ä¸ªä½“ç»“æœï¼ˆåªä¿å­˜å…³é”®ä¿¡æ¯ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
                individual_result = {
                    "index": idx + 1,
                    "question": question[:200] + "..." if len(question) > 200 else question,
                    "ground_truth": ground_truth[:200] + "..." if len(ground_truth) > 200 else ground_truth,
                    "student_response": student_response[:200] + "..." if len(student_response) > 200 else student_response,
                    "teacher_response": teacher_response[:200] + "..." if len(teacher_response) > 200 else teacher_response,
                    "is_correct": is_correct,
                    "student_answer_text": student_answer_text if student_answer_text else "N/A",
                    "student_answer_num": student_answer_num if student_answer_num is not None else "N/A",
                    "ground_truth_text": ground_truth_text if ground_truth_text else "N/A",
                    "ground_truth_answer_num": ground_truth_num if ground_truth_num is not None else "N/A",
                    "reasoning_score": float(reasoning_score) if isinstance(reasoning_score, (int, float)) else 0.0,
                    "distillation_score": float(distillation_score) if isinstance(distillation_score, (int, float)) else 0.0
                }
                results["individual_results"].append(individual_result)
                
            except Exception as e:
                logger.error(f"è¯„ä¼°æ ·æœ¬ {idx+1} æ—¶å‡ºé”™: {e}")
                # è®°å½•é”™è¯¯ä½†ç»§ç»­è¯„ä¼°
                results["individual_results"].append({
                    "index": idx + 1,
                    "error": str(e),
                    "question": question[:200] if len(question) > 200 else question
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        logger.info("=" * 80)
        logger.info("è®¡ç®—æ€»ä½“æŒ‡æ ‡...")
        
        results["accuracy"] = correct_count / total_samples if total_samples > 0 else 0.0
        
        # æ¨ç†è´¨é‡ç»Ÿè®¡
        if reasoning_scores:
            results["reasoning_quality"]["overall_score"] = sum(reasoning_scores) / len(reasoning_scores)
            results["reasoning_quality"]["step_coverage"] = sum(step_coverage_scores) / len(step_coverage_scores) if step_coverage_scores else 0.0
            results["reasoning_quality"]["logical_consistency"] = sum(logical_consistency_scores) / len(logical_consistency_scores) if logical_consistency_scores else 0.0
            results["reasoning_quality"]["answer_correctness"] = sum(answer_correctness_scores) / len(answer_correctness_scores) if answer_correctness_scores else 0.0
            results["reasoning_quality"]["kl_divergence"] = sum(kl_divergences) / len(kl_divergences) if kl_divergences else 0.0
        
        # è’¸é¦æ•ˆæœç»Ÿè®¡
        if distillation_scores:
            results["distillation_effect"]["overall_score"] = sum(distillation_scores) / len(distillation_scores)
            results["distillation_effect"]["kl_divergence"] = sum(kl_divergences) / len(kl_divergences) if kl_divergences else 0.0
            results["distillation_effect"]["cosine_similarity"] = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0.0
        
        # æ€»ä½“ç»Ÿè®¡
        results["statistics"]["total_samples"] = total_samples
        results["statistics"]["correct_samples"] = correct_count
        results["statistics"]["incorrect_samples"] = total_samples - correct_count
        results["statistics"]["average_reasoning_score"] = results["reasoning_quality"]["overall_score"]
        results["statistics"]["average_distillation_score"] = results["distillation_effect"]["overall_score"]
        
        # æ‰“å°ç»“æœæ‘˜è¦
        logger.info("=" * 80)
        logger.info("è¯„ä¼°ç»“æœæ‘˜è¦")
        logger.info("=" * 80)
        logger.info(f"å‡†ç¡®ç‡: {results['accuracy']:.4f} ({correct_count}/{total_samples})")
        logger.info(f"æ¨ç†è´¨é‡æ€»åˆ†: {results['reasoning_quality']['overall_score']:.4f}")
        logger.info(f"  - æ­¥éª¤è¦†ç›–ç‡: {results['reasoning_quality']['step_coverage']:.4f}")
        logger.info(f"  - é€»è¾‘ä¸€è‡´æ€§: {results['reasoning_quality']['logical_consistency']:.4f}")
        logger.info(f"  - ç­”æ¡ˆæ­£ç¡®æ€§: {results['reasoning_quality']['answer_correctness']:.4f}")
        logger.info(f"  - KLæ•£åº¦: {results['reasoning_quality']['kl_divergence']:.4f}")
        logger.info(f"è’¸é¦æ•ˆæœæ€»åˆ†: {results['distillation_effect']['overall_score']:.4f}")
        logger.info(f"  - ä½™å¼¦ç›¸ä¼¼åº¦: {results['distillation_effect']['cosine_similarity']:.4f}")
        logger.info("=" * 80)
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_file)
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path.resolve()}")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # éªŒè¯æ–‡ä»¶å·²ä¿å­˜
        if output_path.exists():
            file_size = output_path.stat().st_size / 1024 / 1024  # MB
            logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœæ–‡ä»¶: {output_path.resolve()} ({file_size:.2f} MB)")
        else:
            logger.error(f"âŒ ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥: {output_path}")
        
        return results
        
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°RLæ¨¡å‹æ£€æŸ¥ç‚¹")
    parser.add_argument("--checkpoint_path", type=str, required=True,
                       help="æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¦‚ checkpoints/rl_model/checkpoint-1000ï¼‰")
    parser.add_argument("--config", type=str, default="config/training_config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--teacher_model_path", type=str, default="Qwen/Qwen2.5-32B-Instruct",
                       help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--eval_samples", type=int, default=None,
                       help="è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰")
    parser.add_argument("--output_file", type=str, default="evaluation_results.json",
                       help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log_level", type=str, default="INFO",
                       help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--log_file", type=str, default=None,
                       help="æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--skip_teacher", action="store_true",
                       help="è·³è¿‡æ•™å¸ˆæ¨¡å‹ç”Ÿæˆï¼ˆé¿å…CUDAé”™è¯¯ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.log_level, args.log_file)
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼ŒVAST AIå…¼å®¹ï¼‰
    checkpoint_path_input = Path(args.checkpoint_path)
    project_root = Path(__file__).parent.parent
    current_dir = Path.cwd()
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
    possible_paths = [
        checkpoint_path_input,  # åŸå§‹è·¯å¾„
        checkpoint_path_input.resolve(),  # ç»å¯¹è·¯å¾„è§£æ
        project_root / args.checkpoint_path,  # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        current_dir / args.checkpoint_path,  # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
    ]
    
    checkpoint_path = None
    for path in possible_paths:
        if path.exists() and path.is_dir():
            checkpoint_path = path.resolve()
            logger.info(f"æ‰¾åˆ°æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
            break
    
    if checkpoint_path is None:
        logger.error(f"âŒ æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {args.checkpoint_path}")
        logger.error(f"å°è¯•çš„è·¯å¾„:")
        for path in possible_paths:
            logger.error(f"  - {path} (å­˜åœ¨: {path.exists()}, æ˜¯ç›®å½•: {path.is_dir() if path.exists() else 'N/A'})")
        logger.error(f"å½“å‰å·¥ä½œç›®å½•: {current_dir}")
        logger.error(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        logger.error(f"è„šæœ¬ä½ç½®: {Path(__file__).parent}")
        sys.exit(1)
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•ä¸­æ˜¯å¦æœ‰adapteræ–‡ä»¶
    adapter_config = checkpoint_path / "adapter_config.json"
    adapter_weights = [
        checkpoint_path / "adapter_model.safetensors",
        checkpoint_path / "adapter_model.bin"
    ]
    
    if not adapter_config.exists():
        logger.error(f"âŒ æ£€æŸ¥ç‚¹ç¼ºå°‘é…ç½®æ–‡ä»¶: {adapter_config}")
        sys.exit(1)
    
    if not any(f.exists() for f in adapter_weights):
        logger.error(f"âŒ æ£€æŸ¥ç‚¹ç¼ºå°‘æƒé‡æ–‡ä»¶")
        logger.error(f"   æŸ¥æ‰¾è·¯å¾„: {[str(f) for f in adapter_weights]}")
        logger.error(f"   æ£€æŸ¥ç‚¹ç›®å½•å†…å®¹: {list(checkpoint_path.iterdir())[:10]}")
        sys.exit(1)
    
    logger.info(f"âœ… æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {checkpoint_path}")
    logger.info(f"   é…ç½®æ–‡ä»¶: âœ“ {adapter_config.name}")
    logger.info(f"   æƒé‡æ–‡ä»¶: âœ“ {[f.name for f in adapter_weights if f.exists()]}")
    
    try:
        # åŠ è½½é…ç½®ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
        config_path = Path(args.config)
        if not config_path.exists():
            project_root = Path(__file__).parent.parent
            config_path = project_root / args.config
        if not config_path.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            sys.exit(1)
        
        config = load_config(str(config_path))
        logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        
        # æ‰§è¡Œè¯„ä¼°
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿VAST AIå…¼å®¹æ€§
        results = evaluate_checkpoint(
            checkpoint_path=str(checkpoint_path.resolve()),
            teacher_model_path=args.teacher_model_path,
            config=config,
            eval_samples=args.eval_samples,
            output_file=args.output_file,
            skip_teacher=args.skip_teacher
        )
        
        logger.info("=" * 80)
        logger.info("è¯„ä¼°ä»»åŠ¡å®Œæˆï¼")
        logger.info(f"ç»“æœæ–‡ä»¶: {args.output_file}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

