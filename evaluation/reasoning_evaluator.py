"""
Reasoning Quality Evaluation Module
Function: Evaluate mathematical reasoning quality, including step coverage, logical consistency, etc.
"""

import re
import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import logging
import numpy as np
from collections import Counter
import sympy as sp
from utils.math_utils import extract_answer_unified  # å¯¼å…¥ç»Ÿä¸€çš„ç­”æ¡ˆæå–å‡½æ•°


class ReasoningEvaluator:
    """Reasoning Quality Evaluator"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        # æ¨ç†æ­¥éª¤æ¨¡å¼
        self.step_patterns = [
            r'Step \d+:',      # "Step 1:", "Step 2:", etc.
            r'\d+\.',          # "1.", "2.", etc.
            r'First,',         # "First,", "Second,", etc.
            r'Then,',          # "Then,", "Next,", etc.
            r'Therefore,',     # "Therefore,", "Thus,", etc.
            r'So,',            # "So,", "Hence,", etc.
            r'Finally,',       # "Finally,", "In conclusion,", etc.
        ]
        
        # æ•°å­¦æ“ä½œç¬¦
        self.math_operators = ['+', '-', '*', '/', '=', '^', '**']
        
        # é€»è¾‘è¿æ¥è¯
        self.logical_connectors = ['therefore', 'thus', 'so', 'hence', 'because', 'since']
        
        logging.info("Reasoning quality evaluator initialized")
    
    def extract_reasoning_steps(self, response: str) -> List[str]:
        """
        æå–æ¨ç†æ­¥éª¤
        
        Args:
            response: æ¨¡å‹å“åº”æ–‡æœ¬
            
        Returns:
            æ¨ç†æ­¥éª¤åˆ—è¡¨
        """
        steps = []
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ­¥éª¤æ¨¡å¼
            if any(re.search(pattern, line, re.IGNORECASE) for pattern in self.step_patterns):
                steps.append(line)
            elif line and self._is_math_expression(line):
                # å¦‚æœåŒ…å«æ•°å­¦è¡¨è¾¾å¼ï¼Œä¹Ÿè®¤ä¸ºæ˜¯æ¨ç†æ­¥éª¤
                steps.append(line)
        
        return steps
    
    def _is_math_expression(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«æ•°å­¦è¡¨è¾¾å¼
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ˜¯å¦åŒ…å«æ•°å­¦è¡¨è¾¾å¼
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦æ“ä½œç¬¦
        has_operator = any(op in text for op in self.math_operators)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—
        has_numbers = bool(re.search(r'\d+', text))
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­¦å…³é”®è¯
        math_keywords = ['calculate', 'compute', 'solve', 'find', 'result', 'answer']
        has_math_keyword = any(keyword in text.lower() for keyword in math_keywords)
        
        return has_operator and (has_numbers or has_math_keyword)
    
    def evaluate_step_coverage(self, student_steps: List[str], 
                             teacher_steps: List[str]) -> Dict[str, float]:
        """
        è¯„ä¼°æ­¥éª¤è¦†ç›–ç‡
        
        Args:
            student_steps: å­¦ç”Ÿæ¨ç†æ­¥éª¤
            teacher_steps: æ•™å¸ˆæ¨ç†æ­¥éª¤
            
        Returns:
            è¦†ç›–ç‡è¯„ä¼°ç»“æœ
        """
        if not teacher_steps:
            return {
                "step_coverage": 0.0,
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0
            }
        
        # æå–å…³é”®æ•°å­¦è¡¨è¾¾å¼
        student_expressions = self._extract_math_expressions(student_steps)
        teacher_expressions = self._extract_math_expressions(teacher_steps)
        
        # è®¡ç®—äº¤é›†
        common_expressions = set(student_expressions) & set(teacher_expressions)
        
        # è®¡ç®—æŒ‡æ ‡
        precision = len(common_expressions) / max(1, len(student_expressions))
        recall = len(common_expressions) / len(teacher_expressions)
        f1_score = 2 * precision * recall / max(1, precision + recall)
        
        return {
            "step_coverage": recall,  # ä½¿ç”¨recallä½œä¸ºè¦†ç›–ç‡
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "student_steps": len(student_steps),
            "teacher_steps": len(teacher_steps),
            "common_steps": len(common_expressions)
        }
    
    def _extract_math_expressions(self, steps: List[str]) -> List[str]:
        """æå–æ•°å­¦è¡¨è¾¾å¼"""
        expressions = []
        
        for step in steps:
            # æŸ¥æ‰¾æ•°å­¦è¡¨è¾¾å¼æ¨¡å¼
            patterns = [
                r'\d+\s*[+\-*/=]\s*\d+',  # åŸºæœ¬è¿ç®—
                r'\d+\s*=\s*\d+',          # ç­‰å¼
                r'[a-zA-Z]\s*=\s*\d+',     # å˜é‡èµ‹å€¼
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, step)
                expressions.extend(matches)
        
        return expressions
    
    def evaluate_logical_consistency(self, response: str) -> Dict[str, float]:
        """
        è¯„ä¼°é€»è¾‘ä¸€è‡´æ€§
        
        Args:
            response: æ¨¡å‹å“åº”
            
        Returns:
            é€»è¾‘ä¸€è‡´æ€§è¯„ä¼°ç»“æœ
        """
        # æå–æ•°å­—åºåˆ—
        numbers = self._extract_numbers(response)
        
        # æ£€æŸ¥æ•°å­—çš„åˆç†æ€§
        number_consistency = self._check_number_consistency(numbers)
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯çš„ä½¿ç”¨
        logical_flow = self._check_logical_flow(response)
        
        # æ£€æŸ¥æ•°å­¦è¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§
        expression_validity = self._check_expression_validity(response)
        
        # ç»¼åˆè¯„åˆ†
        overall_consistency = (number_consistency + logical_flow + expression_validity) / 3
        
        return {
            "overall_consistency": overall_consistency,
            "number_consistency": number_consistency,
            "logical_flow": logical_flow,
            "expression_validity": expression_validity,
            "extracted_numbers": numbers
        }
    
    def _extract_numbers(self, text: str) -> List[float]:
        """æå–æ–‡æœ¬ä¸­çš„æ•°å­—"""
        numbers = re.findall(r'-?\d+\.?\d*', text)
        return [float(num) for num in numbers if num]
    
    def _check_number_consistency(self, numbers: List[float]) -> float:
        """
        æ£€æŸ¥æ•°å­—çš„ä¸€è‡´æ€§ï¼ˆåŸºäºæ•°å­—åºåˆ—çš„å¹³æ»‘åº¦ï¼‰
        
        ç§»é™¤äº†å¯å‘å¼çš„è´Ÿæ•°/å¤§æ•°è¿‡æ»¤ï¼Œå› ä¸ºï¼š
        - é¢˜ç›®æœ¬èº«å¯èƒ½æ¶‰åŠè´Ÿæ•°ï¼ˆäºæŸã€æ¸©åº¦ã€å€ºåŠ¡ç­‰ï¼‰
        - é¢˜ç›®å¯èƒ½æ¶‰åŠå¤§æ•°ï¼ˆäººå£ã€è·ç¦»ã€é‡‘é¢ç­‰ï¼‰
        """
        if not numbers:
            return 0.0
        
        if len(numbers) < 2:
            return 1.0
        
        # è®¡ç®—æ•°å­—åºåˆ—çš„å˜åŒ–å¹³æ»‘åº¦
        # æ£€æŸ¥ç›¸é‚»æ•°å­—ä¹‹é—´çš„å˜åŒ–æ˜¯å¦åˆç†ï¼ˆä¸è¦æœ‰çªç„¶çš„å·¨å¤§è·³è·ƒï¼‰
        score = 1.0
        changes = []
        
        for i in range(len(numbers) - 1):
            if numbers[i] != 0:
                # è®¡ç®—ç›¸å¯¹å˜åŒ–
                relative_change = abs((numbers[i+1] - numbers[i]) / numbers[i])
                changes.append(relative_change)
        
        if changes:
            # å¦‚æœæœ‰è¿‡å¤šçš„æç«¯å˜åŒ–ï¼ˆè¶…è¿‡1000å€ï¼‰ï¼Œå¯èƒ½æœ‰é—®é¢˜
            extreme_changes = sum(1 for change in changes if change > 1000)
            if extreme_changes > len(changes) * 0.5:
                score -= 0.3
        
        return max(0.0, score)
    
    def _check_logical_flow(self, text: str) -> float:
        """æ£€æŸ¥é€»è¾‘æµç¨‹"""
        text_lower = text.lower()
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯çš„ä½¿ç”¨
        connector_count = sum(1 for connector in self.logical_connectors 
                             if connector in text_lower)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æ¨ç†ç»“æ„
        structure_score = 0.0
        
        if 'step' in text_lower or 'first' in text_lower:
            structure_score += 0.3
        if 'then' in text_lower or 'next' in text_lower:
            structure_score += 0.3
        if 'therefore' in text_lower or 'so' in text_lower or 'thus' in text_lower:
            structure_score += 0.4
        
        # å½’ä¸€åŒ–åˆ†æ•°
        normalized_connector_score = min(1.0, connector_count / 3.0)
        
        return min(1.0, structure_score + normalized_connector_score * 0.3)
    
    def _check_expression_validity(self, text: str) -> float:
        """
        æ£€æŸ¥æ•°å­¦è¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§
        
        æ”¹è¿›ï¼šä½¿ç”¨æ›´å®½æ¾çš„æ ‡å‡†ï¼Œä¸ä¾èµ–evalï¼ˆevalå¯¹å¤æ‚æ¨ç†æ–‡æœ¬æ— æ•ˆï¼‰
        åªæ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„æ•°å­¦ç»“æ„
        """
        # æå–æ•°å­¦è¡¨è¾¾å¼
        expressions = self._extract_math_expressions([text])
        
        if not expressions:
            # æ²¡æœ‰æ˜ç¡®çš„æ•°å­¦è¡¨è¾¾å¼ä¸ä»£è¡¨æ¨ç†æ— æ•ˆ
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰æ•°å­—å’Œæ¨ç†è¯æ±‡
            has_numbers = bool(re.search(r'\d+', text))
            has_reasoning = any(word in text.lower() for word in 
                              ['calculate', 'compute', 'total', 'sum', 'multiply', 'divide', 'add', 'subtract'])
            if has_numbers and has_reasoning:
                return 0.7  # æœ‰æ•°å­—å’Œæ¨ç†è¯æ±‡ï¼Œç»™è¾ƒé«˜åˆ†
            elif has_numbers:
                return 0.5  # åªæœ‰æ•°å­—ï¼Œç»™ä¸­ç­‰åˆ†
            else:
                return 0.3  # ç¼ºä¹æ•°å­¦å†…å®¹
        
        # æ£€æŸ¥è¡¨è¾¾å¼çš„ç»“æ„åˆç†æ€§
        valid_count = 0
        for expr in expressions:
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„æ•°å­¦ç»“æ„ï¼ˆæ•°å­— + è¿ç®—ç¬¦ + æ•°å­—ï¼‰
            if re.search(r'\d+\s*[+\-*/]\s*\d+', expr):
                valid_count += 1
            # æ£€æŸ¥ç­‰å¼ç»“æ„ï¼ˆå·¦è¾¹ = å³è¾¹ï¼Œéƒ½æœ‰æ•°å­—ï¼‰
            elif '=' in expr:
                parts = expr.split('=')
                if len(parts) == 2 and all(re.search(r'\d+', part) for part in parts):
                    valid_count += 1
        
        if len(expressions) == 0:
            return 0.5
        
        return max(0.3, valid_count / len(expressions))  # è‡³å°‘ç»™0.3åˆ†
    
    def extract_final_answer(self, text: str) -> Optional[float]:
        """
        æå–æ–‡æœ¬ä¸­çš„æœ€ç»ˆç­”æ¡ˆ
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç°åœ¨è°ƒç”¨ utils.math_utils.extract_answer_unified ç»Ÿä¸€å®ç°
        æ”¯æŒå¤šç§æ ¼å¼ï¼š####, \\boxed{}, "answer:", "The answer is"
        
        Args:
            text: å“åº”æ–‡æœ¬
            
        Returns:
            æå–çš„æ•°å­—ç­”æ¡ˆï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        _, answer_num = extract_answer_unified(text)
        return answer_num
    
    def evaluate_answer_correctness(self, student_response: str, 
                                   ground_truth_answer: float,
                                   tolerance: float = 1e-4) -> Dict[str, Union[float, bool]]:
        """
        è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§
        
        Args:
            student_response: å­¦ç”Ÿå“åº”
            ground_truth_answer: æ­£ç¡®ç­”æ¡ˆï¼ˆæ•°å€¼ï¼‰
            tolerance: å®¹å·®èŒƒå›´
            
        Returns:
            ç­”æ¡ˆæ­£ç¡®æ€§è¯„ä¼°ç»“æœ
        """
        student_answer = self.extract_final_answer(student_response)
        
        if student_answer is None:
            # æ— æ³•æå–ç­”æ¡ˆï¼Œè§†ä¸ºé”™è¯¯
            return {
                "is_correct": False,
                "correctness_score": 0.0,
                "student_answer": None,
                "ground_truth": ground_truth_answer,
                "error": "Unable to extract answer"
            }
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        if abs(ground_truth_answer) < 1e-10:
            # çœŸå€¼æ¥è¿‘0ï¼Œä½¿ç”¨ç»å¯¹è¯¯å·®
            is_correct = abs(student_answer - ground_truth_answer) < tolerance
            relative_error = abs(student_answer - ground_truth_answer)
        else:
            # ä½¿ç”¨ç›¸å¯¹è¯¯å·®
            relative_error = abs(student_answer - ground_truth_answer) / abs(ground_truth_answer)
            is_correct = relative_error < tolerance
        
        # è®¡ç®—è¿ç»­çš„æ­£ç¡®æ€§åˆ†æ•°ï¼ˆå³ä½¿ä¸å®Œå…¨æ­£ç¡®ï¼Œä¹Ÿç»™äºˆéƒ¨åˆ†åˆ†æ•°ï¼‰
        if is_correct:
            correctness_score = 1.0
        else:
            # åŸºäºè¯¯å·®ç»™äºˆéƒ¨åˆ†åˆ†æ•°
            if relative_error < 0.01:  # 1%ä»¥å†…çš„è¯¯å·®
                correctness_score = 0.9
            elif relative_error < 0.05:  # 5%ä»¥å†…çš„è¯¯å·®
                correctness_score = 0.7
            elif relative_error < 0.1:  # 10%ä»¥å†…çš„è¯¯å·®
                correctness_score = 0.5
            elif relative_error < 0.5:  # 50%ä»¥å†…çš„è¯¯å·®
                correctness_score = 0.2
            else:
                correctness_score = 0.0
        
        return {
            "is_correct": is_correct,
            "correctness_score": correctness_score,
            "student_answer": student_answer,
            "ground_truth": ground_truth_answer,
            "relative_error": relative_error
        }
    
    def compute_kl_divergence(self, student_logits: torch.Tensor, 
                            teacher_logits: torch.Tensor) -> float:
        """
        è®¡ç®—KLæ•£åº¦
        
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹logits
            teacher_logits: æ•™å¸ˆæ¨¡å‹logits
            
        Returns:
            KLæ•£åº¦
        """
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿logitsä¸ä¸ºç©ºä¸”ç»´åº¦æ­£ç¡®
        if student_logits is None or teacher_logits is None:
            return 0.0
        
        # ç¡®ä¿æ˜¯tensorç±»å‹
        if not isinstance(student_logits, torch.Tensor) or not isinstance(teacher_logits, torch.Tensor):
            return 0.0
        
        # æ£€æŸ¥tensoræ˜¯å¦ä¸ºç©º
        if student_logits.numel() == 0 or teacher_logits.numel() == 0:
            return 0.0
        
        # ğŸ” ä¿®å¤ç´¢å¼•è¶Šç•Œï¼šç¡®ä¿ç»´åº¦æ­£ç¡®
        try:
            # æ£€æŸ¥ç»´åº¦ï¼ˆåº”è¯¥æ˜¯ [batch, seq_len, vocab_size]ï¼‰
            if len(student_logits.shape) < 2 or len(teacher_logits.shape) < 2:
                return 0.0
            
            # å¦‚æœåªæœ‰2ç»´ï¼Œæ·»åŠ batchç»´åº¦
            if len(student_logits.shape) == 2:
                student_logits = student_logits.unsqueeze(0)
            if len(teacher_logits.shape) == 2:
                teacher_logits = teacher_logits.unsqueeze(0)
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…ï¼ˆå–æœ€å°åºåˆ—é•¿åº¦ï¼Œé¿å…ç´¢å¼•è¶Šç•Œï¼‰
            student_seq_len = student_logits.shape[1]
            teacher_seq_len = teacher_logits.shape[1]
            
            if student_seq_len == 0 or teacher_seq_len == 0:
                return 0.0
            
            min_len = min(student_seq_len, teacher_seq_len)
            
            # ğŸ” å®‰å…¨åˆ‡ç‰‡ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if min_len > 0:
                student_logits = student_logits[:, :min_len, :]
                teacher_logits = teacher_logits[:, :min_len, :]
            else:
                return 0.0
            
            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
            student_probs = F.softmax(student_logits, dim=-1)
            teacher_probs = F.softmax(teacher_logits, dim=-1)
            
            # è®¡ç®—KLæ•£åº¦
            kl_div = F.kl_div(
                F.log_softmax(student_logits, dim=-1),
                teacher_probs,
                reduction='batchmean'
            )
            
            return kl_div.item() if not torch.isnan(kl_div) else 0.0
            
        except (IndexError, RuntimeError, ValueError) as e:
            # æ•è·ç´¢å¼•è¶Šç•Œæˆ–å…¶ä»–è¿è¡Œæ—¶é”™è¯¯
            logging.warning(f"è®¡ç®—KLæ•£åº¦æ—¶å‡ºé”™ï¼ˆå¯èƒ½ç”±äºtensorç»´åº¦ä¸åŒ¹é…ï¼‰: {e}")
            return 0.0
    
    def evaluate_reasoning_quality(self, student_response: str, 
                                 teacher_response: str,
                                 ground_truth_answer: Optional[float] = None,
                                 student_logits: Optional[torch.Tensor] = None,
                                 teacher_logits: Optional[torch.Tensor] = None) -> Dict[str, Union[float, Dict]]:
        """
        ç»¼åˆè¯„ä¼°æ¨ç†è´¨é‡
        
        æ”¹è¿›ï¼šå¢åŠ äº†æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºç‹¬ç«‹ç»´åº¦ï¼Œå¹¶ç»™äºˆæ›´é«˜æƒé‡
        
        æƒé‡åˆ†é…ï¼š
        - ç­”æ¡ˆæ­£ç¡®æ€§ï¼š50% ï¼ˆæœ€é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨GSM8Kç­‰æ•°å­¦ä»»åŠ¡ä¸­ï¼‰
        - æ­¥éª¤è¦†ç›–ç‡ï¼š20% ï¼ˆæ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ï¼‰
        - é€»è¾‘ä¸€è‡´æ€§ï¼š20% ï¼ˆæ¨ç†è¿‡ç¨‹çš„åˆç†æ€§ï¼‰
        - KLæ•£åº¦ï¼š10% ï¼ˆä¸æ•™å¸ˆæ¨¡å‹çš„ä¸€è‡´æ€§ï¼‰
        
        Args:
            student_response: å­¦ç”Ÿå“åº”
            teacher_response: æ•™å¸ˆå“åº”
            ground_truth_answer: æ­£ç¡®ç­”æ¡ˆï¼ˆå¯é€‰ï¼Œå¼ºçƒˆæ¨èæä¾›ï¼‰
            student_logits: å­¦ç”Ÿæ¨¡å‹logitsï¼ˆå¯é€‰ï¼‰
            teacher_logits: æ•™å¸ˆæ¨¡å‹logitsï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¨ç†è´¨é‡è¯„ä¼°ç»“æœ
        """
        # æå–æ¨ç†æ­¥éª¤
        student_steps = self.extract_reasoning_steps(student_response)
        teacher_steps = self.extract_reasoning_steps(teacher_response)
        
        # è¯„ä¼°æ­¥éª¤è¦†ç›–ç‡
        step_coverage_results = self.evaluate_step_coverage(student_steps, teacher_steps)
        
        # è¯„ä¼°é€»è¾‘ä¸€è‡´æ€§
        logical_consistency_results = self.evaluate_logical_consistency(student_response)
        
        # è¯„ä¼°æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§
        answer_correctness_results = None
        if ground_truth_answer is not None:
            answer_correctness_results = self.evaluate_answer_correctness(
                student_response, ground_truth_answer
            )
        
        # è®¡ç®—KLæ•£åº¦ï¼ˆå¦‚æœæœ‰logitsï¼‰
        kl_divergence = 0.0
        if student_logits is not None and teacher_logits is not None:
            kl_divergence = self.compute_kl_divergence(student_logits, teacher_logits)
        
        # ç»¼åˆè¯„åˆ†
        if answer_correctness_results is not None:
            # æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œä½¿ç”¨æ–°çš„æƒé‡åˆ†é…
            overall_score = (
                answer_correctness_results["correctness_score"] * 0.5 +  # ç­”æ¡ˆæ­£ç¡®æ€§ 50%
                step_coverage_results["step_coverage"] * 0.2 +           # æ­¥éª¤è¦†ç›–ç‡ 20%
                logical_consistency_results["overall_consistency"] * 0.2 + # é€»è¾‘ä¸€è‡´æ€§ 20%
                (1.0 / (1.0 + kl_divergence)) * 0.1                       # KLæ•£åº¦ 10%
            )
        else:
            # æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œä½¿ç”¨æ—§çš„æƒé‡ï¼ˆä½†è°ƒæ•´ä¸ºæ›´åˆç†çš„åˆ†é…ï¼‰
            overall_score = (
                step_coverage_results["step_coverage"] * 0.35 +
                logical_consistency_results["overall_consistency"] * 0.35 +
                (1.0 / (1.0 + kl_divergence)) * 0.3
            )
        
        result = {
            "overall_score": overall_score,
            "step_coverage": step_coverage_results,
            "logical_consistency": logical_consistency_results,
            "kl_divergence": kl_divergence,
            "student_steps_count": len(student_steps),
            "teacher_steps_count": len(teacher_steps)
        }
        
        # å¦‚æœæœ‰ç­”æ¡ˆæ­£ç¡®æ€§ç»“æœï¼Œæ·»åŠ åˆ°è¿”å›å€¼ä¸­
        if answer_correctness_results is not None:
            result["answer_correctness"] = answer_correctness_results
        
        return result


class BatchReasoningEvaluator:
    """Batch Reasoning Evaluator"""
    
    def __init__(self, evaluator: ReasoningEvaluator):
        """
        åˆå§‹åŒ–æ‰¹é‡è¯„ä¼°å™¨
        
        Args:
            evaluator: æ¨ç†è¯„ä¼°å™¨
        """
        self.evaluator = evaluator
    
    def evaluate_batch(self, student_responses: List[str],
                      teacher_responses: List[str],
                      ground_truth_answers: Optional[List[float]] = None,
                      student_logits_list: Optional[List[torch.Tensor]] = None,
                      teacher_logits_list: Optional[List[torch.Tensor]] = None) -> Dict[str, List]:
        """
        æ‰¹é‡è¯„ä¼°æ¨ç†è´¨é‡
        
        Args:
            student_responses: å­¦ç”Ÿå“åº”åˆ—è¡¨
            teacher_responses: æ•™å¸ˆå“åº”åˆ—è¡¨
            ground_truth_answers: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œæ¨èæä¾›ï¼‰
            student_logits_list: å­¦ç”Ÿlogitsåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            teacher_logits_list: æ•™å¸ˆlogitsåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ‰¹é‡è¯„ä¼°ç»“æœ
        """
        batch_results = []
        
        for i, (student_resp, teacher_resp) in enumerate(zip(student_responses, teacher_responses)):
            student_logits = student_logits_list[i] if student_logits_list else None
            teacher_logits = teacher_logits_list[i] if teacher_logits_list else None
            ground_truth = ground_truth_answers[i] if ground_truth_answers else None
            
            result = self.evaluator.evaluate_reasoning_quality(
                student_resp, teacher_resp, ground_truth, student_logits, teacher_logits
            )
            
            batch_results.append(result)
        
        # è®¡ç®—æ‰¹é‡ç»Ÿè®¡
        batch_stats = self._compute_batch_statistics(batch_results)
        
        return {
            "individual_results": batch_results,
            "batch_statistics": batch_stats
        }
    
    def _compute_batch_statistics(self, batch_results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—æ‰¹é‡ç»Ÿè®¡ä¿¡æ¯"""
        if not batch_results:
            return {}
        
        # æå–å„é¡¹æŒ‡æ ‡
        overall_scores = [result["overall_score"] for result in batch_results]
        step_coverage_scores = [result["step_coverage"]["step_coverage"] for result in batch_results]
        logical_consistency_scores = [result["logical_consistency"]["overall_consistency"] for result in batch_results]
        kl_divergences = [result["kl_divergence"] for result in batch_results]
        
        stats = {
            "mean_overall_score": np.mean(overall_scores),
            "std_overall_score": np.std(overall_scores),
            "mean_step_coverage": np.mean(step_coverage_scores),
            "mean_logical_consistency": np.mean(logical_consistency_scores),
            "mean_kl_divergence": np.mean(kl_divergences),
            "batch_size": len(batch_results)
        }
        
        # å¦‚æœæœ‰ç­”æ¡ˆæ­£ç¡®æ€§æ•°æ®ï¼Œä¹Ÿè®¡ç®—å…¶ç»Ÿè®¡ä¿¡æ¯
        if "answer_correctness" in batch_results[0]:
            correctness_scores = [
                result["answer_correctness"]["correctness_score"] 
                for result in batch_results 
                if "answer_correctness" in result
            ]
            is_correct_list = [
                result["answer_correctness"]["is_correct"] 
                for result in batch_results 
                if "answer_correctness" in result
            ]
            
            if correctness_scores:
                stats["mean_answer_correctness"] = np.mean(correctness_scores)
                stats["std_answer_correctness"] = np.std(correctness_scores)
                stats["accuracy"] = np.mean(is_correct_list)  # å®Œå…¨æ­£ç¡®çš„æ¯”ä¾‹
                stats["correct_count"] = sum(is_correct_list)
                stats["total_count"] = len(is_correct_list)
        
        return stats


def create_reasoning_evaluator() -> ReasoningEvaluator:
    """åˆ›å»ºæ¨ç†è¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°"""
    return ReasoningEvaluator()






