#!/usr/bin/env python3
"""
Reinforcement Learning Training Script
Function: PPO training based on intrinsic rewards
"""

import argparse
import yaml
import logging
import os
from pathlib import Path
import sys
from tqdm import tqdm
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from training.rl_trainer import RLTrainer
from data.gsm8k_processor import GSM8KProcessor
from datasets import load_dataset


def setup_logging(log_level: str = "INFO", log_file: str = None):
    """è®¾ç½®æ—¥å¿—"""
    import os
    from pathlib import Path
    from datetime import datetime
    
    # åˆ›å»ºlogsç›®å½•
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åç§°
    if log_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = logs_dir / f"rl_training_{timestamp}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # é…ç½®æ—¥å¿—ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),  # æ–‡ä»¶è¾“å‡º
            logging.StreamHandler()  # æ§åˆ¶å°è¾“å‡º
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def prepare_data(config: dict):
    """å‡†å¤‡æ•°æ®"""
    # åŠ è½½GSM8Kæ•°æ®é›†
    dataset = load_dataset("gsm8k", "main")
    
    # æ•°æ®é™åˆ¶ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    max_train_samples = config["data"].get("max_train_samples")
    max_eval_samples = config["data"].get("max_eval_samples")
    
    if max_train_samples:
        dataset["train"] = dataset["train"].select(range(min(max_train_samples, len(dataset["train"]))))
    
    if max_eval_samples:
        dataset["test"] = dataset["test"].select(range(min(max_eval_samples, len(dataset["test"]))))
    
    print(f"Training set size: {len(dataset['train'])}")
    print(f"Test set size: {len(dataset['test'])}")
    
    return dataset


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    parser.add_argument("--config", type=str, default="config/training_config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--student_model_path", type=str, default=None,
                       help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„ï¼ˆSFTåçš„æ¨¡å‹ï¼‰")
    parser.add_argument("--max_steps", type=int, default=None,
                       help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--log_level", type=str, default="INFO",
                       help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--log_file", type=str, default=None,
                       help="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šlogs/rl_training_YYYYMMDD_HHMMSS.logï¼‰")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                       help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = args.log_file if hasattr(args, 'log_file') and args.log_file else None
    setup_logging(args.log_level, log_file)
    logger = logging.getLogger(__name__)
    
    # æ€»ä½“è¿›åº¦æ¡ï¼ˆå¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤åˆ™æœ‰6æ­¥ï¼Œå¦åˆ™5æ­¥ï¼‰
    total_steps = 6 if args.resume_from_checkpoint else 5
    main_progress = tqdm(total=total_steps, desc="RLè®­ç»ƒæ€»è¿›åº¦", ncols=100, position=0)
    
    try:
        start_time = time.time()
        
        # æ­¥éª¤1: åŠ è½½é…ç½®
        main_progress.set_description("ğŸ“‹ åŠ è½½é…ç½®")
        config = load_config(args.config)
        
        # è¦†ç›–é…ç½®
        if args.output_dir:
            config["ppo"]["output_dir"] = args.output_dir
        
        if args.student_model_path:
            config["model"]["student_model_name"] = args.student_model_path
        
        if args.max_steps:
            config["training"]["max_steps"] = args.max_steps
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = config["ppo"]["output_dir"]
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        main_progress.update(1)
        main_progress.set_postfix({"status": "é…ç½®åŠ è½½å®Œæˆ"})
        
        # æ­¥éª¤2: å‡†å¤‡æ•°æ®
        main_progress.set_description("ğŸ“Š å‡†å¤‡æ•°æ®")
        logger.info("Preparing dataset...")
        dataset = prepare_data(config)
        main_progress.update(1)
        main_progress.set_postfix({"status": "æ•°æ®å‡†å¤‡å®Œæˆ"})
        
        # æ­¥éª¤3: åˆ›å»ºè®­ç»ƒå™¨
        main_progress.set_description("ğŸ—ï¸ åˆå§‹åŒ–è®­ç»ƒå™¨")
        logger.info("Initializing RL trainer...")
        trainer = RLTrainer(config)
        main_progress.update(1)
        main_progress.set_postfix({"status": "è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ"})
        
        # æ­¥éª¤4: è®¾ç½®æ¨¡å‹å’Œç»„ä»¶
        main_progress.set_description("âš™ï¸ è®¾ç½®æ¨¡å‹å’Œç»„ä»¶")
        logger.info("Setting up models...")
        trainer.setup_models()
        trainer.setup_components()
        trainer.setup_ppo_trainer()
        main_progress.update(1)
        main_progress.set_postfix({"status": "æ¨¡å‹å’Œç»„ä»¶è®¾ç½®å®Œæˆ"})
        
        # æ­¥éª¤5: å¦‚æœéœ€è¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        if args.resume_from_checkpoint:
            main_progress.set_description("ğŸ”„ æ¢å¤æ£€æŸ¥ç‚¹")
            logger.info(f"Resuming from checkpoint: {args.resume_from_checkpoint}")
            trainer.load_checkpoint(args.resume_from_checkpoint)
            main_progress.update(1)
            main_progress.set_postfix({"status": "æ£€æŸ¥ç‚¹æ¢å¤å®Œæˆ"})
        
        # æ­¥éª¤6: å¼€å§‹è®­ç»ƒ
        main_progress.set_description("ğŸš€ å¼€å§‹è®­ç»ƒ")
        logger.info("Starting RL training...")
        train_dataset = dataset[config["data"]["train_split"]]
        
        trainer.train(train_dataset, max_steps=config["training"]["max_steps"])
        main_progress.update(1)
        main_progress.set_postfix({"status": "è®­ç»ƒå®Œæˆ"})
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        total_time = time.time() - start_time
        main_progress.close()
        
        print(f"\nğŸ‰ å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
        
        logger.info("Reinforcement learning training completed!")
        logger.info(f"Model saved to: {output_dir}")
        
    except Exception as e:
        main_progress.close()
        logger.error(f"RL training failed: {e}")
        raise
    
    finally:
        # æ¸…ç†èµ„æº
        if 'trainer' in locals():
            trainer.cleanup()


if __name__ == "__main__":
    main()






