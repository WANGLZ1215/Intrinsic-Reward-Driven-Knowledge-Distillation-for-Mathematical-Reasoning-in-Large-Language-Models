#!/usr/bin/env python3
"""
ç‹¬ç«‹æµ‹è¯•ï¼šQwen2.5-32B-Instruct åœ¨ GSM8K æ•°æ®é›†ä¸Šçš„è¡¨ç°

å®Œå…¨ç‹¬ç«‹ç‰ˆæœ¬ - ä¸ä¾èµ–é¡¹ç›®ä¸­çš„å…¶ä»–æ¨¡å—
å¯å•ç‹¬ä¸Šä¼ åˆ°VAST AIè¿è¡Œ

åŠŸèƒ½ï¼š
- ç›´æ¥åŠ è½½æ¨¡å‹å’Œtokenizer
- æµ‹è¯•GSM8Kæ•°æ®é›†
- æå–ç­”æ¡ˆå¹¶è®¡ç®—å‡†ç¡®ç‡
- å¯¼å‡ºJSONLæ ¼å¼ï¼ˆä¸å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸€è‡´ï¼ŒåŒ…å«top_ids/top_probsï¼‰
- ä¿å­˜ç»“æœåˆ°JSONLæ–‡ä»¶ï¼Œä¾¿äºçŸ¥è¯†è’¸é¦å¯¹æ¯”

ç”¨æ³•:
python test_qwen32b_gsm8k.py --eval_samples 200 --out teacher_gsm8k.jsonl
"""

import argparse
import json
import logging
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import threading
import hashlib
from collections import OrderedDict

import torch
import torch.nn.functional as F
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList
from tqdm import tqdm


# âœ… ä¿®å¤1: è®¾ç½®CUDA_LAUNCH_BLOCKING
if "CUDA_LAUNCH_BLOCKING" not in os.environ:
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"


def setup_logging(level: str = "INFO") -> None:
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    if os.environ.get("CUDA_LAUNCH_BLOCKING") == "1":
        logging.info("âœ… CUDA_LAUNCH_BLOCKING=1 å·²å¯ç”¨")


def extract_answer_unified(text: str) -> Tuple[str, Optional[float]]:
    """
    ç»Ÿä¸€çš„ç­”æ¡ˆæå–å‡½æ•° - æ”¯æŒå¤šç§æ ¼å¼
    è¿™æ˜¯é¡¹ç›®ä¸­å”¯ä¸€çš„ç­”æ¡ˆæå–å®ç°ï¼Œå…¶ä»–æ¨¡å—åº”è°ƒç”¨æ­¤å‡½æ•°
    
    æ”¯æŒæ ¼å¼ï¼š
    - #### (GSM8Kæ ‡å‡†æ ¼å¼) - æœ€é«˜ä¼˜å…ˆçº§
    - \\boxed{} (LaTeXæ ¼å¼)
    - "answer:" æˆ– "answerï¼š"
    - "The answer is"
    - å…œåº•ï¼šæœ€åä¸€ä¸ªæ•°å­—
    
    Args:
        text: è¾“å…¥æ–‡æœ¬ï¼ˆåŒ…å«æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆï¼‰
        
    Returns:
        (ç­”æ¡ˆæ–‡æœ¬, ç­”æ¡ˆæ•°å­—)
        - ç­”æ¡ˆæ–‡æœ¬: æå–çš„åŸå§‹ç­”æ¡ˆå­—ç¬¦ä¸²
        - ç­”æ¡ˆæ•°å­—: è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¦‚æœæ— æ³•æå–åˆ™ä¸ºNone
    """
    if not text:
        return "", None
    
    # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    text_clean = re.sub(r'[^\w\s\.,!?;:()\[\]{}"\'-]', '', text)
    
    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šGSM8Kæ ‡å‡†çš„ #### æ ¼å¼
    matches = re.findall(r"####\s*([\$]?[-+]?\d{1,7}(?:,\d{3})*(?:\.\d+)?%?)", text_clean)
    if matches:
        # ä½¿ç”¨æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé¿å…ç¤ºä¾‹ä¸­çš„å¹²æ‰°ï¼‰
        last_match = matches[-1]
        # æ£€æŸ¥è¿™ä¸ªåŒ¹é…æ˜¯å¦åœ¨ç¤ºä¾‹ä¹‹åï¼ˆé¿å…æå–ç¤ºä¾‹ä¸­çš„ç­”æ¡ˆï¼‰
        match_pos = text_clean.rfind(f"#### {last_match}")
        example_pos = max(text_clean.rfind("Example"), text_clean.rfind("æ ·ä¾‹"), text_clean.rfind("example"))
        
        if example_pos == -1 or match_pos > example_pos:
            answer_text = last_match
        else:
            # å¦‚æœæœ€åä¸€ä¸ªåŒ¹é…åœ¨ç¤ºä¾‹ä¸­ï¼Œå°è¯•å€’æ•°ç¬¬äºŒä¸ª
            if len(matches) > 1:
                answer_text = matches[-2]
            else:
                answer_text = None
        
        if answer_text:
            # æ¸…ç†æ•°å­—æ ¼å¼å¹¶è½¬æ¢
            num_clean = answer_text.replace('$', '').replace(',', '').strip()
            if num_clean.endswith('%'):
                num_clean = num_clean[:-1]
            try:
                return answer_text, float(num_clean)
            except ValueError:
                pass
    
    # ç¬¬äºŒä¼˜å…ˆçº§ï¼š\boxed{} æ ¼å¼
    match = re.search(r"\\boxed\{([\$]?[-+]?\d{1,7}(?:,\d{3})*(?:\.\d+)?%?)\}", text_clean)
    if match:
        answer_text = match.group(1)
        num_clean = answer_text.replace('$', '').replace(',', '').strip()
        if num_clean.endswith('%'):
            num_clean = num_clean[:-1]
        try:
            return answer_text, float(num_clean)
        except ValueError:
            pass
    
    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼š"answer:" æˆ– "answerï¼š" æ ¼å¼
    match = re.search(r"answer[:ï¼š]?\s*([\$]?[-+]?\d{1,7}(?:,\d{3})*(?:\.\d+)?%?)", text_clean, re.IGNORECASE)
    if match:
        answer_text = match.group(1)
        num_clean = answer_text.replace('$', '').replace(',', '').strip()
        if num_clean.endswith('%'):
            num_clean = num_clean[:-1]
        try:
            return answer_text, float(num_clean)
        except ValueError:
            pass
    
    # ç¬¬å››ä¼˜å…ˆçº§ï¼š"The answer is" æ ¼å¼
    match = re.search(r'The answer is\s*[\$]?([-+]?\d{1,7}(?:,\d{3})*(?:\.\d+)?)', text, re.IGNORECASE)
    if match:
        answer_text = match.group(1)
        num_clean = answer_text.replace(',', '').strip()
        try:
            return answer_text, float(num_clean)
        except ValueError:
            pass
    
    # å…œåº•æ–¹æ¡ˆï¼šæå–æœ€åä¸€ä¸ªæ•°å­—
    numbers = re.findall(r'[-+]?\d{1,7}(?:,\d{3})*(?:\.\d+)?', text)
    if numbers:
        answer_text = numbers[-1]
        num_clean = answer_text.replace(',', '')
        try:
            return answer_text, float(num_clean)
        except ValueError:
            pass
    
    return "", None


def softmax_topk_from_logits(logits: torch.Tensor, top_k: int = 50) -> Tuple[List[int], List[float]]:
    """ä»æœ€åä¸€æ­¥logitsè®¡ç®—top-kæ¦‚ç‡åˆ†å¸ƒï¼ˆè¿”å›idsä¸probsåˆ—è¡¨ï¼‰ã€‚
    ä»…å–æœ€åä¸€æ­¥ï¼ˆpromptæœ€åtokenï¼‰ä»¥é™ä½ä½“ç§¯ï¼Œä¾›è¿‘ä¼¼è’¸é¦å¯¹æ¯”ä½¿ç”¨ã€‚
    """
    if logits is None:
        return [], []
    last_logits = logits[-1]  # (vocab_size,)
    probs = torch.softmax(last_logits.float(), dim=-1)
    topk = min(top_k, probs.shape[-1])
    values, indices = torch.topk(probs, k=topk, dim=-1)
    return indices.tolist(), values.tolist()


class SimpleTeacherModel:
    """ç®€åŒ–ç‰ˆæ•™å¸ˆæ¨¡å‹åŒ…è£…å™¨ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-32B-Instruct",
                 cache_size: int = 1000, device_map: str = "auto",
                 torch_dtype: str = "bfloat16"):
        """åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹"""
        self.model_name = model_name
        self.cache_size = cache_size
        self.cache = OrderedDict()
        self._tokenizer_lock = threading.Lock()
        
        # è½¬æ¢torch_dtype
        dtype_map = {
            "float32": torch.float32,
            "float16": torch.float16,
            "bfloat16": torch.bfloat16
        }
        torch_dtype_val = dtype_map.get(torch_dtype, torch.bfloat16)
        
        # åŠ è½½tokenizer
        logging.info(f"åŠ è½½tokenizer: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        logging.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device_map,
            torch_dtype=torch_dtype_val,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        self.model.eval()
        
        # æ£€æŸ¥vocab_size
        tokenizer_vocab_size = len(self.tokenizer)
        model_emb_size = self.model.get_input_embeddings().weight.size(0)
        logging.info(f"ğŸ“Š Vocabå¤§å°æ£€æŸ¥:")
        logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
        logging.info(f"   model embedding size: {model_emb_size}")
        
        if model_emb_size != tokenizer_vocab_size:
            logging.warning(f"âš ï¸  Vocabå¤§å°ä¸åŒ¹é…ï¼ˆ{model_emb_size} vs {tokenizer_vocab_size}ï¼‰")
            logging.info("   é‡‡ç”¨æ–¹å¼1ï¼šä¸resizeï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹åŸå§‹æƒé‡")
        else:
            logging.info("âœ… Vocabå¤§å°å·²åŒ¹é…")
        
        self.vocab_size = model_emb_size
        # è·å–æ¨¡å‹æ‰€åœ¨çš„deviceï¼ˆå¯¹äºdevice_map="auto"çš„æƒ…å†µï¼‰
        self.device = next(self.model.parameters()).device
        logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _update_cache(self, key: str, value: torch.Tensor):
        """æ›´æ–°ç¼“å­˜"""
        self.cache[key] = value.clone().detach().cpu()
        if len(self.cache) > self.cache_size:
            self.cache.popitem(last=False)
    
    def get_logits(self, text: str, use_cache: bool = True) -> torch.Tensor:
        """è·å–æ–‡æœ¬çš„logits"""
        cache_key = None
        if use_cache:
            cache_key = self._get_cache_key(text)
            if cache_key in self.cache:
                return self.cache[cache_key]
        
        # è®¡ç®—logits
        with self._tokenizer_lock:
            with torch.no_grad():
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=True
                )
                
                # ç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
                if 'input_ids' in inputs:
                    input_ids = inputs['input_ids']
                    if input_ids.numel() > 0:
                        if input_ids.max().item() >= self.vocab_size or input_ids.min().item() < 0:
                            logging.warning(f"âš ï¸ input_idsè¶…å‡ºèŒƒå›´ï¼Œè‡ªåŠ¨clamp")
                            inputs['input_ids'] = torch.clamp(input_ids, 0, self.vocab_size - 1)
                
                # å°†inputsç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨device
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                outputs = self.model(**inputs)
                logits = outputs.logits[0]  # å»æ‰batchç»´åº¦
                
                if use_cache and cache_key is not None:
                    self._update_cache(cache_key, logits)
                
                return logits
    
    def generate_response(self, prompt: str, max_length: int = 512,
                         temperature: float = 0.7, do_sample: bool = True) -> str:
        """ç”Ÿæˆå“åº”"""
        with self._tokenizer_lock:
            with torch.no_grad():
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                )
                
                # ç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
                if 'input_ids' in inputs:
                    input_ids = inputs['input_ids']
                    if input_ids.numel() > 0:
                        if input_ids.max().item() >= self.vocab_size or input_ids.min().item() < 0:
                            logging.warning(f"âš ï¸ input_idsè¶…å‡ºèŒƒå›´ï¼Œè‡ªåŠ¨clamp")
                            inputs['input_ids'] = torch.clamp(input_ids, 0, self.vocab_size - 1)
                
                # è®¾ç½®æœ‰æ•ˆçš„pad_token_idå’Œeos_token_id
                pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
                eos_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else pad_token_id
                
                # ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
                pad_token_id = min(pad_token_id, self.vocab_size - 1)
                eos_token_id = min(eos_token_id, self.vocab_size - 1)
                
                # è®¡ç®—max_new_tokens
                current_len = inputs['input_ids'].shape[1]
                max_allowed_new_tokens = min(max_length, 2048 - current_len - 10)
                
                if max_allowed_new_tokens <= 0:
                    logging.error(f"âŒ æ— æ³•ç”Ÿæˆæ–°token: å½“å‰é•¿åº¦ {current_len} >= æœ€å¤§é•¿åº¦ {2048}")
                    return ""
                
                # LogitsProcessoræ¥maskè¶…å‡ºèŒƒå›´çš„token
                class TokenRangeLogitsProcessor:
                    def __init__(self, max_valid_token_id: int):
                        self.max_valid_token_id = max_valid_token_id
                        self.vocab_end_idx = max_valid_token_id + 1
                    
                    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
                        if scores.shape[-1] > self.vocab_end_idx:
                            scores[..., self.vocab_end_idx:] = float('-inf')
                        return scores
                
                logits_processor = LogitsProcessorList([
                    TokenRangeLogitsProcessor(self.vocab_size - 1)
                ])
                
                # å°†inputsç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨device
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ç”Ÿæˆ
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    repetition_penalty=1.1,
                    logits_processor=logits_processor,
                    use_cache=True
                )
                
                # è§£ç 
                input_length = inputs['input_ids'].shape[1]
                if len(outputs[0]) > input_length:
                    generated_text = self.tokenizer.decode(
                        outputs[0][input_length:],
                        skip_special_tokens=True
                    )
                else:
                    generated_text = ""
                
                return generated_text


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Qwen2.5-32B-Instructåœ¨GSM8Kä¸Šçš„è¡¨ç°")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-32B-Instruct",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--eval_samples", type=int, default=200,
                       help="è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--eval_split", type=str, default="test", choices=["train", "test"],
                       help="æ•°æ®é›†åˆ†ç‰‡")
    parser.add_argument("--max_length", type=int, default=512,
                       help="ç”Ÿæˆæœ€å¤§é•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--topk_dist", type=int, default=50,
                       help="å¯¼å‡ºä¸‹ä¸€æ­¥åˆ†å¸ƒçš„top-kå¤§å°ï¼ˆ0è¡¨ç¤ºä¸å¯¼å‡ºï¼‰")
    parser.add_argument("--device_map", type=str, default="auto",
                       help="è®¾å¤‡æ˜ å°„")
    parser.add_argument("--torch_dtype", type=str, default="bfloat16",
                       choices=["float32", "float16", "bfloat16"],
                       help="æ•°æ®ç±»å‹")
    parser.add_argument("--out", type=str, default="teacher_gsm8k.jsonl",
                       help="è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log_level", type=str, default="INFO",
                       help="æ—¥å¿—çº§åˆ«")
    args = parser.parse_args()
    
    setup_logging(args.log_level)
    torch.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    logging.info(f"åŠ è½½æ•°æ®é›†: GSM8K/{args.eval_split}")
    try:
        ds = load_dataset("gsm8k", "main")
        split = ds[args.eval_split]
        n = min(args.eval_samples, len(split))
        eval_ds = split.select(range(n))
        logging.info(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {n} ä¸ªæ ·æœ¬")
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    logging.info("åŠ è½½æ•™å¸ˆæ¨¡å‹...")
    try:
        teacher = SimpleTeacherModel(
            model_name=args.model_name,
            cache_size=1000,
            device_map=args.device_map,
            torch_dtype=args.torch_dtype
        )
        logging.info("âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return
    
    # æµ‹è¯•å¹¶å¯¼å‡º
    logging.info("=" * 80)
    logging.info("å¼€å§‹æµ‹è¯•")
    logging.info("=" * 80)
    
    num_ok = 0
    num_failed = 0
    num_correct = 0
    failed_indices = []
    
    with open(out_path, 'w', encoding='utf-8') as f:
        for idx, sample in enumerate(tqdm(eval_ds, desc="æµ‹è¯•è¿›åº¦", ncols=100)):
            q = sample["question"]
            gt = sample["answer"]
            prompt = f"Question: {q}\nAnswer: "
            
            # ç”Ÿæˆ
            resp = ""
            generation_error = None
            try:
                resp = teacher.generate_response(
                    prompt, 
                    max_length=args.max_length,
                    temperature=args.temperature, 
                    do_sample=True
                )
                if not isinstance(resp, str):
                    resp = str(resp) if resp else ""
            except Exception as e:
                generation_error = str(e)
                logging.warning(f"æ ·æœ¬{idx+1} ç”Ÿæˆå¤±è´¥: {e}")
                resp = ""
                if "CUDA" in str(e) or "device-side assert" in str(e):
                    torch.cuda.empty_cache()
            
            # æå–ç­”æ¡ˆï¼ˆç»Ÿä¸€å®ç°ï¼Œä¼˜å…ˆ ####ï¼‰
            gt_text, gt_num = extract_answer_unified(gt)
            pred_text, pred_num = extract_answer_unified(resp) if resp else ("", None)
            
            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = False
            if pred_text and gt_text:
                if pred_num is not None and gt_num is not None:
                    if abs(gt_num) < 1e-10:
                        is_correct = abs(pred_num - gt_num) < 1e-6
                    else:
                        relative_error = abs(pred_num - gt_num) / abs(gt_num)
                        is_correct = relative_error < 1e-6
                else:
                    is_correct = pred_text.strip().lower() == gt_text.strip().lower()
            
            if is_correct:
                num_correct += 1
            
            # å¯é€‰ï¼šå¯¼å‡ºä¸‹ä¸€æ­¥åˆ†å¸ƒï¼ˆåœ¨promptä¸Šï¼Œé¿å…å“åº”é•¿åº¦å·®å¼‚å½±å“ä½“ç§¯ï¼‰
            top_ids: List[int] = []
            top_probs: List[float] = []
            logits_error = None
            if args.topk_dist and args.topk_dist > 0 and not generation_error:
                try:
                    logits = teacher.get_logits(prompt)
                    if logits is not None and logits.ndim >= 2:
                        top_ids, top_probs = softmax_topk_from_logits(logits, args.topk_dist)
                except Exception as e:
                    logits_error = str(e)
                    logging.debug(f"æ ·æœ¬{idx+1} æå–top-kå¤±è´¥: {e}")
            
            # ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼ˆä¸å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸€è‡´ï¼‰
            record = {
                "index": idx,
                "question": q,
                "prompt": prompt,
                "ground_truth": gt,
                "ground_truth_text": gt_text if gt_text else "N/A",
                "ground_truth_num": gt_num if gt_num is not None else "N/A",
                "response": resp if resp else "",
                "answer_text": pred_text if pred_text else "N/A",
                "answer_num": pred_num if pred_num is not None else "N/A",
                "top_ids": top_ids,
                "top_probs": top_probs,
                "error": generation_error if generation_error else (logits_error if logits_error else None)
            }
            
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
            f.flush()
            os.fsync(f.fileno())
            
            if generation_error:
                num_failed += 1
                failed_indices.append(idx)
            else:
                num_ok += 1
    
    # æ‰“å°æ€»ç»“
    accuracy = num_correct / n if n > 0 else 0.0
    logging.info("=" * 80)
    logging.info("æµ‹è¯•å®Œæˆ")
    logging.info("=" * 80)
    logging.info(f"æ€»æ ·æœ¬æ•°: {n}")
    logging.info(f"æˆåŠŸç”Ÿæˆ: {num_ok}")
    logging.info(f"ç”Ÿæˆå¤±è´¥: {num_failed}")
    logging.info(f"æ­£ç¡®ç­”æ¡ˆ: {num_correct}")
    logging.info(f"å‡†ç¡®ç‡: {accuracy:.4f} ({num_correct}/{n})")
    logging.info(f"ç»“æœå·²ä¿å­˜åˆ°: {out_path.absolute()}")
    if num_failed > 0:
        logging.warning(f"å¤±è´¥æ ·æœ¬ç´¢å¼•: {failed_indices[:20]}{'...' if len(failed_indices) > 20 else ''}")
    logging.info("=" * 80)


if __name__ == "__main__":
    main()
