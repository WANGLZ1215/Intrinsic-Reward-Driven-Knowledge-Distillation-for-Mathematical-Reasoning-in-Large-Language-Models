# è®­ç»ƒé…ç½®æ–‡ä»¶
# é€‚ç”¨äºVAST AI GPUç¯å¢ƒ - H200 140GBç‰ˆæœ¬

# æ¨¡å‹é…ç½® - 280GBæ˜¾å­˜ä¼˜åŒ–é…ç½®ï¼ˆå……åˆ†åˆ©ç”¨2Ã—140GB H200ï¼‰
model:
  teacher_model_name: "Qwen/Qwen2.5-32B-Instruct"
  student_model_name: "Qwen/Qwen2.5-7B-Instruct"
  cache_size: 500  # ğŸ’¡ å·²ç¦ç”¨ï¼šå‘½ä¸­ç‡0%æ— æ•ˆï¼Œä»£ç ä¸­è®¾ç½®use_cache=False
  cache_policy: "LRU"
  use_lora: true

# LoRAé…ç½®
lora:
  r: 8                     # é™ä½ç§©ä»¥å‡å°‘å‚æ•°æ•°é‡
  lora_alpha: 16           # ä¿æŒalpha/r=2çš„æ¯”ä¾‹
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  lora_dropout: 0.1        # ä¿æŒdropouté˜²æ­¢è¿‡æ‹Ÿåˆ
  bias: "none"
  task_type: "CAUSAL_LM"

# ç›‘ç£å¾®è°ƒé…ç½®
sft:
  output_dir: "./checkpoints/sft_model"
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 3
  learning_rate: 2e-5
  save_strategy: "epoch"
  eval_strategy: "epoch"  # ä½¿ç”¨æ–°çš„å‚æ•°åï¼Œé¿å…å¼ƒç”¨è­¦å‘Š
  logging_steps: 100
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  max_length: 512
  warmup_steps: 100

# PPOé…ç½® - 280GBæ˜¾å­˜ä¼˜åŒ–é…ç½®ï¼ˆå……åˆ†åˆ©ç”¨H200 140GBï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆæœï¼‰
ppo:
  output_dir: "./checkpoints/rl_model"
  learning_rate: 1e-5
  batch_size: 2  # ğŸ”¥ ç´§æ€¥é™ä½ï¼šä»4é™åˆ°2é¿å…OOM
  mini_batch_size: 1  # ğŸ”¥ ç´§æ€¥é™ä½ï¼šä»2é™åˆ°1
  forward_batch_size: 1  # âœ… ä¿å®ˆé…ç½®ï¼šé™åˆ°1ï¼Œlog_softmaxå³°å€¼å†…å­˜å‡åŠ
  ppo_epochs: 2  # ğŸ”¥ ç´§æ€¥é™ä½ï¼šä»3é™åˆ°2å‡å°‘é‡å¤è®¡ç®—
  clip_ratio: 0.2
  value_loss_coef: 0.1
  entropy_coef: 0.01
  kl_coef: 0.05
  gamma: 0.99
  lambda_gae: 0.95
  max_grad_norm: 1.0
  max_length: 256  # ğŸ”¥ ç´§æ€¥é™ä½ï¼šä»384é™åˆ°256å‡å°‘åºåˆ—é•¿åº¦
  temperature: 0.7
  do_sample: true
  gradient_checkpointing: true  # âœ… å»ºè®®å¯ç”¨ï¼šH200è™½å……è¶³ï¼Œä½†å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨

# å¥–åŠ±é…ç½®
reward:
  lambda_intrinsic: 0.7  # å†…åœ¨å¥–åŠ±æƒé‡
  lambda_correctness: 0.3  # ç­”æ¡ˆæ­£ç¡®æ€§æƒé‡
  temperature: 1.0  # å†…åœ¨å¥–åŠ±æ¸©åº¦å‚æ•°
  normalization: "mean_std"  # å½’ä¸€åŒ–æ–¹æ³•
  clip_min: -5.0  # å¥–åŠ±è£å‰ªæœ€å°å€¼
  clip_max: 5.0  # å¥–åŠ±è£å‰ªæœ€å¤§å€¼
  update_rate: 0.01  # å†…åœ¨å¥–åŠ±ç»Ÿè®¡æ›´æ–°ç‡
  # è‡ªé€‚åº”æƒé‡é…ç½®
  use_adaptive_weights: true  # å¯ç”¨è‡ªé€‚åº”æƒé‡
  adaptation_rate: 0.01  # æƒé‡é€‚åº”ç‡
  reasoning_weight: 0.0  # æ¨ç†è¿‡ç¨‹æƒé‡
  format_weight: 0.0  # æ ¼å¼çº¦æŸæƒé‡

# æ•°æ®é…ç½®
data:
  dataset_name: "gsm8k"
  dataset_config: "main"
  max_train_samples: null  # nullè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
  max_eval_samples: null 
  train_split: "train"
  eval_split: "test"

# è®­ç»ƒé…ç½® - 2Ã—H200 140GBä¼˜åŒ–é…ç½®
training:
  max_steps: 1000
  save_steps: 50  # ğŸš€ H200æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥æ›´é¢‘ç¹ä¿å­˜
  eval_steps: 100  # ğŸš€ H200æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥æ›´é¢‘ç¹è¯„ä¼°
  logging_steps: 10
  gradient_accumulation_steps: 16  # ğŸ”¥ è°ƒæ•´ï¼šbatch_size=2æ—¶ç”¨16ï¼Œä¿æŒæœ‰æ•ˆbatch_size=32
  fp16: false  # ç¦ç”¨FP16é¿å…æ¢¯åº¦ä¸ç¨³å®š
  bf16: true   # ä½¿ç”¨BF16æ›¿ä»£FP16ï¼Œæ›´ç¨³å®š
  dataloader_num_workers: 4  # ğŸš€ H200æ€§èƒ½å¼ºåŠ²ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨å¤šçº¿ç¨‹
  remove_unused_columns: false

# è¯„ä¼°é…ç½®
evaluation:
  eval_batch_size: 8
  max_eval_samples: null  # nullè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†æ•°æ®
  metrics: ["accuracy", "step_coverage", "logical_consistency", "kl_divergence"]
  save_predictions: true

# è®¾å¤‡é…ç½®
device:
  use_cuda: true
  device_map: "auto"
  torch_dtype: "bfloat16"  # ä½¿ç”¨bfloat16æ›¿ä»£float16
  use_mixed_precision: false  # ç¦ç”¨æ··åˆç²¾åº¦é¿å…FP16é—®é¢˜

# å¹¶è¡Œå¤„ç†é…ç½®
parallel:
  enabled: false  # æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆå·²ç¦ç”¨ï¼Œä½¿ç”¨å•çº¿ç¨‹é¿å…tokenizerçº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
  num_workers: 1  # å¹¶è¡Œå·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°
  use_threads: true  # æ˜¯å¦ä½¿ç”¨çº¿ç¨‹æ± ï¼ˆTrueï¼‰è¿˜æ˜¯è¿›ç¨‹æ± ï¼ˆFalseï¼‰
  inference_batch_size: 16  # å¹¶è¡Œæ¨ç†æ‰¹æ¬¡å¤§å°
  cache_queue_size: 1000  # å¼‚æ­¥ç¼“å­˜é˜Ÿåˆ—å¤§å°
  use_parallel_data_loader: false  # æ˜¯å¦ä½¿ç”¨å¹¶è¡Œæ•°æ®åŠ è½½å™¨ï¼ˆå·²ç¦ç”¨ï¼‰
  data_loader_workers: 1  # æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°

# æ—¥å¿—é…ç½®
logging:
  log_level: "INFO"
  use_wandb: true
  wandb_project: "intrinsic-reward-distillation"
  use_tensorboard: true
  tensorboard_log_dir: "./logs"

