"""
Student Model Wrapper Module
Function: Wrap Qwen-7B-math student model, support LoRA fine-tuning and PPO training
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from trl import AutoModelForCausalLMWithValueHead
from typing import Dict, List, Optional, Union
import logging
from pathlib import Path
from utils.cache_utils import suppress_past_key_values_warning, update_model_for_modern_cache


class StudentModel:
    """Student Model Wrapper Class"""
    
    def __init__(self, model_name: str = "Qwen/Qwen-7B-Math",
                 lora_config: Optional[Dict] = None,
                 device: str = "auto", 
                 torch_dtype: torch.dtype = torch.bfloat16,
                 use_lora: bool = True):
        """
        åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            lora_config: LoRAé…ç½®
            device: è®¾å¤‡
            torch_dtype: æ•°æ®ç±»å‹
            use_lora: æ˜¯å¦ä½¿ç”¨LoRA
        """
        self.model_name = model_name
        self.device = device
        self.torch_dtype = torch_dtype
        self.use_lora = use_lora
        self.lora_config = lora_config or self._default_lora_config()
        
        # æŠ‘åˆ¶past_key_valuesè­¦å‘Š
        suppress_past_key_values_warning()
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        
        # æ›´æ–°æ¨¡å‹ä»¥ä½¿ç”¨ç°ä»£ç¼“å­˜
        self.model = update_model_for_modern_cache(self.model)
        
        logging.info(f"Student model {model_name} loaded successfully")
        logging.info(f"LoRA configuration: {self.lora_config}")
    
    def _default_lora_config(self) -> Dict:
        """é»˜è®¤LoRAé…ç½®"""
        return {
            "r": 16,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "lora_dropout": 0.1,
            "bias": "none",
            "task_type": TaskType.CAUSAL_LM
        }
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            import os
            from pathlib import Path
            
            # æ£€æŸ¥model_nameæ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯HuggingFaceæ¨¡å‹å
            model_path = Path(self.model_name)
            is_local_path = model_path.exists() and model_path.is_dir()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²è®­ç»ƒçš„æ¨¡å‹ï¼ˆåŒ…å«adapteræ–‡ä»¶ï¼‰
            is_trained_model = False
            if is_local_path:
                adapter_files = [
                    model_path / "adapter_model.bin",
                    model_path / "adapter_model.safetensors",
                    model_path / "adapter_config.json"
                ]
                is_trained_model = any(f.exists() for f in adapter_files[:2])  # æ£€æŸ¥æƒé‡æ–‡ä»¶
                
                if is_trained_model:
                    logging.info(f"æ£€æµ‹åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ç›®å½•: {self.model_name}")
                    logging.info("å°†åŠ è½½å·²è®­ç»ƒçš„LoRAé€‚é…å™¨")
            
            if is_trained_model and self.use_lora:
                # æƒ…å†µ1: åŠ è½½å·²è®­ç»ƒçš„LoRAé€‚é…å™¨
                # é¦–å…ˆéœ€è¦åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä»é…ç½®æˆ–æ¨¡å‹ç›®å½•çš„çˆ¶ç›®å½•è·å–ï¼‰
                base_model_name = None
                
                # æ£€æŸ¥æ˜¯å¦æœ‰adapter_config.jsonè¯´æ˜åŸºç¡€æ¨¡å‹
                config_path = model_path / "adapter_config.json"
                if config_path.exists():
                    import json
                    with open(config_path, 'r') as f:
                        adapter_config = json.load(f)
                        # PEFTé€‚é…å™¨é…ç½®ä¸­åŒ…å«åŸºç¡€æ¨¡å‹è·¯å¾„
                        base_model_name = adapter_config.get("base_model_name_or_path", None)
                
                # å¦‚æœæ— æ³•ä»é€‚é…å™¨é…ç½®è·å–åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€æ¨¡å‹
                if base_model_name is None:
                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åŸºç¡€æ¨¡å‹åï¼Œæˆ–é»˜è®¤å€¼
                    default_base = "Qwen/Qwen2.5-7B-Instruct"
                    logging.info(f"ä»é€‚é…å™¨é…ç½®æœªæ‰¾åˆ°åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤: {default_base}")
                    base_model_name = default_base
                else:
                    # æ£€æŸ¥base_model_nameæ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ä¸”ä¸å­˜åœ¨
                    if Path(base_model_name).exists():
                        logging.info(f"ä»é€‚é…å™¨é…ç½®æ‰¾åˆ°åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_name}")
                    else:
                        # å¯èƒ½æ˜¯HuggingFaceæ¨¡å‹åï¼Œç›´æ¥ä½¿ç”¨
                        logging.info(f"ä»é€‚é…å™¨é…ç½®æ‰¾åˆ°åŸºç¡€æ¨¡å‹: {base_model_name}")
                
                # âœ… ä¿®å¤ï¼šå…ˆåŠ è½½åˆ†è¯å™¨ï¼ˆä»åŸºç¡€æ¨¡å‹æˆ–æœ¬åœ°ç›®å½•ï¼‰
                # å¦‚æœæœ¬åœ°ç›®å½•æœ‰tokenizer.jsonï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°
                if (model_path / "tokenizer.json").exists() or (model_path / "tokenizer_config.json").exists():
                    logging.info(f"ä»æœ¬åœ°ç›®å½•åŠ è½½tokenizer: {self.model_name}")
                    tokenizer_path = self.model_name
                else:
                    logging.info(f"ä»åŸºç¡€æ¨¡å‹åŠ è½½tokenizer: {base_model_name}")
                    tokenizer_path = base_model_name
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    tokenizer_path,
                    trust_remote_code=True,
                    padding_side="left"
                )
                
                # è®¾ç½®pad token
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # åŠ è½½åŸºç¡€æ¨¡å‹
                # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨ device_mapï¼Œå¿…é¡»è®¾ç½® low_cpu_mem_usage=True
                # RL è®­ç»ƒéœ€è¦æ¨¡å‹å®Œå…¨åœ¨ GPU ä¸Šï¼Œä¸èƒ½ä½¿ç”¨ offload
                device_map_for_load = None if self.device == "auto" else self.device
                # âœ… ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ device_mapï¼Œå¿…é¡»è®¾ç½® low_cpu_mem_usage=True
                low_cpu_mem_usage = True if device_map_for_load is not None else False
                self.base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=self.torch_dtype,
                    device_map=device_map_for_load,  # å¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨device_map
                    trust_remote_code=True,
                    low_cpu_mem_usage=low_cpu_mem_usage  # âœ… ä¿®å¤ï¼šä½¿ç”¨device_mapæ—¶å¿…é¡»ä¸ºTrue
                )
                
                # å¦‚æœ device_map æ˜¯ Noneï¼Œéœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
                if device_map_for_load is None and torch.cuda.is_available():
                    self.base_model = self.base_model.to(torch.device("cuda:0"))
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆåŸºåº§ã€å†LoRA - resize_token_embeddingså¿…é¡»åœ¨åŠ è½½LoRAä¹‹å‰æ‰§è¡Œ
                tokenizer_vocab_size = len(self.tokenizer)
                try:
                    # è·å–åŸºç¡€æ¨¡å‹çš„çœŸå®embeddingå¤§å°
                    input_emb_size = self.base_model.get_input_embeddings().weight.size(0)
                    output_emb_size = None
                    if hasattr(self.base_model, 'get_output_embeddings') and self.base_model.get_output_embeddings() is not None:
                        output_emb_size = self.base_model.get_output_embeddings().weight.size(0)
                    
                    logging.info(f"ğŸ“Š Embeddingå¤§å°æ£€æŸ¥ï¼ˆåŠ è½½LoRAå‰ï¼‰:")
                    logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                    logging.info(f"   model input_embeddings.size(0): {input_emb_size}")
                    if output_emb_size is not None:
                        logging.info(f"   model output_embeddings.size(0): {output_emb_size}")
                    logging.info(f"   model.config.vocab_size: {getattr(self.base_model.config, 'vocab_size', 'N/A')}")
                    
                    # å¦‚æœembeddingå¤§å°ä¸tokenizerä¸åŒ¹é…ï¼Œæ‰§è¡Œresize
                    if input_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ æ¨¡å‹embeddingå¤§å° ({input_emb_size}) != tokenizerå¤§å° ({tokenizer_vocab_size})")
                        logging.info(f"   æ­£åœ¨resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                        self.base_model.resize_token_embeddings(tokenizer_vocab_size)
                        logging.info(f"âœ… resize_token_embeddingså®Œæˆ")
                        
                        # éªŒè¯resizeæ˜¯å¦æˆåŠŸ
                        new_input_emb_size = self.base_model.get_input_embeddings().weight.size(0)
                        if new_input_emb_size != tokenizer_vocab_size:
                            logging.error(f"âŒ resize_token_embeddingså¤±è´¥ï¼æ–°å¤§å°: {new_input_emb_size} != {tokenizer_vocab_size}")
                            logging.warning(f"   å°†ä¿ç•™'é™åŸŸ+clamp'çš„ä¿é™©ç­–ç•¥")
                        else:
                            logging.info(f"âœ… resizeæˆåŠŸéªŒè¯: input_embeddings.size(0) = {new_input_emb_size}")
                    else:
                        logging.info(f"âœ… embeddingå¤§å°ä¸tokenizeråŒ¹é…ï¼Œæ— éœ€resize")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿model.configä¸­çš„vocab_sizeå’Œpad/eos_token_idä¸tokenizerä¸€è‡´
                    self.base_model.config.vocab_size = tokenizer_vocab_size
                    self.base_model.config.pad_token_id = self.tokenizer.pad_token_id
                    self.base_model.config.eos_token_id = self.tokenizer.eos_token_id
                    logging.info(f"âœ… å·²æ›´æ–°base_model.config: vocab_size={tokenizer_vocab_size}, pad_token_id={self.tokenizer.pad_token_id}, eos_token_id={self.tokenizer.eos_token_id}")
                        
                except Exception as e:
                    logging.warning(f"âš ï¸ resize_token_embeddingsæ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸æ”¯æŒæˆ–å·²é‡åŒ–ï¼‰: {e}")
                    logging.warning(f"   å°†ä¿ç•™'é™åŸŸ+clamp'çš„ä¿é™©ç­–ç•¥")
                
                # åŠ è½½å·²è®­ç»ƒçš„LoRAé€‚é…å™¨
                self.model = PeftModel.from_pretrained(
                    self.base_model,
                    self.model_name,
                    torch_dtype=self.torch_dtype
                )
                logging.info(f"æˆåŠŸåŠ è½½å·²è®­ç»ƒçš„LoRAé€‚é…å™¨ä»: {self.model_name}")
                
                # ğŸ”¥ å…³é”®ï¼šåŠ è½½LoRAåå†æ¬¡éªŒè¯embeddingå¤§å°ï¼Œå¦‚æœä»ä¸åŒ¹é…åˆ™å†æ¬¡resize
                try:
                    final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                    final_output_emb_size = None
                    if hasattr(self.model, 'get_output_embeddings') and self.model.get_output_embeddings() is not None:
                        final_output_emb_size = self.model.get_output_embeddings().weight.size(0)
                    logging.info(f"ğŸ“Š Embeddingå¤§å°æ£€æŸ¥ï¼ˆåŠ è½½LoRAåï¼‰:")
                    logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                    logging.info(f"   model input_embeddings.size(0): {final_input_emb_size}")
                    if final_output_emb_size is not None:
                        logging.info(f"   model output_embeddings.size(0): {final_output_emb_size}")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœåŠ è½½LoRAåembeddingå¤§å°ä¸åŒ¹é…ï¼Œå†æ¬¡resize
                    if final_input_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ åŠ è½½LoRAåï¼Œinput_embeddingså¤§å° ({final_input_emb_size}) != tokenizer ({tokenizer_vocab_size})")
                        logging.info(f"   æ­£åœ¨å†æ¬¡resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                        try:
                            self.model.resize_token_embeddings(tokenizer_vocab_size)
                            # éªŒè¯resizeæ˜¯å¦æˆåŠŸ
                            new_final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                            if new_final_input_emb_size == tokenizer_vocab_size:
                                logging.info(f"âœ… LoRAåresizeæˆåŠŸ: input_embeddings = {new_final_input_emb_size}")
                            else:
                                logging.error(f"âŒ LoRAåresizeå¤±è´¥: {new_final_input_emb_size} != {tokenizer_vocab_size}")
                        except Exception as e:
                            logging.warning(f"âš ï¸ LoRAåresizeå¤±è´¥: {e}")
                            logging.warning(f"   å°†ä½¿ç”¨'é™åŸŸ+clamp'ç­–ç•¥")
                    
                    # ğŸ”¥ å…³é”®ï¼šæ£€æŸ¥å¹¶resize output embeddings (lm_head)
                    if final_output_emb_size is not None and final_output_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ output_embeddingså¤§å° ({final_output_emb_size}) != tokenizer ({tokenizer_vocab_size})")
                        # resize_token_embeddingsåº”è¯¥åŒæ—¶resize inputå’Œoutputï¼Œä½†å¦‚æœå¤±è´¥ï¼Œæ‰‹åŠ¨æ£€æŸ¥
                        try:
                            # æ£€æŸ¥resizeåæ˜¯å¦å·²ä¿®å¤
                            check_output_emb_size = self.model.get_output_embeddings().weight.size(0)
                            if check_output_emb_size != tokenizer_vocab_size:
                                logging.warning(f"   output_embeddingsä»ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨'é™åŸŸ+clamp'ç­–ç•¥")
                        except:
                            pass
                except Exception as e:
                    logging.warning(f"âš ï¸ æ— æ³•æ£€æŸ¥LoRAåçš„embeddingå¤§å°: {e}")
                
            else:
                # æƒ…å†µ2: åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨æ–°çš„LoRAé…ç½®
                # å…ˆåŠ è½½åˆ†è¯å™¨
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    padding_side="left"
                )
                
                # è®¾ç½®pad token
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨ device_mapï¼Œå¿…é¡»è®¾ç½® low_cpu_mem_usage=True
                # RL è®­ç»ƒéœ€è¦æ¨¡å‹å®Œå…¨åœ¨ GPU ä¸Šï¼Œä¸èƒ½ä½¿ç”¨ offload
                device_map_for_load = None if self.device == "auto" else self.device
                # âœ… ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ device_mapï¼Œå¿…é¡»è®¾ç½® low_cpu_mem_usage=True
                low_cpu_mem_usage = True if device_map_for_load is not None else False
                self.base_model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=self.torch_dtype,
                    device_map=device_map_for_load,  # å¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨device_map
                    trust_remote_code=True,
                    low_cpu_mem_usage=low_cpu_mem_usage  # âœ… ä¿®å¤ï¼šä½¿ç”¨device_mapæ—¶å¿…é¡»ä¸ºTrue
                )
                
                # å¦‚æœ device_map æ˜¯ Noneï¼Œéœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
                if device_map_for_load is None and torch.cuda.is_available():
                    self.base_model = self.base_model.to(torch.device("cuda:0"))
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆåŸºåº§ã€å†LoRA - resize_token_embeddingså¿…é¡»åœ¨åŠ è½½LoRAä¹‹å‰æ‰§è¡Œ
                tokenizer_vocab_size = len(self.tokenizer)
                try:
                    # è·å–åŸºç¡€æ¨¡å‹çš„çœŸå®embeddingå¤§å°
                    input_emb_size = self.base_model.get_input_embeddings().weight.size(0)
                    output_emb_size = None
                    if hasattr(self.base_model, 'get_output_embeddings') and self.base_model.get_output_embeddings() is not None:
                        output_emb_size = self.base_model.get_output_embeddings().weight.size(0)
                    
                    logging.info(f"ğŸ“Š Embeddingå¤§å°æ£€æŸ¥ï¼ˆåŠ è½½LoRAå‰ï¼‰:")
                    logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                    logging.info(f"   model input_embeddings.size(0): {input_emb_size}")
                    if output_emb_size is not None:
                        logging.info(f"   model output_embeddings.size(0): {output_emb_size}")
                    logging.info(f"   model.config.vocab_size: {getattr(self.base_model.config, 'vocab_size', 'N/A')}")
                    
                    # å¦‚æœembeddingå¤§å°ä¸tokenizerä¸åŒ¹é…ï¼Œæ‰§è¡Œresize
                    if input_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ æ¨¡å‹embeddingå¤§å° ({input_emb_size}) != tokenizerå¤§å° ({tokenizer_vocab_size})")
                        logging.info(f"   æ­£åœ¨resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                        self.base_model.resize_token_embeddings(tokenizer_vocab_size)
                        logging.info(f"âœ… resize_token_embeddingså®Œæˆ")
                        
                        # éªŒè¯resizeæ˜¯å¦æˆåŠŸ
                        new_input_emb_size = self.base_model.get_input_embeddings().weight.size(0)
                        if new_input_emb_size != tokenizer_vocab_size:
                            logging.error(f"âŒ resize_token_embeddingså¤±è´¥ï¼æ–°å¤§å°: {new_input_emb_size} != {tokenizer_vocab_size}")
                            logging.warning(f"   å°†ä¿ç•™'é™åŸŸ+clamp'çš„ä¿é™©ç­–ç•¥")
                        else:
                            logging.info(f"âœ… resizeæˆåŠŸéªŒè¯: input_embeddings.size(0) = {new_input_emb_size}")
                    else:
                        logging.info(f"âœ… embeddingå¤§å°ä¸tokenizeråŒ¹é…ï¼Œæ— éœ€resize")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿model.configä¸­çš„vocab_sizeå’Œpad/eos_token_idä¸tokenizerä¸€è‡´
                    self.base_model.config.vocab_size = tokenizer_vocab_size
                    self.base_model.config.pad_token_id = self.tokenizer.pad_token_id
                    self.base_model.config.eos_token_id = self.tokenizer.eos_token_id
                    logging.info(f"âœ… å·²æ›´æ–°base_model.config: vocab_size={tokenizer_vocab_size}, pad_token_id={self.tokenizer.pad_token_id}, eos_token_id={self.tokenizer.eos_token_id}")
                        
                except Exception as e:
                    logging.warning(f"âš ï¸ resize_token_embeddingsæ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸æ”¯æŒæˆ–å·²é‡åŒ–ï¼‰: {e}")
                    logging.warning(f"   å°†ä¿ç•™'é™åŸŸ+clamp'çš„ä¿é™©ç­–ç•¥")
                
                # åº”ç”¨LoRA
                if self.use_lora:
                    peft_config = LoraConfig(**self.lora_config)
                    self.model = get_peft_model(self.base_model, peft_config)
                    logging.info("åº”ç”¨æ–°çš„LoRAé…ç½®")
                else:
                    self.model = self.base_model
                
                # ğŸ”¥ å…³é”®ï¼šåº”ç”¨LoRAåå†æ¬¡éªŒè¯embeddingå¤§å°ï¼Œå¦‚æœä»ä¸åŒ¹é…åˆ™å†æ¬¡resize
                try:
                    final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                    final_output_emb_size = None
                    if hasattr(self.model, 'get_output_embeddings') and self.model.get_output_embeddings() is not None:
                        final_output_emb_size = self.model.get_output_embeddings().weight.size(0)
                    logging.info(f"ğŸ“Š Embeddingå¤§å°æ£€æŸ¥ï¼ˆåŠ è½½LoRAåï¼‰:")
                    logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                    logging.info(f"   model input_embeddings.size(0): {final_input_emb_size}")
                    if final_output_emb_size is not None:
                        logging.info(f"   model output_embeddings.size(0): {final_output_emb_size}")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœåº”ç”¨LoRAåembeddingå¤§å°ä¸åŒ¹é…ï¼Œå†æ¬¡resize
                    if final_input_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ åº”ç”¨LoRAåï¼Œinput_embeddingså¤§å° ({final_input_emb_size}) != tokenizer ({tokenizer_vocab_size})")
                        logging.info(f"   æ­£åœ¨å†æ¬¡resize_token_embeddingsåˆ° {tokenizer_vocab_size}...")
                        try:
                            self.model.resize_token_embeddings(tokenizer_vocab_size)
                            # éªŒè¯resizeæ˜¯å¦æˆåŠŸ
                            new_final_input_emb_size = self.model.get_input_embeddings().weight.size(0)
                            if new_final_input_emb_size == tokenizer_vocab_size:
                                logging.info(f"âœ… LoRAåresizeæˆåŠŸ: input_embeddings = {new_final_input_emb_size}")
                            else:
                                logging.error(f"âŒ LoRAåresizeå¤±è´¥: {new_final_input_emb_size} != {tokenizer_vocab_size}")
                        except Exception as e:
                            logging.warning(f"âš ï¸ LoRAåresizeå¤±è´¥: {e}")
                            logging.warning(f"   å°†ä½¿ç”¨'é™åŸŸ+clamp'ç­–ç•¥")
                    
                    # ğŸ”¥ å…³é”®ï¼šæ£€æŸ¥å¹¶resize output embeddings (lm_head)
                    if final_output_emb_size is not None and final_output_emb_size != tokenizer_vocab_size:
                        logging.warning(f"âš ï¸ output_embeddingså¤§å° ({final_output_emb_size}) != tokenizer ({tokenizer_vocab_size})")
                        # resize_token_embeddingsåº”è¯¥åŒæ—¶resize inputå’Œoutputï¼Œä½†å¦‚æœå¤±è´¥ï¼Œæ‰‹åŠ¨æ£€æŸ¥
                        try:
                            # æ£€æŸ¥resizeåæ˜¯å¦å·²ä¿®å¤
                            check_output_emb_size = self.model.get_output_embeddings().weight.size(0)
                            if check_output_emb_size != tokenizer_vocab_size:
                                logging.warning(f"   output_embeddingsä»ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨'é™åŸŸ+clamp'ç­–ç•¥")
                        except:
                            pass
                except Exception as e:
                    logging.warning(f"âš ï¸ æ— æ³•æ£€æŸ¥LoRAåçš„embeddingå¤§å°: {e}")
            
            logging.info("Student model loaded successfully")
            
        except Exception as e:
            logging.error(f"Student model loading failed: {e}")
            raise
    
    def setup_for_ppo(self) -> AutoModelForCausalLMWithValueHead:
        """
        è®¾ç½®æ¨¡å‹ç”¨äºPPOè®­ç»ƒ
        
        Returns:
            å¸¦ä»·å€¼å¤´çš„æ¨¡å‹
        """
        try:
            # ValueHead æ¨¡å‹ä¸æ”¯æŒ CPU/ç£ç›˜å¸è½½ï¼Œéœ€è¦ç¡®ä¿æ¨¡å‹å®Œå…¨åœ¨GPUä¸Š
            # æ£€æŸ¥åŸºç¡€æ¨¡å‹æ˜¯å¦ä½¿ç”¨äº† device_map="auto"ï¼ˆå¯èƒ½éƒ¨åˆ†å±‚è¢«å¸è½½ï¼‰
            model_to_check = self.base_model if hasattr(self, 'base_model') else self.model
            has_device_map = False
            
            # æ£€æŸ¥è®¾å¤‡æ˜ å°„ï¼ˆå¯èƒ½åœ¨ä¸åŒä½ç½®ï¼‰
            if hasattr(model_to_check, 'hf_device_map') and model_to_check.hf_device_map:
                has_device_map = True
            elif hasattr(model_to_check, 'device_map') and model_to_check.device_map:
                has_device_map = True
            
            if has_device_map:
                logging.warning("æ£€æµ‹åˆ°æ¨¡å‹ä½¿ç”¨äº†è®¾å¤‡æ˜ å°„ï¼Œéœ€è¦å°†æ‰€æœ‰å±‚ç§»åŠ¨åˆ°å•ä¸€è®¾å¤‡ä»¥é¿å…ValueHeadä¸æ”¯æŒå¸è½½çš„é—®é¢˜")
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªGPUè®¾å¤‡
                target_device = None
                if torch.cuda.is_available():
                    target_device = torch.device("cuda:0")
                    logging.info(f"å°†æ‰€æœ‰æ¨¡å‹å‚æ•°ç§»åŠ¨åˆ°è®¾å¤‡: {target_device}")
                else:
                    target_device = torch.device("cpu")
                    logging.warning("æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨CPUï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨ meta device ä¸Š
                # å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°çš„ device ç±»å‹æ˜¯ metaï¼Œéœ€è¦ä½¿ç”¨ to_empty è€Œä¸æ˜¯ to
                try:
                    first_param = next(model_to_check.parameters())
                    is_meta_device = first_param.device.type == 'meta'
                except StopIteration:
                    is_meta_device = False
                
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°å•ä¸€è®¾å¤‡
                # å¯¹äº PEFT æ¨¡å‹ï¼Œéœ€è¦åŒæ—¶ç§»åŠ¨åŸºç¡€æ¨¡å‹å’Œ PEFT æ¨¡å‹
                if hasattr(self, 'base_model'):
                    # ç§»åŠ¨åŸºç¡€æ¨¡å‹
                    if is_meta_device:
                        logging.warning("æ£€æµ‹åˆ°æ¨¡å‹åœ¨ meta device ä¸Šï¼Œéœ€è¦å…ˆåŠ è½½æƒé‡")
                        # å¯¹äº meta deviceï¼Œåº”è¯¥é‡æ–°åŠ è½½è€Œä¸æ˜¯ç§»åŠ¨
                        # è¿™ç§æƒ…å†µé€šå¸¸ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨åŠ è½½æ—¶å…³é—­äº† low_cpu_mem_usage
                        raise RuntimeError("æ¨¡å‹åœ¨ meta device ä¸Šã€‚è¯·ç¡®ä¿åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨ low_cpu_mem_usage=False")
                    else:
                        if hasattr(self.base_model, 'to'):
                            self.base_model = self.base_model.to(target_device)
                    
                    # ç§»åŠ¨ PEFT æ¨¡å‹
                    if hasattr(self.model, 'to'):
                        self.model = self.model.to(target_device)
                else:
                    # æ™®é€šæ¨¡å‹
                    if is_meta_device:
                        logging.warning("æ£€æµ‹åˆ°æ¨¡å‹åœ¨ meta device ä¸Šï¼Œéœ€è¦å…ˆåŠ è½½æƒé‡")
                        raise RuntimeError("æ¨¡å‹åœ¨ meta device ä¸Šã€‚è¯·ç¡®ä¿åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨ low_cpu_mem_usage=False")
                    else:
                        if hasattr(self.model, 'to'):
                            self.model = self.model.to(target_device)
                
                # âœ… ä¿®å¤ï¼šä¸èƒ½å°† hf_device_map è®¾ç½®ä¸º Noneï¼Œå› ä¸º TRL åº“æœŸæœ›å®ƒæ˜¯å­—å…¸
                # åº”è¯¥å°†å…¶è®¾ç½®ä¸ºè¡¨ç¤ºæ‰€æœ‰å±‚éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šçš„å­—å…¸æ ¼å¼
                if hasattr(model_to_check, 'hf_device_map'):
                    # è·å–ç›®æ ‡è®¾å¤‡çš„å­—ç¬¦ä¸²è¡¨ç¤º
                    if isinstance(target_device, torch.device):
                        device_str = str(target_device)
                    else:
                        device_str = target_device
                    
                    # å¦‚æœ hf_device_map å­˜åœ¨ä¸”ä¸ä¸º Noneï¼Œå°†å…¶æ›´æ–°ä¸ºå•ä¸€è®¾å¤‡çš„æ˜ å°„
                    if model_to_check.hf_device_map is not None and isinstance(model_to_check.hf_device_map, dict):
                        # å°†æ‰€æœ‰è®¾å¤‡æ˜ å°„ç»Ÿä¸€åˆ°ç›®æ ‡è®¾å¤‡
                        model_to_check.hf_device_map = {name: device_str for name in model_to_check.hf_device_map.keys()}
                    else:
                        # å¦‚æœ hf_device_map æ˜¯ None æˆ–ä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬æ˜ å°„
                        # TRL åº“éœ€è¦å®ƒæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ‰€ä»¥è‡³å°‘æä¾›ä¸€ä¸ªé”®å€¼å¯¹
                        model_to_check.hf_device_map = {"model": device_str}
                
                # device_map å¯ä»¥è®¾ç½®ä¸º Noneï¼Œå› ä¸ºå®ƒä¸è¢« TRL åº“ä½¿ç”¨
                if hasattr(model_to_check, 'device_map'):
                    model_to_check.device_map = None
            
            # åˆ›å»ºå¸¦ä»·å€¼å¤´çš„æ¨¡å‹
            # æ³¨æ„ï¼šä¸ä½¿ç”¨ device_map="auto"ï¼Œå› ä¸º ValueHead ä¸æ”¯æŒå¸è½½
            # ç¡®ä¿æ¨¡å‹å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆä¸æ˜¯ meta deviceï¼‰
            
            # æ£€æŸ¥æ¨¡å‹å½“å‰è®¾å¤‡
            try:
                current_device = next(self.model.parameters()).device
                if current_device.type == 'meta':
                    raise RuntimeError("æ¨¡å‹ä»åœ¨ meta device ä¸Šã€‚è¯·ç¡®ä¿åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨ low_cpu_mem_usage=False")
            except StopIteration:
                # æ¨¡å‹æ²¡æœ‰å‚æ•°ï¼Œè¿™æ˜¯å¼‚å¸¸æƒ…å†µ
                raise RuntimeError("æ¨¡å‹æ²¡æœ‰å‚æ•°ï¼Œæ— æ³•ç¡®å®šè®¾å¤‡ä½ç½®")
            
            # å¦‚æœæ¨¡å‹åœ¨ CPU ä¸Šï¼Œç§»åŠ¨åˆ°åŸå§‹GPUè®¾å¤‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if torch.cuda.is_available() and current_device.type == 'cpu':
                # å°è¯•ä¿æŒæ¨¡å‹åœ¨åŸå§‹è®¾å¤‡ä¸Šï¼Œè€Œä¸æ˜¯å¼ºåˆ¶ç§»åŠ¨åˆ°cuda:0
                # æ£€æŸ¥æ¨¡å‹å‚æ•°çš„å®é™…è®¾å¤‡
                try:
                    # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡ä½œä¸ºç›®æ ‡è®¾å¤‡
                    for param in self.model.parameters():
                        if param.device.type == 'cuda':
                            target_device = param.device
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°GPUå‚æ•°ï¼Œä½¿ç”¨cuda:0
                        target_device = torch.device("cuda:0")
                except:
                    target_device = torch.device("cuda:0")
                
                logging.info(f"å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUè®¾å¤‡: {target_device}ï¼ˆä¿æŒåŸå§‹è®¾å¤‡ï¼‰")
                self.model = self.model.to(target_device)
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿åœ¨åˆ›å»º ValueHead ä¹‹å‰ï¼Œhf_device_map å­˜åœ¨ä¸”æ˜¯å­—å…¸æ ¼å¼
            # TRL åº“çš„ post_init ä¼šæ£€æŸ¥ hf_device_map.values()ï¼Œä¸èƒ½æ˜¯ None
            if hasattr(self.model, 'hf_device_map'):
                if self.model.hf_device_map is None or not isinstance(self.model.hf_device_map, dict):
                    # è·å–å½“å‰è®¾å¤‡
                    try:
                        current_device_str = str(next(self.model.parameters()).device)
                    except:
                        current_device_str = "cuda:0" if torch.cuda.is_available() else "cpu"
                    # åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„è®¾å¤‡æ˜ å°„
                    self.model.hf_device_map = {"model": current_device_str}
            
            # åˆ›å»º ValueHead æ¨¡å‹
            # æ³¨æ„ï¼šfrom_pretrained çš„ç¬¬ä¸€ä¸ªå‚æ•°å¯ä»¥æ˜¯æ¨¡å‹å®ä¾‹æˆ–è·¯å¾„
            # è¿™é‡Œä¼ å…¥æ¨¡å‹å®ä¾‹ï¼Œç¡®ä¿ä½¿ç”¨å·²åŠ è½½çš„æƒé‡
            ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(
                self.model,  # ä¼ å…¥æ¨¡å‹å®ä¾‹ï¼Œä¸æ˜¯è·¯å¾„
                torch_dtype=self.torch_dtype,
                device_map=None  # ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œé¿å…å¸è½½
            )
            
            # âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆæ€»æ˜¯å¯ç”¨ä»¥æœ€å¤§åŒ–æ˜¾å­˜èŠ‚çœï¼‰ï¼šç‰ºç‰²è®­ç»ƒé€Ÿåº¦æ¢å–æ˜¾å­˜
            # æ¢¯åº¦æ£€æŸ¥ç‚¹å¯ä»¥èŠ‚çœ30-40%çš„æ¿€æ´»æ˜¾å­˜ï¼Œå¯¹äºlog_softmax OOMç‰¹åˆ«æœ‰æ•ˆ
            # æ³¨æ„ï¼šé…ç½®åœ¨rl_trainerä¸­ä¼šå†æ¬¡æ£€æŸ¥ï¼Œè¿™é‡Œæ€»æ˜¯å¯ç”¨ä»¥æœ€å¤§åŒ–æ˜¾å­˜èŠ‚çœ
            try:
                # å°è¯•åœ¨ppo_modelä¸Šå¯ç”¨
                if hasattr(ppo_model, 'gradient_checkpointing_enable'):
                    ppo_model.gradient_checkpointing_enable()
                    logging.info("âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ¿€æ´»æ˜¾å­˜ï¼‰")
                # å°è¯•åœ¨pretrained_modelä¸Šå¯ç”¨ï¼ˆAutoModelForCausalLMWithValueHeadçš„ç»“æ„ï¼‰
                elif hasattr(ppo_model, 'pretrained_model') and hasattr(ppo_model.pretrained_model, 'gradient_checkpointing_enable'):
                    ppo_model.pretrained_model.gradient_checkpointing_enable()
                    logging.info("âœ… å·²åœ¨pretrained_modelä¸Šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ¿€æ´»æ˜¾å­˜ï¼‰")
                # å°è¯•åœ¨åŸºç¡€æ¨¡å‹ä¸Šå¯ç”¨
                elif hasattr(ppo_model, 'base_model') and hasattr(ppo_model.base_model, 'gradient_checkpointing_enable'):
                    ppo_model.base_model.gradient_checkpointing_enable()
                    logging.info("âœ… å·²åœ¨base_modelä¸Šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ¿€æ´»æ˜¾å­˜ï¼‰")
            except Exception as e:
                logging.warning(f"âš ï¸ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¤±è´¥ï¼ˆå¯èƒ½ä¸è¢«æ”¯æŒï¼‰: {e}")
            
            logging.info("PPO model setup completed")
            return ppo_model
            
        except Exception as e:
            logging.error(f"PPO model setup failed: {e}")
            raise
    
    def generate(self, prompts: Union[str, List[str]], 
                max_length: int = 256,
                temperature: float = 0.7,
                do_sample: bool = True,
                top_p: float = 0.9,
                top_k: int = 50) -> Union[str, List[str]]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompts: æç¤ºæ–‡æœ¬æˆ–åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            top_p: top_på‚æ•°
            top_k: top_kå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if isinstance(prompts, str):
            prompts = [prompts]
        
        self.model.eval()
        
        max_retries = 2
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                with torch.no_grad():
                    # åˆ†è¯
                    inputs = self.tokenizer(
                        prompts,
                        return_tensors="pt",
                        truncation=True,
                        max_length=512,
                        padding=True
                    )
                    
                    # ğŸ” è¯¦ç»†è¯Šæ–­ï¼šéªŒè¯token IDèŒƒå›´ï¼Œé˜²æ­¢CUDAç´¢å¼•è¶Šç•Œ
                    # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨tokenizerçš„vocab_sizeä½œä¸ºé™åˆ¶ï¼ˆæ›´ä¸¥æ ¼ï¼‰
                    # å› ä¸ºç”Ÿæˆåéœ€è¦ç”¨tokenizerè§£ç ï¼Œå¿…é¡»ç¡®ä¿ç”Ÿæˆçš„token IDåœ¨tokenizerèŒƒå›´å†…
                    model_vocab_size = getattr(self.model.config, 'vocab_size', None)
                    tokenizer_vocab_size = len(self.tokenizer)
                    
                    # ğŸ”¥ ä½¿ç”¨è¾ƒå°çš„vocab_sizeï¼ˆtokenizerçš„å®é™…èŒƒå›´ï¼‰ï¼Œé˜²æ­¢ç”Ÿæˆæ— æ•ˆtoken
                    if model_vocab_size is not None and tokenizer_vocab_size is not None:
                        if model_vocab_size > tokenizer_vocab_size:
                            # ğŸ” å…³é”®é—®é¢˜ï¼šæ¨¡å‹å’Œtokenizerè¯æ±‡è¡¨ä¸åŒ¹é…
                            vocab_diff = model_vocab_size - tokenizer_vocab_size
                            if retry_count == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡è®°å½•
                                logging.warning(f"âš ï¸ æ¨¡å‹vocab_size ({model_vocab_size}) > tokenizer vocab_size ({tokenizer_vocab_size})")
                                logging.warning(f"   å·®å¼‚: {vocab_diff} ä¸ªtokenï¼Œè¿™å¯èƒ½å¯¼è‡´ç”Ÿæˆæ— æ•ˆtoken")
                                logging.warning(f"   å°†ä½¿ç”¨tokenizerèŒƒå›´ ({tokenizer_vocab_size}) ä½œä¸ºé™åˆ¶")
                            vocab_size = tokenizer_vocab_size  # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é™åˆ¶
                        else:
                            vocab_size = min(model_vocab_size, tokenizer_vocab_size)
                    else:
                        vocab_size = tokenizer_vocab_size if tokenizer_vocab_size is not None else (model_vocab_size if model_vocab_size is not None else 50000)
                    
                    # ğŸ” è®°å½•æœ€ç»ˆä½¿ç”¨çš„vocab_size
                    if retry_count == 0 and model_vocab_size is not None and tokenizer_vocab_size is not None:
                        if model_vocab_size != tokenizer_vocab_size:
                            logging.debug(f"ğŸ“Š Vocabå¤§å°æ£€æŸ¥: æ¨¡å‹={model_vocab_size}, tokenizer={tokenizer_vocab_size}, ä½¿ç”¨={vocab_size}")
                    
                    if 'input_ids' in inputs:
                        input_ids = inputs['input_ids']
                        # ğŸ” è¯¦ç»†æ£€æŸ¥ï¼šæ‰“å°è¾“å…¥ä¿¡æ¯
                        max_token_id = input_ids.max().item()
                        min_token_id = input_ids.min().item()
                        input_len = input_ids.shape[1]
                        
                        # æ£€æŸ¥æ‰€æœ‰token IDæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        invalid_mask = (input_ids >= vocab_size) | (input_ids < 0)
                        if torch.any(invalid_mask):
                            invalid_ids = input_ids[invalid_mask].unique().tolist()
                            logging.error(f"âŒ æ£€æµ‹åˆ°æ— æ•ˆtoken ID!")
                            logging.error(f"   æ— æ•ˆIDåˆ—è¡¨: {invalid_ids[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
                            logging.error(f"   è¾“å…¥åºåˆ—é•¿åº¦: {input_len}")
                            logging.error(f"   æœ€å¤§token ID: {max_token_id}, æœ€å°: {min_token_id}")
                            logging.error(f"   æ¨¡å‹vocab_size: {model_vocab_size}")
                            logging.error(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
                            logging.error(f"   ä½¿ç”¨çš„vocab_size: {vocab_size}")
                            logging.error(f"   å°†æˆªæ–­åˆ°æœ‰æ•ˆèŒƒå›´ [0, {vocab_size-1}]")
                            inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                        
                        # ğŸ” æ£€æŸ¥è¾“å…¥é•¿åº¦æ˜¯å¦è¶…å‡ºæ¨¡å‹é™åˆ¶
                        max_position_embeddings = getattr(self.model.config, 'max_position_embeddings', None)
                        if max_position_embeddings and input_len > max_position_embeddings:
                            logging.error(f"âŒ è¾“å…¥åºåˆ—é•¿åº¦ {input_len} è¶…å‡ºæ¨¡å‹æœ€å¤§ä½ç½® {max_position_embeddings}!")
                            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
                            inputs['input_ids'] = input_ids[:, :max_position_embeddings]
                            if 'attention_mask' in inputs:
                                inputs['attention_mask'] = inputs['attention_mask'][:, :max_position_embeddings]
                            logging.warning(f"   å·²æˆªæ–­åˆ° {max_position_embeddings}")
                    
                    # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨device_mapï¼Œå¦‚æœæœ‰åˆ™ä¸å¼ºåˆ¶ç§»åŠ¨
                    # å¯¹äºdevice_map="auto"çš„æ¨¡å‹ï¼Œè®©HFè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
                    has_device_map = (hasattr(self.model, 'hf_device_map') and self.model.hf_device_map) or \
                                   (hasattr(self.model, 'device_map') and self.model.device_map)
                    if not has_device_map and hasattr(self.model, 'device'):
                        # åªæœ‰åœ¨æ²¡æœ‰device_mapä¸”æ¨¡å‹æœ‰å•ä¸€deviceæ—¶æ‰ç§»åŠ¨
                        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    # å¦åˆ™è®©HFè‡ªåŠ¨å¤„ç†
                    
                    # ğŸ” è®¾ç½®æœ‰æ•ˆçš„pad_token_idå’Œeos_token_id
                    pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
                    eos_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else pad_token_id
                    
                    # ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if pad_token_id is not None:
                        pad_token_id = min(pad_token_id, vocab_size - 1)
                    if eos_token_id is not None:
                        eos_token_id = min(eos_token_id, vocab_size - 1)
                    
                    # ğŸ” ç”Ÿæˆå‰ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®
                    max_positions = getattr(self.model.config, 'max_position_embeddings', None)
                    max_total_len = max_positions if max_positions else 2048
                    current_len = inputs['input_ids'].shape[1]
                    max_allowed_new_tokens = min(max_length, max_total_len - current_len - 10)  # ç•™10ä¸ªtokençš„å®‰å…¨ä½™é‡
                    
                    if max_allowed_new_tokens <= 0:
                        logging.error(f"âŒ æ— æ³•ç”Ÿæˆæ–°token: å½“å‰é•¿åº¦ {current_len} + é¢„ç•™ {10} >= æœ€å¤§é•¿åº¦ {max_total_len}")
                        return [""] * len(prompts) if len(prompts) > 1 else ""
                    
                    # ğŸ” è¯¦ç»†çš„ç”Ÿæˆå‚æ•°æ—¥å¿—ï¼ˆä»…åœ¨é”™è¯¯æ—¶ï¼‰
                    if retry_count > 0:
                        logging.info(f"ğŸ” ç”Ÿæˆå‚æ•°: max_new_tokens={max_allowed_new_tokens}, current_len={current_len}, max_total={max_total_len}")
                        logging.info(f"   pad_token_id={pad_token_id}, eos_token_id={eos_token_id}, vocab_size={vocab_size}")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ LogitsProcessoræ¥å¼ºåˆ¶æ¨¡å‹åªç”ŸæˆtokenizerèŒƒå›´å†…çš„token
                    from transformers import LogitsProcessorList
                    
                    class TokenRangeLogitsProcessor:
                        """å¼ºåˆ¶æ¨¡å‹åªç”Ÿæˆæœ‰æ•ˆèŒƒå›´å†…çš„tokenï¼ˆä¸¥æ ¼é™åˆ¶åˆ°tokenizerèŒƒå›´ï¼‰"""
                        def __init__(self, max_valid_token_id: int):
                            self.max_valid_token_id = max_valid_token_id
                            self.vocab_end_idx = max_valid_token_id + 1
                        
                        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
                            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªmaskï¼Œä¸åˆ‡ç‰‡ï¼ˆä¿æŒå½¢çŠ¶ä¸å˜ï¼‰
                            # Transformersçº¦å®šlogitsçš„æœ€åä¸€ç»´å½¢çŠ¶å¿…é¡»ä¿æŒä¸å˜
                            if scores.shape[-1] > self.vocab_end_idx:
                                scores[..., self.vocab_end_idx:] = float('-inf')
                            return scores
                    
                    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨tokenizerçš„å®é™…æœ€å¤§token IDï¼ˆvocab_size - 1ï¼‰
                    # è¿™æ˜¯å”¯ä¸€çš„å®‰å…¨èŒƒå›´ï¼Œå› ä¸ºtokenizeræ— æ³•è§£ç è¶…å‡ºæ­¤èŒƒå›´çš„token
                    max_valid_token_id = vocab_size - 1
                    logits_processor = LogitsProcessorList([
                        TokenRangeLogitsProcessor(max_valid_token_id)
                    ])
                    
                    # ğŸ” éªŒè¯LogitsProcessorè®¾ç½®
                    if retry_count > 0:
                        logging.info(f"   LogitsProcessor: æœ€å¤§æœ‰æ•ˆtoken ID = {max_valid_token_id} (vocab_size={vocab_size})")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒ…è£…æ¨¡å‹çš„forwardæ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰input_idséƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    # è¿™é˜²æ­¢åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„å†…éƒ¨æ“ä½œè®¿é—®è¶…å‡ºtokenizerèŒƒå›´çš„embedding
                    original_forward = None
                    original_model = self.model
                    
                    # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯PeftModelåŒ…è£…çš„åŸºç¡€æ¨¡å‹ï¼‰
                    model_to_wrap = self.model
                    if hasattr(self.model, 'get_base_model'):
                        model_to_wrap = self.model.get_base_model()
                    elif hasattr(self.model, 'base_model'):
                        model_to_wrap = self.model.base_model.model if hasattr(self.model.base_model, 'model') else self.model.base_model
                    
                    # åŒ…è£…embeddingå±‚ï¼Œç¡®ä¿æ‰€æœ‰token IDéƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    def create_safe_embedding_wrapper(embedding_layer, max_valid_token_id, layer_name="embedding"):
                        """åˆ›å»ºå®‰å…¨çš„embeddingåŒ…è£…å™¨"""
                        original_embed = embedding_layer.forward
                        
                        # è·å–embeddingå±‚çš„å®é™…å¤§å°
                        try:
                            actual_emb_size = embedding_layer.weight.size(0)
                        except:
                            actual_emb_size = None
                        
                        def safe_forward(input_ids, *args, **kwargs):
                            # ğŸ”¥ å…³é”®ï¼šåœ¨embeddingæŸ¥è¡¨å‰ï¼Œå°†æ‰€æœ‰token IDé™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´
                            if input_ids is not None and isinstance(input_ids, torch.Tensor):
                                # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºå®é™…embeddingå¤§å°çš„token
                                if actual_emb_size is not None:
                                    max_id_in_input = input_ids.max().item() if input_ids.numel() > 0 else -1
                                    if max_id_in_input >= actual_emb_size:
                                        if not hasattr(safe_forward, '_warned'):
                                            logging.error(f"âŒ {layer_name}: input_idsåŒ…å«è¶…å‡ºembeddingå¤§å°çš„token! max={max_id_in_input}, embedding_size={actual_emb_size}, é™åˆ¶åˆ°={max_valid_token_id}")
                                            safe_forward._warned = True
                                        # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é™åˆ¶ï¼šmin(å®é™…embeddingå¤§å°, tokenizerå¤§å°)
                                        safe_max = min(actual_emb_size - 1, max_valid_token_id)
                                        input_ids = torch.clamp(input_ids, 0, safe_max)
                                    else:
                                        # å³ä½¿æ²¡æœ‰è¶…å‡ºï¼Œä¹Ÿé™åˆ¶åˆ°tokenizerèŒƒå›´
                                        input_ids = torch.clamp(input_ids, 0, max_valid_token_id)
                                else:
                                    # å¦‚æœæ— æ³•è·å–å®é™…å¤§å°ï¼Œä½¿ç”¨tokenizerèŒƒå›´
                                    input_ids = torch.clamp(input_ids, 0, max_valid_token_id)
                            return original_embed(input_ids, *args, **kwargs)
                        
                        embedding_layer.forward = safe_forward
                        return original_embed
                    
                    # åŒ…è£…embeddingå±‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„æ‰¾åˆ°embeddingå±‚
                    restored_embeddings = []
                    embedding_layers_to_wrap = []
                    
                    # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„embeddingå±‚è·¯å¾„
                    if hasattr(model_to_wrap, 'embed_tokens'):
                        embedding_layers_to_wrap.append(('embed_tokens', model_to_wrap.embed_tokens))
                    elif hasattr(model_to_wrap, 'model') and hasattr(model_to_wrap.model, 'embed_tokens'):
                        embedding_layers_to_wrap.append(('model.embed_tokens', model_to_wrap.model.embed_tokens))
                    elif hasattr(model_to_wrap, 'wte'):  # GPTé£æ ¼æ¨¡å‹
                        embedding_layers_to_wrap.append(('wte', model_to_wrap.wte))
                    
                    for layer_name, embedding_layer in embedding_layers_to_wrap:
                        original_embed = create_safe_embedding_wrapper(
                            embedding_layer, 
                            vocab_size - 1,
                            layer_name
                        )
                        restored_embeddings.append((embedding_layer, original_embed, layer_name))
                        if retry_count == 0:
                            logging.debug(f"âœ… å·²åŒ…è£…embeddingå±‚: {layer_name}ï¼Œé™åˆ¶èŒƒå›´: [0, {vocab_size - 1}]")
                    
                    try:
                        # ğŸ”¥ å…³é”®ï¼šç¡®ä¿æ¨¡å‹ä½¿ç”¨tokenizerçš„vocab_sizeï¼Œè€Œä¸æ˜¯æ¨¡å‹è‡ªå·±çš„vocab_size
                        # é€šè¿‡é™åˆ¶logitsç»´åº¦å¹¶åœ¨ç”Ÿæˆå‚æ•°ä¸­æ˜¾å¼æŒ‡å®švocab_size
                        generate_kwargs = {
                            **inputs,
                            "max_new_tokens": max_allowed_new_tokens,
                            "temperature": temperature,
                            "do_sample": do_sample,
                            "top_p": top_p,
                            "top_k": min(top_k, vocab_size - 1),  # ç¡®ä¿top_kä¸è¶…è¿‡tokenizerèŒƒå›´
                            "pad_token_id": pad_token_id,
                            "eos_token_id": eos_token_id,
                            "repetition_penalty": 1.1,
                            "logits_processor": logits_processor,  # ğŸ”¥ å…³é”®ï¼šå¼ºåˆ¶ä½¿ç”¨tokenizerèŒƒå›´
                            "use_cache": True
                        }
                        
                        # ç”Ÿæˆ
                        outputs = self.model.generate(**generate_kwargs)
                    finally:
                        # æ¢å¤åŸå§‹çš„embedding forwardæ–¹æ³•
                        for embedding_layer, original_embed, layer_name in restored_embeddings:
                            embedding_layer.forward = original_embed
                    
                    # ğŸ” è¯¦ç»†éªŒè¯ç”Ÿæˆçš„token ID
                    invalid_mask = (outputs >= vocab_size) | (outputs < 0)
                    if torch.any(invalid_mask):
                        invalid_ids = outputs[invalid_mask].unique().tolist()
                        output_max = outputs.max().item()
                        output_min = outputs.min().item()
                        logging.error(f"âŒ ç”Ÿæˆçš„token IDè¶…å‡ºèŒƒå›´!")
                        logging.error(f"   æ— æ•ˆIDåˆ—è¡¨: {invalid_ids[:10]}")
                        logging.error(f"   è¾“å‡ºtokenèŒƒå›´: [{output_min}, {output_max}], vocab_size={vocab_size}")
                        logging.error(f"   è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
                        logging.error(f"   å°†æˆªæ–­åˆ°æœ‰æ•ˆèŒƒå›´")
                        outputs = torch.clamp(outputs, 0, vocab_size - 1)
                    
                    # è§£ç 
                    generated_texts = []
                    for i, output in enumerate(outputs):
                        try:
                            input_length = inputs['input_ids'][i].shape[0]
                            if len(output) > input_length:
                                generated_text = self.tokenizer.decode(
                                    output[input_length:],
                                    skip_special_tokens=True
                                )
                            else:
                                generated_text = ""
                            generated_texts.append(generated_text)
                        except Exception as e:
                            logging.warning(f"è§£ç å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")
                            generated_texts.append("")
                    
                return generated_texts if len(generated_texts) > 1 else generated_texts[0]
                
            except RuntimeError as e:
                error_str = str(e)
                if "device-side assert" in error_str or "CUDA error" in error_str:
                    retry_count += 1
                    # ğŸ” è¯¦ç»†é”™è¯¯è¯Šæ–­
                    logging.error(f"âŒ CUDA device-side asserté”™è¯¯ (é‡è¯• {retry_count}/{max_retries})")
                    logging.error(f"   é”™è¯¯ä¿¡æ¯: {error_str[:500]}")  # åªæ˜¾ç¤ºå‰500å­—ç¬¦
                    
                    # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥æ¨¡å‹å’Œè¾“å…¥çŠ¶æ€
                    try:
                        if 'inputs' in locals():
                            logging.error(f"   è¾“å…¥å½¢çŠ¶: {inputs.get('input_ids', 'N/A').shape if 'input_ids' in inputs else 'N/A'}")
                            if 'input_ids' in inputs:
                                input_ids = inputs['input_ids']
                                logging.error(f"   è¾“å…¥tokenèŒƒå›´: [{input_ids.min().item()}, {input_ids.max().item()}]")
                                logging.error(f"   è¾“å…¥åºåˆ—é•¿åº¦: {input_ids.shape[1]}")
                        
                        model_vocab = getattr(self.model.config, 'vocab_size', None)
                        max_pos = getattr(self.model.config, 'max_position_embeddings', None)
                        logging.error(f"   æ¨¡å‹vocab_size: {model_vocab}")
                        logging.error(f"   æ¨¡å‹max_position_embeddings: {max_pos}")
                        logging.error(f"   tokenizer vocab_size: {len(self.tokenizer)}")
                        logging.error(f"   æç¤ºæ•°é‡: {len(prompts)}")
                    except Exception as diag_e:
                        logging.error(f"   è¯Šæ–­ä¿¡æ¯è·å–å¤±è´¥: {diag_e}")
                    
                    if retry_count < max_retries:
                        logging.warning(f"   æ¸…ç†CUDAç¼“å­˜å¹¶é‡è¯•...")
                        # æ¸…ç†CUDAç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                        # é‡ç½®æ¨¡å‹çŠ¶æ€
                        self.model.eval()
                        import time
                        time.sleep(0.1 * retry_count)
                    else:
                        logging.error(f"âŒ CUDAé”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        logging.error(f"   å»ºè®®: æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦æŸåï¼Œæˆ–å°è¯•é‡æ–°åŠ è½½æ¨¡å‹")
                        # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                        return [""] * len(prompts) if len(prompts) > 1 else ""
                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    raise
            except Exception as e:
                logging.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {e}")
                # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                return [""] * len(prompts) if len(prompts) > 1 else ""
    
    def get_logits(self, text: Union[str, List[str]]) -> torch.Tensor:
        """
        è·å–æ–‡æœ¬çš„logits
        
        Args:
            text: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            logitså¼ é‡
        """
        if isinstance(text, str):
            text = [text]
        
        self.model.eval()
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·å–tokenizerçš„å®é™…vocab_sizeä½œä¸ºé™åˆ¶
        model_vocab_size = getattr(self.model.config, 'vocab_size', None)
        tokenizer_vocab_size = len(self.tokenizer)
        vocab_size = tokenizer_vocab_size if tokenizer_vocab_size is not None else (model_vocab_size if model_vocab_size is not None else 50000)
        if model_vocab_size is not None and tokenizer_vocab_size is not None:
            if model_vocab_size > tokenizer_vocab_size:
                vocab_size = tokenizer_vocab_size  # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é™åˆ¶
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒ…è£…embeddingå±‚ä»¥ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
        restored_embeddings = []
        model_to_wrap = self.model
        if hasattr(self.model, 'get_base_model'):
            model_to_wrap = self.model.get_base_model()
        elif hasattr(self.model, 'base_model'):
            if hasattr(self.model.base_model, 'model'):
                model_to_wrap = self.model.base_model.model
            else:
                model_to_wrap = self.model.base_model
        elif hasattr(self.model, 'pretrained_model'):
            model_to_wrap = self.model.pretrained_model
        
        def create_safe_embedding_wrapper(embedding_layer, max_valid_token_id):
            """åˆ›å»ºå®‰å…¨çš„embeddingåŒ…è£…å™¨"""
            original_embed = embedding_layer.forward
            
            def safe_forward(input_ids, *args, **kwargs):
                if input_ids is not None and isinstance(input_ids, torch.Tensor):
                    input_ids = torch.clamp(input_ids, 0, max_valid_token_id)
                return original_embed(input_ids, *args, **kwargs)
            
            embedding_layer.forward = safe_forward
            return original_embed
        
        embedding_layers_to_wrap = []
        if hasattr(model_to_wrap, 'embed_tokens'):
            embedding_layers_to_wrap.append(('embed_tokens', model_to_wrap.embed_tokens))
        elif hasattr(model_to_wrap, 'model') and hasattr(model_to_wrap.model, 'embed_tokens'):
            embedding_layers_to_wrap.append(('model.embed_tokens', model_to_wrap.model.embed_tokens))
        elif hasattr(model_to_wrap, 'wte'):
            embedding_layers_to_wrap.append(('wte', model_to_wrap.wte))
        
        for layer_name, embedding_layer in embedding_layers_to_wrap:
            original_embed = create_safe_embedding_wrapper(embedding_layer, vocab_size - 1)
            restored_embeddings.append((embedding_layer, original_embed, layer_name))
        
        try:
            with torch.no_grad():
                # åˆ†è¯
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=True
                )
                
                # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨device_mapï¼Œå¦‚æœæœ‰åˆ™ä¸å¼ºåˆ¶ç§»åŠ¨
                has_device_map = (hasattr(self.model, 'hf_device_map') and self.model.hf_device_map) or \
                               (hasattr(self.model, 'device_map') and self.model.device_map)
                if not has_device_map and hasattr(self.model, 'device'):
                    # åªæœ‰åœ¨æ²¡æœ‰device_mapä¸”æ¨¡å‹æœ‰å•ä¸€deviceæ—¶æ‰ç§»åŠ¨
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                # å¦åˆ™è®©HFè‡ªåŠ¨å¤„ç†
                
                # ğŸ”¥ å…³é”®ï¼šç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
                if 'input_ids' in inputs:
                    input_ids = inputs['input_ids']
                    invalid_mask = (input_ids >= vocab_size) | (input_ids < 0)
                    if torch.any(invalid_mask):
                        inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # ğŸ”¥ æ³¨æ„ï¼šä¸å†åˆ‡ç‰‡logitsï¼Œå› ä¸ºvocab_sizeå·²åŒ¹é…ï¼ˆå¦‚æœè¿˜æœ‰é—®é¢˜åº”è¯¥åœ¨åŠ è½½æ—¶æŠ¥é”™ï¼‰
                
                # å¦‚æœæ˜¯å•ä¸ªæ–‡æœ¬ï¼Œå»æ‰batchç»´åº¦ï¼ˆä¸TeacherModelä¸€è‡´ï¼‰
                if logits.shape[0] == 1:
                    return logits[0]
                
                return logits
        finally:
            # æ¢å¤åŸå§‹çš„embedding forwardæ–¹æ³•
            for embedding_layer, original_embed, layer_name in restored_embeddings:
                embedding_layer.forward = original_embed
    
    def compute_log_probs(self, text: str) -> torch.Tensor:
        """
        è®¡ç®—æ–‡æœ¬çš„å¯¹æ•°æ¦‚ç‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¯¹æ•°æ¦‚ç‡å¼ é‡
        """
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·å–tokenizerçš„å®é™…vocab_sizeä½œä¸ºé™åˆ¶
        model_vocab_size = getattr(self.model.config, 'vocab_size', None)
        tokenizer_vocab_size = len(self.tokenizer)
        vocab_size = tokenizer_vocab_size if tokenizer_vocab_size is not None else (model_vocab_size if model_vocab_size is not None else 50000)
        if model_vocab_size is not None and tokenizer_vocab_size is not None:
            if model_vocab_size > tokenizer_vocab_size:
                vocab_size = tokenizer_vocab_size  # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é™åˆ¶
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒ…è£…embeddingå±‚ä»¥ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
        restored_embeddings = []
        model_to_wrap = self.model
        if hasattr(self.model, 'get_base_model'):
            model_to_wrap = self.model.get_base_model()
        elif hasattr(self.model, 'base_model'):
            if hasattr(self.model.base_model, 'model'):
                model_to_wrap = self.model.base_model.model
            else:
                model_to_wrap = self.model.base_model
        elif hasattr(self.model, 'pretrained_model'):
            model_to_wrap = self.model.pretrained_model
        
        def create_safe_embedding_wrapper(embedding_layer, max_valid_token_id):
            """åˆ›å»ºå®‰å…¨çš„embeddingåŒ…è£…å™¨"""
            original_embed = embedding_layer.forward
            
            def safe_forward(input_ids, *args, **kwargs):
                if input_ids is not None and isinstance(input_ids, torch.Tensor):
                    input_ids = torch.clamp(input_ids, 0, max_valid_token_id)
                return original_embed(input_ids, *args, **kwargs)
            
            embedding_layer.forward = safe_forward
            return original_embed
        
        embedding_layers_to_wrap = []
        if hasattr(model_to_wrap, 'embed_tokens'):
            embedding_layers_to_wrap.append(('embed_tokens', model_to_wrap.embed_tokens))
        elif hasattr(model_to_wrap, 'model') and hasattr(model_to_wrap.model, 'embed_tokens'):
            embedding_layers_to_wrap.append(('model.embed_tokens', model_to_wrap.model.embed_tokens))
        elif hasattr(model_to_wrap, 'wte'):
            embedding_layers_to_wrap.append(('wte', model_to_wrap.wte))
        
        for layer_name, embedding_layer in embedding_layers_to_wrap:
            original_embed = create_safe_embedding_wrapper(embedding_layer, vocab_size - 1)
            restored_embeddings.append((embedding_layer, original_embed, layer_name))
        
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                )
                
                # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨device_mapï¼Œå¦‚æœæœ‰åˆ™ä¸å¼ºåˆ¶ç§»åŠ¨
                has_device_map = (hasattr(self.model, 'hf_device_map') and self.model.hf_device_map) or \
                               (hasattr(self.model, 'device_map') and self.model.device_map)
                if not has_device_map and hasattr(self.model, 'device'):
                    # åªæœ‰åœ¨æ²¡æœ‰device_mapä¸”æ¨¡å‹æœ‰å•ä¸€deviceæ—¶æ‰ç§»åŠ¨
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                # å¦åˆ™è®©HFè‡ªåŠ¨å¤„ç†
                
                # ğŸ”¥ å…³é”®ï¼šç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
                if 'input_ids' in inputs:
                    input_ids = inputs['input_ids']
                    invalid_mask = (input_ids >= vocab_size) | (input_ids < 0)
                    if torch.any(invalid_mask):
                        inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # è®¡ç®—å¯¹æ•°æ¦‚ç‡
                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
                
                return log_probs
        finally:
            # æ¢å¤åŸå§‹çš„embedding forwardæ–¹æ³•
            for embedding_layer, original_embed, layer_name in restored_embeddings:
                embedding_layer.forward = original_embed
    
    def save_model(self, save_path: str, save_adapter: bool = True):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            save_adapter: æ˜¯å¦åªä¿å­˜é€‚é…å™¨
        """
        Path(save_path).mkdir(parents=True, exist_ok=True)
        
        if save_adapter and self.use_lora:
            # åªä¿å­˜LoRAé€‚é…å™¨
            self.model.save_pretrained(save_path)
            # âœ… ä¿®å¤ï¼šä¿å­˜tokenizerï¼Œè¯„ä¼°æ—¶éœ€è¦
            self.tokenizer.save_pretrained(save_path)
            logging.info(f"LoRA adapter and tokenizer saved to: {save_path}")
        else:
            # ä¿å­˜å®Œæ•´æ¨¡å‹
            self.model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            logging.info(f"Complete model saved to: {save_path}")
    
    def load_model(self, load_path: str, load_adapter: bool = True):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            load_adapter: æ˜¯å¦åªåŠ è½½é€‚é…å™¨
        """
        try:
            if load_adapter and self.use_lora:
                # åŠ è½½LoRAé€‚é…å™¨
                self.model = PeftModel.from_pretrained(self.base_model, load_path)
                logging.info(f"LoRA adapter loaded from {load_path}")
            else:
                # åŠ è½½å®Œæ•´æ¨¡å‹
                self.model = AutoModelForCausalLM.from_pretrained(load_path)
                self.tokenizer = AutoTokenizer.from_pretrained(load_path)
                logging.info(f"Complete model loaded from {load_path}")
                
        except Exception as e:
            logging.error(f"Model loading failed: {e}")
            raise
    
    def get_model_info(self) -> Dict[str, Union[str, int]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        info = {
            "model_name": self.model_name,
            "device": str(next(self.model.parameters()).device),
            "dtype": str(next(self.model.parameters()).dtype),
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "use_lora": self.use_lora
        }
        
        if self.use_lora:
            info["lora_config"] = self.lora_config
        
        return info
    
    def freeze_base_model(self):
        """å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = False
        logging.info("Base model parameters frozen")
    
    def unfreeze_base_model(self):
        """è§£å†»åŸºç¡€æ¨¡å‹å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = True
        logging.info("Base model parameters unfrozen")
    
    def print_trainable_parameters(self):
        """æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯"""
        trainable_params = 0
        all_param = 0
        
        for _, param in self.model.named_parameters():
            all_param += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        
        print(f"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}")


class StudentModelManager:
    """Student Model Manager"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.student_model = None
        self.ppo_model = None
        
    def initialize_student(self) -> StudentModel:
        """åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹"""
        if self.student_model is None:
            self.student_model = StudentModel(
                model_name=self.config["student_model"]["base_model_name"],
                lora_config=self.config["student_model"]["lora_config"],
                device=self.config["device"]["device_map"],
                torch_dtype=getattr(torch, self.config["device"]["torch_dtype"]),
                use_lora=self.config["student_model"]["use_lora"]
            )
        
        return self.student_model
    
    def get_student(self) -> StudentModel:
        """è·å–å­¦ç”Ÿæ¨¡å‹å®ä¾‹"""
        if self.student_model is None:
            return self.initialize_student()
        return self.student_model
    
    def setup_ppo_model(self) -> AutoModelForCausalLMWithValueHead:
        """è®¾ç½®PPOæ¨¡å‹"""
        if self.ppo_model is None:
            student = self.get_student()
            self.ppo_model = student.setup_for_ppo()
        
        return self.ppo_model
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.ppo_model is not None:
            del self.ppo_model
        if self.student_model is not None:
            del self.student_model
        torch.cuda.empty_cache()


def create_student_model(config: Dict) -> StudentModel:
    """
    åˆ›å»ºå­¦ç”Ÿæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        å­¦ç”Ÿæ¨¡å‹å®ä¾‹
    """
    return StudentModel(
        model_name=config["student_model"]["base_model_name"],
        lora_config=config["student_model"]["lora_config"],
        device=config["device"]["device_map"],
        torch_dtype=getattr(torch, config["device"]["torch_dtype"]),
        use_lora=config["student_model"]["use_lora"]
    )

