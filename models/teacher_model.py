"""
Teacher Model Wrapper Module
Function: Wrap Qwen-32B-instruct teacher model, provide logits computation and caching functionality
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Optional, Tuple, Union
import hashlib
from collections import OrderedDict
import logging
from pathlib import Path
import threading
from utils.cache_utils import suppress_past_key_values_warning, update_model_for_modern_cache


class TeacherModel:
    """Teacher Model Wrapper Class"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-32B-Instruct", 
                 cache_size: int = 10000, cache_policy: str = "LRU",
                 device: str = "auto", torch_dtype: torch.dtype = torch.bfloat16,
                 max_memory: Optional[Dict[int, str]] = None):
        """
        åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            cache_size: ç¼“å­˜å¤§å°
            cache_policy: ç¼“å­˜ç­–ç•¥
            device: è®¾å¤‡
            torch_dtype: æ•°æ®ç±»å‹
            max_memory: æ¯ä¸ªGPUçš„æœ€å¤§æ˜¾å­˜é™åˆ¶ï¼ˆå­—å…¸ï¼Œå¦‚{0: "75GB", 1: "75GB"}ï¼‰
        """
        self.model_name = model_name
        self.device = device
        self.torch_dtype = torch_dtype
        self.max_memory = max_memory
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.cache_size = cache_size
        self.cache_policy = cache_policy
        self.cache = OrderedDict()
        self.cache_hits = 0
        self.cache_misses = 0
        
        # çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤ tokenizer çš„çº¿ç¨‹å®‰å…¨
        self._tokenizer_lock = threading.Lock()
        
        # æŠ‘åˆ¶past_key_valuesè­¦å‘Š
        suppress_past_key_values_warning()
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        
        # æ›´æ–°æ¨¡å‹ä»¥ä½¿ç”¨ç°ä»£ç¼“å­˜
        self.model = update_model_for_modern_cache(self.model)
        
        logging.info(f"Teacher model {model_name} loaded successfully")
        logging.info(f"Cache configuration: size={cache_size}, policy={cache_policy}")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            # âœ… ç¡®ä¿ï¼šå¦‚æœä½¿ç”¨ device_mapï¼Œå¿…é¡»è®¾ç½® low_cpu_mem_usage=True
            # å¦‚æœ device ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä¸ä½¿ç”¨ device_map
            device_map_value = self.device if (self.device and self.device.lower() != 'none') else None
            low_cpu_mem_usage = True if device_map_value is not None else False
            
            load_kwargs = {
                "torch_dtype": self.torch_dtype,
                "trust_remote_code": True,
                "low_cpu_mem_usage": low_cpu_mem_usage  # âœ… æ¡ä»¶è®¾ç½®ï¼šä½¿ç”¨device_mapæ—¶å¿…é¡»ä¸ºTrue
            }
            
            # åªåœ¨ device_map ä¸ä¸º None æ—¶æ·»åŠ 
            if device_map_value is not None:
                load_kwargs["device_map"] = device_map_value
            
            # å¦‚æœæŒ‡å®šäº†max_memoryï¼Œæ·»åŠ è¯¥å‚æ•°ï¼ˆç”¨äºé™åˆ¶ç‰¹å®šGPUçš„æ˜¾å­˜ä½¿ç”¨ï¼‰
            if self.max_memory is not None:
                load_kwargs["max_memory"] = self.max_memory
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )
            
            # âœ… æ–¹å¼1ï¼šä¸resizeï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹åŸå§‹æƒé‡ï¼ˆæœ€ç¨³å®šæ–¹å¼ï¼‰
            tokenizer_vocab_size = len(self.tokenizer)
            model_emb_size = self.model.get_input_embeddings().weight.size(0)
            
            logging.info(f"ğŸ“Š Vocabå¤§å°æ£€æŸ¥:")
            logging.info(f"   tokenizer vocab_size: {tokenizer_vocab_size}")
            logging.info(f"   model embedding size: {model_emb_size}")
            logging.info(f"   model.config.vocab_size: {getattr(self.model.config, 'vocab_size', 'N/A')}")
            
            if model_emb_size != tokenizer_vocab_size:
                logging.warning(f"âš ï¸  Vocabå¤§å°ä¸åŒ¹é…ï¼ˆ{model_emb_size} vs {tokenizer_vocab_size}ï¼‰")
                logging.info(f"   ä½†é‡‡ç”¨æ–¹å¼1ï¼šä¸resizeï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹åŸå§‹æƒé‡")
                logging.info(f"   è¿™æ˜¯æœ€ç¨³å®šçš„æ–¹å¼ï¼Œå³ä½¿æœ‰å·®å¼‚ä¹Ÿä¸ä¼šè§¦å‘CUDAé”™è¯¯")
            else:
                logging.info(f"âœ… Vocabå¤§å°åŒ¹é…: {model_emb_size}")
            
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            logging.info("Model loaded successfully")
            
        except FileNotFoundError as e:
            logging.error(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            raise
        except RuntimeError as e:
            logging.error(f"æ¨¡å‹åŠ è½½è¿è¡Œæ—¶é”™è¯¯: {e}")
            raise
        except Exception as e:
            logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _update_cache(self, key: str, value: torch.Tensor):
        """æ›´æ–°ç¼“å­˜"""
        # ğŸ”¥ ä¿®å¤æ˜¾å­˜æ³„æ¼ï¼šå°†logitsç§»åŠ¨åˆ°CPU
        self.cache[key] = value.clone().detach().cpu()
        
        # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œç§»é™¤æœ€æ—§çš„é¡¹
        if len(self.cache) > self.cache_size:
            if self.cache_policy == "LRU":
                self.cache.popitem(last=False)
            else:
                # éšæœºç§»é™¤
                import random
                random_key = random.choice(list(self.cache.keys()))
                del self.cache[random_key]
    
    def get_logits(self, text: Union[str, List[str]], 
                   use_cache: bool = True) -> torch.Tensor:
        """
        è·å–æ–‡æœ¬çš„logits
        
        Args:
            text: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            logitså¼ é‡
        """
        if isinstance(text, str):
            text = [text]
        
        batch_logits = []
        
        for single_text in text:
            cache_key = None
            if use_cache:
                cache_key = self._get_cache_key(single_text)
                
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in self.cache:
                self.cache_hits += 1
                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                self.cache.move_to_end(cache_key)
                # ğŸ”¥ ä¿®å¤ï¼šä¸è¦ç§»åŠ¨ç¼“å­˜åˆ°deviceï¼Œè®©æ¨¡å‹è‡ªåŠ¨å¤„ç†
                cached_logits = self.cache[cache_key]
                batch_logits.append(cached_logits)
                continue
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œè®¡ç®—logits
            self.cache_misses += 1
            
            # ğŸ”¥ è·å–vocab_sizeï¼ˆåº”è¯¥å·²åŒ¹é…ï¼Œå› ä¸º_load_modelä¸­å·²æ£€æŸ¥ï¼‰
            vocab_size = len(self.tokenizer)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªåœ¨æ ¡éªŒé˜¶æ®µclamp input_idsï¼Œä¸ä¿®æ”¹embeddingå±‚
            # åˆ é™¤monkey patchï¼Œå› ä¸ºvocab_sizeå·²åŒ¹é…ï¼Œæ— éœ€åŒ…è£…embedding
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ tokenizer è°ƒç”¨ï¼Œé¿å… "Already borrowed" é”™è¯¯
            max_retries = 3
            retry_count = 0
            logits = None
            
            while retry_count < max_retries and logits is None:
                try:
                    with self._tokenizer_lock:
                        with torch.no_grad():
                            # åˆ†è¯
                            inputs = self.tokenizer(
                                single_text,
                                return_tensors="pt",
                                truncation=True,
                                max_length=512,
                                padding=True
                            )
                            
                            # ğŸ”¥ ä¿®å¤ï¼šä¸è¦å¼ºåˆ¶ç§»åŠ¨åˆ°æŸä¸ªdevice
                            # å¯¹äºdevice_map="auto"çš„æ¨¡å‹ï¼Œè®©HFè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
                            # inputsä¿æŒåœ¨CPUï¼Œmodel(**inputs)ä¼šè‡ªåŠ¨å¤„ç†
                            
                            # ğŸ”¥ å…³é”®ï¼šç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆåœ¨é€å…¥æ¨¡å‹å‰æ ¡éªŒï¼‰
                            if 'input_ids' in inputs:
                                input_ids = inputs['input_ids']
                                # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºèŒƒå›´çš„tokenï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºvocabå·²åŒ¹é…ï¼‰
                                if input_ids.numel() > 0:
                                    max_id = input_ids.max().item()
                                    min_id = input_ids.min().item()
                                    if max_id >= vocab_size or min_id < 0:
                                        logging.warning(f"âš ï¸ input_idsè¶…å‡ºèŒƒå›´: [{min_id}, {max_id}], vocab_size={vocab_size}, è‡ªåŠ¨clamp")
                                        inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                            
                            # å‰å‘ä¼ æ’­ï¼ˆä¸ç§»åŠ¨inputsåˆ°deviceï¼Œè®©HFè‡ªåŠ¨å¤„ç†device_mapï¼‰
                            outputs = self.model(**inputs)
                            logits = outputs.logits
                            
                            # å­˜å‚¨åˆ°ç¼“å­˜
                            if use_cache and cache_key is not None:
                                self._update_cache(cache_key, logits)
                            
                            batch_logits.append(logits)
                            break  # æˆåŠŸè·å–logitsï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                except RuntimeError as e:
                    if "Already borrowed" in str(e) and retry_count < max_retries - 1:
                        retry_count += 1
                        logging.warning(f"Tokenizer çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œé‡è¯• {retry_count}/{max_retries}: {e}")
                        import time
                        time.sleep(0.01 * retry_count)  # é€’å¢ç­‰å¾…æ—¶é—´
                    else:
                        logging.error(f"Tokenizer è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise
        
        # å¦‚æœæ˜¯å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›å•ä¸ªlogits
        if len(batch_logits) == 1:
            return batch_logits[0]
        
        # å¤šä¸ªæ–‡æœ¬æ—¶ï¼Œè¿”å›listï¼ˆå› ä¸ºseq_lenå¯èƒ½ä¸åŒï¼Œcatä¼šå¤±è´¥ï¼‰
        return batch_logits
    
    def generate_response(self, prompt: str, max_length: int = 256,
                         temperature: float = 0.7, do_sample: bool = True) -> str:
        """
        ç”Ÿæˆå“åº”
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„å“åº”
        """
        # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ tokenizer è°ƒç”¨ï¼Œé¿å… "Already borrowed" é”™è¯¯
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                with self._tokenizer_lock:
                    with torch.no_grad():
                        inputs = self.tokenizer(
                            prompt,
                            return_tensors="pt",
                            truncation=True,
                            max_length=512
                        )
                        
                        # ğŸ”¥ è·å–vocab_sizeï¼ˆåº”è¯¥å·²åŒ¹é…ï¼Œå› ä¸º_load_modelä¸­å·²æ£€æŸ¥ï¼‰
                        vocab_size = len(self.tokenizer)
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿input_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if 'input_ids' in inputs:
                            input_ids = inputs['input_ids']
                            if input_ids.numel() > 0:
                                max_token_id = input_ids.max().item()
                                min_token_id = input_ids.min().item()
                                input_len = input_ids.shape[1]
                                
                                if max_token_id >= vocab_size or min_token_id < 0:
                                    logging.warning(f"âš ï¸ input_idsè¶…å‡ºèŒƒå›´: [{min_token_id}, {max_token_id}], vocab_size={vocab_size}, è‡ªåŠ¨clamp")
                                    inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                            
                            # ğŸ” æ£€æŸ¥è¾“å…¥é•¿åº¦æ˜¯å¦è¶…å‡ºæ¨¡å‹é™åˆ¶
                            max_position_embeddings = getattr(self.model.config, 'max_position_embeddings', None)
                            if max_position_embeddings and input_len > max_position_embeddings:
                                logging.error(f"âŒ è¾“å…¥åºåˆ—é•¿åº¦ {input_len} è¶…å‡ºæ¨¡å‹æœ€å¤§ä½ç½® {max_position_embeddings}!")
                                # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
                                inputs['input_ids'] = input_ids[:, :max_position_embeddings]
                                if 'attention_mask' in inputs:
                                    inputs['attention_mask'] = inputs['attention_mask'][:, :max_position_embeddings]
                                logging.warning(f"   å·²æˆªæ–­åˆ° {max_position_embeddings}")
                            
                            # è®°å½•è¾“å…¥ä¿¡æ¯ï¼ˆä»…åœ¨å‰å‡ ä¸ªæ ·æœ¬æˆ–å‡ºç°é—®é¢˜æ—¶ï¼‰
                            if retry_count > 0 or max_token_id >= vocab_size * 0.9:
                                logging.debug(f"ğŸ“Š ç”Ÿæˆå‰æ£€æŸ¥: input_len={input_len}, token_range=[{min_token_id}, {max_token_id}], vocab_size={vocab_size}")
                        
                        # ğŸ”¥ ä¿®å¤ï¼šä¸è¦ç§»åŠ¨inputsåˆ°deviceï¼Œè®©HFè‡ªåŠ¨å¤„ç†device_map
                        
                        # ğŸ” è®¾ç½®æœ‰æ•ˆçš„pad_token_idå’Œeos_token_id
                        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
                        eos_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else pad_token_id
                        
                        # ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if pad_token_id is not None:
                            pad_token_id = min(pad_token_id, vocab_size - 1)
                        if eos_token_id is not None:
                            eos_token_id = min(eos_token_id, vocab_size - 1)
                        
                        # ğŸ” ç”Ÿæˆå‰ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®
                        max_positions = getattr(self.model.config, 'max_position_embeddings', None)
                        max_total_len = max_positions if max_positions else 2048
                        current_len = inputs['input_ids'].shape[1]
                        max_allowed_new_tokens = min(max_length, max_total_len - current_len - 10)  # ç•™10ä¸ªtokençš„å®‰å…¨ä½™é‡
                        
                        if max_allowed_new_tokens <= 0:
                            logging.error(f"âŒ æ— æ³•ç”Ÿæˆæ–°token: å½“å‰é•¿åº¦ {current_len} + é¢„ç•™ {10} >= æœ€å¤§é•¿åº¦ {max_total_len}")
                            return ""
                        
                        # ğŸ” è¯¦ç»†çš„ç”Ÿæˆå‚æ•°æ—¥å¿—ï¼ˆä»…åœ¨é”™è¯¯æ—¶ï¼‰
                        if retry_count > 0:
                            logging.info(f"ğŸ” ç”Ÿæˆå‚æ•°: max_new_tokens={max_allowed_new_tokens}, current_len={current_len}, max_total={max_total_len}")
                            logging.info(f"   pad_token_id={pad_token_id}, eos_token_id={eos_token_id}, vocab_size={vocab_size}")
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ LogitsProcessoræ¥maskè¶…å‡ºèŒƒå›´çš„tokenï¼ˆä¸æ”¹å˜å½¢çŠ¶ï¼‰
                        from transformers import LogitsProcessorList
                        
                        class TokenRangeLogitsProcessor:
                            """Maskè¶…å‡ºtokenizerèŒƒå›´çš„logitsï¼ˆä¸æ”¹å˜å½¢çŠ¶ï¼‰"""
                            def __init__(self, max_valid_token_id: int):
                                self.max_valid_token_id = max_valid_token_id
                                self.vocab_end_idx = max_valid_token_id + 1
                            
                            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
                                # ğŸ”¥ å…³é”®ï¼šåªmaskï¼Œä¸åˆ‡ç‰‡ï¼ˆä¿æŒå½¢çŠ¶ä¸å˜ï¼‰
                                if scores.shape[-1] > self.vocab_end_idx:
                                    scores[..., self.vocab_end_idx:] = float('-inf')
                                return scores
                        
                        max_valid_token_id = vocab_size - 1
                        logits_processor = LogitsProcessorList([
                            TokenRangeLogitsProcessor(max_valid_token_id)
                        ])
                        
                        # ğŸ”¥ å…³é”®ï¼šåªåœ¨è¾“å…¥é˜¶æ®µæ ¡éªŒinput_idsï¼Œä¸ä¿®æ”¹embeddingå±‚
                        if 'input_ids' in inputs and inputs['input_ids'].numel() > 0:
                            input_ids = inputs['input_ids']
                            if input_ids.max().item() >= vocab_size or input_ids.min().item() < 0:
                                logging.warning(f"âš ï¸ input_idsè¶…å‡ºèŒƒå›´ï¼Œclampåˆ°[0, {vocab_size-1}]")
                                inputs['input_ids'] = torch.clamp(input_ids, 0, vocab_size - 1)
                        
                        # ğŸ”¥ ä¿®å¤ï¼šä¸è¦ç§»åŠ¨inputsåˆ°deviceï¼Œè®©HFè‡ªåŠ¨å¤„ç†device_map
                        # ç”Ÿæˆï¼ˆinputsä¿æŒåœ¨CPUï¼Œmodel.generateä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…ï¼‰
                        generate_kwargs = {
                            **inputs,
                            "max_new_tokens": max_allowed_new_tokens,
                            "temperature": temperature,
                            "do_sample": do_sample,
                            "pad_token_id": pad_token_id,
                            "eos_token_id": eos_token_id,
                            "repetition_penalty": 1.1,
                            "logits_processor": logits_processor,
                            "use_cache": True
                        }
                        
                        outputs = self.model.generate(**generate_kwargs)
                        
                        # ğŸ” è¯¦ç»†éªŒè¯ç”Ÿæˆçš„token ID
                        invalid_mask = (outputs >= vocab_size) | (outputs < 0)
                        if torch.any(invalid_mask):
                            invalid_ids = outputs[invalid_mask].unique().tolist()
                            output_max = outputs.max().item()
                            output_min = outputs.min().item()
                            logging.error(f"âŒ ç”Ÿæˆçš„token IDè¶…å‡ºèŒƒå›´!")
                            logging.error(f"   æ— æ•ˆIDåˆ—è¡¨: {invalid_ids[:10]}")
                            logging.error(f"   è¾“å‡ºtokenèŒƒå›´: [{output_min}, {output_max}], vocab_size={vocab_size}")
                            logging.error(f"   è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
                            logging.error(f"   å°†æˆªæ–­åˆ°æœ‰æ•ˆèŒƒå›´")
                            outputs = torch.clamp(outputs, 0, vocab_size - 1)
                        
                        # è§£ç 
                        input_length = inputs['input_ids'].shape[1]
                        if len(outputs[0]) > input_length:
                            generated_text = self.tokenizer.decode(
                                outputs[0][input_length:],
                                skip_special_tokens=True
                            )
                        else:
                            generated_text = ""
                        
                        return generated_text
            except RuntimeError as e:
                error_str = str(e)
                if "device-side assert" in error_str or "CUDA error" in error_str:
                    retry_count += 1
                    # ğŸ” è¯¦ç»†é”™è¯¯è¯Šæ–­
                    logging.error(f"âŒ CUDA device-side asserté”™è¯¯ (é‡è¯• {retry_count}/{max_retries})")
                    logging.error(f"   é”™è¯¯ä¿¡æ¯: {error_str[:500]}")  # åªæ˜¾ç¤ºå‰500å­—ç¬¦
                    
                    # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥æ¨¡å‹å’Œè¾“å…¥çŠ¶æ€
                    try:
                        if 'input_ids' in locals():
                            logging.error(f"   è¾“å…¥å½¢çŠ¶: {inputs.get('input_ids', 'N/A').shape if 'input_ids' in inputs else 'N/A'}")
                            if 'input_ids' in inputs:
                                input_ids = inputs['input_ids']
                                logging.error(f"   è¾“å…¥tokenèŒƒå›´: [{input_ids.min().item()}, {input_ids.max().item()}]")
                                logging.error(f"   è¾“å…¥åºåˆ—é•¿åº¦: {input_ids.shape[1]}")
                        
                        model_vocab = getattr(self.model.config, 'vocab_size', None)
                        max_pos = getattr(self.model.config, 'max_position_embeddings', None)
                        logging.error(f"   æ¨¡å‹vocab_size: {model_vocab}")
                        logging.error(f"   æ¨¡å‹max_position_embeddings: {max_pos}")
                        logging.error(f"   tokenizer vocab_size: {len(self.tokenizer)}")
                    except Exception as diag_e:
                        logging.error(f"   è¯Šæ–­ä¿¡æ¯è·å–å¤±è´¥: {diag_e}")
                    
                    if retry_count < max_retries:
                        logging.warning(f"   æ¸…ç†CUDAç¼“å­˜å¹¶é‡è¯•...")
                        # æ¸…ç†CUDAç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                        # é‡ç½®æ¨¡å‹çŠ¶æ€
                        self.model.eval()
                        import time
                        time.sleep(0.1 * retry_count)
                    else:
                        logging.error(f"âŒ CUDAé”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        logging.error(f"   å»ºè®®: æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦æŸåï¼Œæˆ–å°è¯•é‡æ–°åŠ è½½æ¨¡å‹")
                        # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                        return ""
                elif "Already borrowed" in error_str and retry_count < max_retries - 1:
                    retry_count += 1
                    logging.warning(f"Tokenizer çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼Œé‡è¯• {retry_count}/{max_retries}: {e}")
                    import time
                    time.sleep(0.01 * retry_count)  # é€’å¢ç­‰å¾…æ—¶é—´
                else:
                    logging.error(f"ç”Ÿæˆå“åº”å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            except Exception as e:
                logging.error(f"ç”Ÿæˆå“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
    
    def compute_log_probs(self, text: str) -> torch.Tensor:
        """
        è®¡ç®—æ–‡æœ¬çš„å¯¹æ•°æ¦‚ç‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¯¹æ•°æ¦‚ç‡å¼ é‡
        """
        with torch.no_grad():
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸è¦ç§»åŠ¨inputsåˆ°deviceï¼Œè®©HFè‡ªåŠ¨å¤„ç†device_map
            
            outputs = self.model(**inputs)
            logits = outputs.logits
            
            # è®¡ç®—å¯¹æ•°æ¦‚ç‡
            log_probs = F.log_softmax(logits, dim=-1)
            
            return log_probs
    
    def get_cache_stats(self) -> Dict[str, Union[int, float]]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": hit_rate,
            "cache_size": len(self.cache),
            "max_cache_size": self.cache_size
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        logging.info("Cache cleared")
    
    def save_cache(self, filepath: str):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        cache_data = {
            "cache": dict(self.cache),
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses
        }
        
        torch.save(cache_data, filepath)
        logging.info(f"Cache saved to: {filepath}")
    
    def load_cache(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        if Path(filepath).exists():
            cache_data = torch.load(filepath, map_location='cpu')
            self.cache = OrderedDict(cache_data["cache"])
            self.cache_hits = cache_data["cache_hits"]
            self.cache_misses = cache_data["cache_misses"]
            logging.info(f"Cache loaded from {filepath}")
        else:
            logging.warning(f"Cache file does not exist: {filepath}")
    
    def get_model_info(self) -> Dict[str, Union[str, int]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # ğŸ”¥ ä¿®å¤ï¼šdevice_map="auto"æ—¶æ¨¡å‹æ²¡æœ‰å•ä¸€device
        try:
            if hasattr(self.model, 'hf_device_map'):
                device_info = "sharded"  # åˆ†å¸ƒå¼æ¨¡å‹
            elif hasattr(self.model, 'device'):
                device_info = str(self.model.device)
            else:
                device_info = "unknown"
        except:
            device_info = "unknown"
        
        return {
            "model_name": self.model_name,
            "device": device_info,
            "dtype": str(self.model.dtype) if hasattr(self.model, 'dtype') else "unknown",
            "num_parameters": sum(p.numel() for p in self.model.parameters()),
            "cache_size": len(self.cache),
            "cache_policy": self.cache_policy
        }


class TeacherModelManager:
    """Teacher Model Manager"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.teacher_model = None
        
    def initialize_teacher(self) -> TeacherModel:
        """åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹"""
        if self.teacher_model is None:
            self.teacher_model = TeacherModel(
                model_name=self.config["teacher_model"]["model_name"],
                cache_size=self.config["teacher_model"]["cache_size"],
                cache_policy=self.config["teacher_model"]["cache_policy"],
                device=self.config["device"]["device_map"],
                torch_dtype=getattr(torch, self.config["device"]["torch_dtype"])
            )
        
        return self.teacher_model
    
    def get_teacher(self) -> TeacherModel:
        """è·å–æ•™å¸ˆæ¨¡å‹å®ä¾‹"""
        if self.teacher_model is None:
            return self.initialize_teacher()
        return self.teacher_model
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.teacher_model is not None:
            # ä¿å­˜ç¼“å­˜
            cache_file = "./cache/teacher_cache.pkl"
            self.teacher_model.save_cache(cache_file)
            
            # æ¸…ç†GPUå†…å­˜
            del self.teacher_model
            torch.cuda.empty_cache()


def create_teacher_model(config: Dict) -> TeacherModel:
    """
    åˆ›å»ºæ•™å¸ˆæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        æ•™å¸ˆæ¨¡å‹å®ä¾‹
    """
    return TeacherModel(
        model_name=config["teacher_model"]["model_name"],
        cache_size=config["teacher_model"]["cache_size"],
        cache_policy=config["teacher_model"]["cache_policy"],
        device=config["device"]["device_map"],
        torch_dtype=getattr(torch, config["device"]["torch_dtype"])
    )

